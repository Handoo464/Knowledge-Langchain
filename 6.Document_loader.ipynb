{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load from PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local file or online file\n",
    "# 16 Pages\n",
    "url = \"https://arxiv.org/pdf/2312.16862.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Load per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader(url)\n",
    "docs = pdf_loader. load()\n",
    "\n",
    "len (docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 0}, page_content='TinyGPT-V: Efficient Multimodal Large Language Model\\nvia Small Backbones\\nZhengqing Yuan1 Zhaoxu Li 2 Weiran Huang3 Yanfang Ye1 Lichao Sun 4\\nAbstract\\nIn recent years, multimodal large language mod-\\nels (MLLMs) such as GPT-4V have demonstrated\\nremarkable advancements, excelling in a variety\\nof vision-language tasks. Despite their prowess,\\nthe closed-source nature and computational de-\\nmands of such models limit their accessibility and\\napplicability. This study introduces TinyGPT-V ,\\na novel open-source MLLM, designed for effi-\\ncient training and inference across various vision-\\nlanguage tasks, including image captioning (IC)\\nand visual question answering (VQA). Leverag-\\ning a compact yet powerful architecture, TinyGPT-\\nV integrates the Phi-2 language model with pre-\\ntrained vision encoders, utilizing a unique map-\\nping module for visual and linguistic information\\nfusion. With a training regimen optimized for\\nsmall backbones and employing a diverse dataset\\namalgam, TinyGPT-V requires significantly lower\\ncomputational resources—24GB for training and\\nas little as 8GB for inference—without compro-\\nmising on performance. Our experiments demon-\\nstrate that TinyGPT-V , with its language model\\n2.8 billion parameters, achieves comparable re-\\nsults in VQA and image inference tasks to its\\nlarger counterparts while being uniquely suited\\nfor deployment on resource-constrained devices\\nthrough innovative quantization techniques. This\\nwork not only paves the way for more accessible\\nand efficient MLLMs but also underscores the po-\\ntential of smaller, optimized models in bridging\\nthe gap between high performance and computa-\\ntional efficiency in real-world applications. Addi-\\ntionally, this paper introduces a new approach to\\nmultimodal large language models using smaller\\nbackbones. Our code and training weights are\\navailable in the supplementary material.\\n1Universiy of Notre Dame 2Nanyang Technological University\\n3Shanghai Jiao Tong University 4Lehigh University. Correspon-\\ndence to: Lichao Sun <lis221@lehigh.edu>.\\nAccepted to the Workshop on Advancing Neural Network Training\\nat International Conference on Machine Learning (W ANT@ICML\\n2024).\\n5.07.510.012.515.0\\nParameter (Billions)\\n38\\n40\\n42\\n44\\n46Performance Flamingo\\nBLIP-2\\nLLaVA\\nInstructBLIP\\nMiniGPT-4\\nTinyGPT-V\\nFigure 1: Comparison of TinyGPT-V with other current\\nMLLMs models shows TinyGPT-V achieves cost-effective,\\nefficient, and high-performing with fewer parameters.\\n1. Introduction\\nIn recent years, the field of artificial intelligence has seen\\nsignificant advancements through the development of multi-\\nmodal large language models (MLLMs), such as GPT-4V ,\\nwhich have shown exceptional performance across a range\\nof vision-language tasks (Yang et al., 2023). Despite GPT-\\n4V’s impressive capabilities, its closed-source nature limits\\nits widespread application and adaptability. In contrast, the\\nopen-source landscape for MLLMs is rapidly evolving, pre-\\nsenting models like LLaV A and MiniGPT-4 that excel in\\nimage captioning (IC), visual question answering (VQA)\\noften comparable GPT-4V in these areas (Dai et al., 2023;\\nLiu et al., 2023a;b; Zhu et al., 2023). Notably, MiniGPT-\\nv2 (Chen et al., 2023) has demonstrated superior perfor-\\nmance in various visual grounding and question-answering\\ntasks. However, its training code remains proprietary, which\\nposes challenges for community-driven advancements and\\nadaptability.\\nAlthough the impressive vision-language capabilities\\ndemonstrated by some open-source MLLMs, they fre-\\nquently necessitate significant computational resources for\\ntraining and inference. For example, training LLaV A-v1.5-\\n13B (Liu et al., 2023a) required 8 × A100 GPUs, each\\nequipped with 80GB of memory, cumulating in 25.5 hours\\nof continuous training. As shown in Figure 2 (a), the un-\\n1\\narXiv:2312.16862v3  [cs.CV]  21 Jun 2024'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 1}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nProjection (110M)\\n0.8%\\nEVA-VIT (0.98B)\\n7.0%\\nVicuna (13B)\\n92.3%\\nProjection (119M)\\n3.1%\\nEVA-VIT (0.98B)\\n25.1%\\nPhi-2 (2.8B)\\n71.8%\\n(a) MiniGPT-4 (a) TinyGPT-V\\nFigure 2: (a) is the occupancy ratio of each component in MiniGPT-4, and (b) is the occupancy ratio of each component in\\nTinyGPT-V . We have considerably narrowed down the occupancy ratio of the Language model in MLLMs.\\nderlying performance of large language models, which are\\nintegral to MLLMs, is pivotal. Models such as LLaV A-\\nv1.5-13B and MiniGPT-v2 depend on high-capacity back-\\nbones which are Vicuna-13b-v1.5 (Zheng et al., 2023) and\\nLLaMA2-7B-Chat (Touvron et al., 2023), respectively, ne-\\ncessitating a substantial number of parameters to effectively\\ntackle complex tasks including IC and VQA.\\nWe introduce TinyGPT-V , a novel model designed for effi-\\ncient training and inference, requiring only 24GB of GPU\\nmemory for training and as little as 8GB of GPU or CPU\\nmemory for inference. This model makes use of the ad-\\nvanced large language model Phi-2 (Javaheripi et al., 2023)\\nand incorporates pre-trained vision modules from (Li et al.,\\n2023a) and CLIP (Radford et al., 2021) as its vision encoder,\\ncoupled with a mapping module to facilitate the integration\\nof visual information. During training, TinyGPT-V adopts\\na novel training methodology focused on small pre-trained\\nbackbones, unlike any other MLLMs, utilizing the unique\\nmapping module between the visual encoder and the lan-\\nguage model as well as novelty normalization methods,\\nwhile keeping all other components frozen. For its train-\\ning dataset, TinyGPT-V employs the multi-tasks datasets,\\nincluding LAION (Schuhmann et al., 2021), Conceptual\\nCaptions (Changpinyo et al., 2021; Sharma et al., 2018),\\nSBU (Ordonez et al., 2011), and others (Lin et al., 2015;\\nSchwenk et al., 2022; Hudson & Manning, 2019; Kiela et al.,\\n2020; Lu et al., 2021; Gurari et al., 2018; Mao et al., 2016;\\nKazemzadeh et al., 2014; Yu et al., 2016).\\nIn our study, we found that TinyGPT-V exhibits similar traits\\nwith GPT-4, especially when doing some VQA and image\\ninference. With only 2.8 billion parameters of its language\\nmodel, TinyGPT-V employs a unique quantization process,\\nlike using 8-bit quantization, making it well-suited for local\\ndeployment and inference on 8GB mobile devices. This\\nmodel represents a significant advancement in achieving a\\nbalance between exceptional performance and efficiency in\\nMLLMs, as shown in Figure 1. Our work not only aims\\nto enable the community to develop more cost-effective,\\nefficient, and high-performing MLLMs for widespread real-\\nworld applications but also introduces a training framework\\noptimized for small pre-trained backbones.\\n2. Related Work\\nAdvanced language model. The evolution of language\\nmodels has been marked by significant milestones, start-\\ning with early successes like GPT2 (Radford et al., 2019)\\nand BERT (Devlin et al., 2018) in natural language process-\\ning (NLP). These foundational models set the stage for the\\nsubsequent development of vastly larger language models,\\nencompassing hundreds of billions of parameters. This dra-\\nmatic increase in scale has led to the emergence of advanced\\ncapabilities as seen in models like GPT-3 (Brown et al.,\\n2020), Chinchilla (Hoffmann et al., 2022), OPT (Zhang\\net al., 2022), and BLOOM (Workshop et al., 2022). These\\nlarge language models (LLMs) have been instrumental\\nin further advancements in the field. For instance, Chat-\\nGPT (OpenAI, 2022) and InstructGPT (Ouyang et al., 2022)\\nleverage these powerful models to answer diverse ques-\\ntions and perform complex tasks such as coding. The in-\\ntroduction of open-source LLMs like LLaMA (Touvron\\net al., 2023) has further propelled research in this area, in-\\nspiring subsequent developments like Alpaca (Taori et al.,\\n2023), Vicuna (Chiang et al., 2023). These models fine-tune\\nthe LLaMA model with additional high-quality instruction\\ndatasets, showcasing the versatility and adaptability of LLM\\nframeworks.Among the most notable recent advancements\\nare Phi (Li et al., 2023b) and its successor, Phi-2 (Javaheripi\\net al., 2023). These models have demonstrated exceptional\\nperformance, rivaling or even surpassing models up to 25\\ntimes larger in scale. This indicates a significant shift in\\n2'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 2}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nFigure 3: Compared to other general-purpose MLLMs, our TinyGPT-V achieves the same performance as 13B or 7B models\\nin a variety of visual language tasks.\\nthe landscape of language modeling, emphasizing efficiency\\nand effectiveness without necessarily relying on sheer size.\\nMultimodal language model. In recent years, the trend of\\naligning visual input to large language models for vision-\\nlanguage tasks has gained significant attention (Chen et al.,\\n2022; Tsimpoukelli et al., 2021; Alayrac et al., 2022; Li\\net al., 2023a; Liu et al., 2023b;a; Zhu et al., 2023; Chen\\net al., 2023). Seminal works like VisualGPT (Chen et al.,\\n2022) and Frozen (Tsimpoukelli et al., 2021), which uti-\\nlized pre-trained language models for image captioning and\\nvisual question answering. This approach was further ad-\\nvanced by models such as Flamingo (Alayrac et al., 2022),\\nwhich incorporated gated cross-attention mechanisms to\\nalign pre-trained vision encoders and language models, train-\\ning on vast image-text pairs. BLIP-2 (Li et al., 2023a)\\nintroduced an efficient Q-Former for aligning visual and\\nlanguage modalities. These groundbreaking studies have\\npaved the way for further innovations in the field, leading to\\nthe development of models like LLaV A (Liu et al., 2023b)\\nand MiniGPT4 (Zhu et al., 2023), and their subsequent iter-\\nations, LLaV A-v1.5 (Liu et al., 2023a), MiniGPT-v2 (Chen\\net al., 2023), ArtGPT-4 (Yuan et al., 2023), instruction GPT-\\n4 (Wei et al., 2023) and Instruction Mining (Cao et al.,\\n2023). These models have demonstrated advanced multi-\\nmodal capabilities through instruction tuning, showcasing\\nremarkable generalization abilities.\\n3. Method\\nWe briefly introduce our vision-language model, TinyGPT-\\nV , followed by an analysis of its structure, culminating in a\\ndetailed description of the training process for each stage.\\n3.1. Model Architecture\\nIn this subsection, we present the architecture of TinyGPT-V ,\\nwhich consists of a visual encoder, projection layers, and a\\nlarge language model, as shown in Figure 4.\\nVisual encoder backbone. In the TinyGPT-V , it utilizes\\nEV A (Fang et al., 2022) of the ViT serves as the visual\\nfoundation model, which remains inactive during the entire\\ntraining process. Our model operates at an image resolution\\nof 224x224 for Stages 1, 2, and 3, and at 448x448 for Stage\\n4. The positional encoding is enhanced to accommodate the\\nincreased image resolution which is known as the Relative\\nPosition Bias (Dufter et al., 2021). It enhances the model’s\\nunderstanding of the spatial relationships between elements\\nin an image.\\nProjection layers. The Projection layers embed visual\\nfeatures extracted by the visual encoder into the language\\nmodel, enhancing the model’s ability to process image-\\nbased information. We adopt the Q-Former layers from\\nthe BLIP-2 architecture (Li et al., 2023a) as the initial pro-\\n3'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 3}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nPhi-2\\nLoRA+Normalization\\nFreeze Train\\nQ-Former\\nViT\\nLinear 1 (MiniGPT-4)\\nLinear 2\\nThis is an image of an alpaca. \\nAlpacas are domesticated \\nspecies of South American \\ncamelids, known for their \\nsoft fluffy coats which are \\nused for making wool.\\n[vqa]What’s this?\\nFigure 4: Architecture of TinyGPT-V . The model takes a\\nvisual backbone, which remains frozen during all training\\nphases. We concatenate Q-Former layer visual output tokens\\nfrom ViT backbone and project them into Phi-2 language\\nmodel space via two linear projection layers.\\njection layer, aiming to leverage the full potential of the\\npre-trained BLIP system within visual language models.\\nThis strategy significantly reduces the number of parameters\\nneeding training. The second and third layers are linear\\nprojection layers, designed to bridge the dimensionality gap\\nbetween the Q-Former output and the language model’s\\nembedding layer, thereby aligning visual tokens more effec-\\ntively with the language model’s hidden space. As shown in\\nFigure 6, to expedite TinyGPT-V’s training, we initially use\\na pre-trained Linear Projection from MiniGPT-4 (Vicuna\\n7B) as the second layer. We then introduce an additional lin-\\near projection layer, initialized with a Gaussian distribution,\\nas the third layer to seamlessly integrate into the hidden\\nspace of the Phi-2 model.\\nLarge lanuguage model backbone. Our TinyGPT-V large\\nlanguage model is built upon the Phi-2 model (Javaheripi\\net al., 2023) as its backbone. Phi-2, a 2.7 billion-parameter\\nlanguage model, exhibits exceptional reasoning and lan-\\nguage comprehension abilities, achieving state-of-the-art\\nperformance among language models with fewer than 13\\nbillion parameters. In complex benchmarks, Phi-2 either\\nmatches or exceeds the performance of models up to 25\\ntimes its size. We primarily use Phi-2’s linguistic abilities\\nto do various vision-language tasks. Specifically, for vision\\nreasoning tasks that involve spatial location identification,\\nwe instruct the linguistic model to generate textual descrip-\\ntions of what will happen in the next scenario, representing\\ntheir objects’ coordinates, as shown in Table 8.\\nNormalization and LoRA for TinyGPT-V .In Section 4.4,\\nwe conclude that training smaller-scale large language mod-\\nels for transfer learning, especially across different modali-\\nties (e.g., from text to image), poses significant challenges.\\nOur studies indicate that these smaller models are prone to\\nencountering NaN or INF values during multimodal data\\ncomputations. This issue often leads to a computational\\nloss value of NaN, thereby causing failure in the initial\\nbatch forward propagation. Moreover, the limited number\\nof trainable parameters in these models may lead to gradient\\nvanishing during training. To mitigate these problems, as\\ndepicted in Figure 5 (c), we incorporate the post-norm and\\ninput norm mechanisms from LLaMA-2, applying RMS\\nNorm after each Multi-Head Attention Layer (MHA) to\\nnormalize data for downstream processing. In addition, we\\nhave to update all layer norms in the Phi-2 model to improve\\ntraining stability, as detailed in the subsequent equation.\\nLayerNorminput(xhidden) =γ xhidden − µ√\\nσ2 + ϵ\\n+ β (1)\\nWhere, xhidden is the input of this layer, µ and σ2 are the\\nmean and variance of the inputs to the layer, respectively,ϵ\\nis a small number to prevent division by zero, γ and β are\\ntrainable parameters.\\nRMSNorm(xpost) = xpostq\\n1\\nN\\nPN\\ni=1 x2\\ni + ϵ\\n(2)\\nwhere xpost is the input after MHA, N is the dimension of\\nxpost.\\nFurthermore, (Henry et al., 2020) have underscored the vital\\nrole of Query-Key Normalization in low-resource learning\\nscenarios. Hence, as show in Figure 5 (d), we have incor-\\nporated Query-Key Normalization into the Phi-2 model, as\\ndetailed in the following equation.\\nAttention(Q, K, V) =softmax\\n\\x12LayerNorm(Q)LayerNorm(K)T\\n√dk\\n\\x13\\nV\\n(3)\\nwhere dk denotes the dimension of Q or K.\\nThe structure of the LoRA mechanism (Hu et al., 2021)\\nis show in Figure 5 (a), which is an efficient fine-tuning\\nmethod in parallel to the frozen pre-training weights as\\nshown in Figure 5 (c), which does not increase the inference\\ntime consuming for large language models and is easier to\\noptimize.\\n3.2. Training Stages\\nIn this subsection, the four-stage training process of\\nTinyGPT-V will be described.\\nWarm-up training for the first training stage. During\\nthe initial pretraining stage, TinyGPT-V is taught vision-\\nlanguage understanding using large datasets of aligned\\n4'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 4}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nLinear\\nDown\\nr\\nLinear\\nUpPretrained\\nWeights\\n\\ud835\\nInput\\nOutput\\nLoRA\\n(a) LoRA\\nMLP\\nNormalization\\nMHA LoRA\\n(b) LoRA Module \\nfor LLMs Block\\nMHA\\nQuery-Key \\nNormalization\\nLayer Norm\\nRMS Norm\\nMLP\\nLoRA\\nAfter \\nStage1\\n(c) LLMs Block\\n for TinyGPT-V\\nAttention\\nQ K V\\nLayer \\nNorm\\nLayer \\nNorm\\n(d) Query-Key \\nNormalization for MHA\\nFreeze Train Data Pathway Conditional Pathway\\nFigure 5: (a) represents the structure of LoRA, (b) represents how LoRA can efficiently fine-tune large language models\\n(LLMs) in natural language processing, (c) represents the structure of LLMs for TinyGPT-V , and (d) represents the structure\\nof QK Normalization.\\nPhi-2\\nImage-text Pair \\nInstruction Learning\\nFreezeTrain\\nStage 1\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\nPhi-2\\nStage 2\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\n& LoRA\\nPhi-2\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\n& LoRA\\nStage 3\\nPhi-2\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\n& LoRA\\nMulti-Tasks Learning\\nStage 4\\nFigure 6: The training process of TinyGPT-V , the first stage\\nis warm-up training, the second stage is pre-training, the\\nthird stage is instruction fine-tuning, and the fourth stage is\\nmulti-task learning.\\nimage-text pairs. The model identifies the output from the\\nprojection layers as a soft prompt directing it to create rel-\\nevant texts and to allow large language models to accept\\ninputs from the image modality. The pretraining process\\nuses a dataset combination of Conceptual Caption, SBU,\\nand LAION, involving 20000 training steps covering about\\n5 million image-text pairs.\\nPre-training for the second training stage. Following the\\ninitial training stage, the large language model becomes\\nequipped to process image modality inputs. To guarantee\\nmore consistent performance as the model transitions into\\nthe subsequent training stage, we re-employ the dataset from\\nthe first stage, specifically for training the LoRA module.\\nInstruction tuning for the third training stage. We fine-\\ntuned this TinyGPT-V model using a selection of image-text\\npairings from MiniGPT4 or LLaV A, which included in-\\nstructions like “###Human: <Img><ImageHere></Img>\\nTake a look at this image and describe what you no-\\ntice.###Assistant:.”. We used a uniform template inclusive\\nof a randomly chosen prompt that improved the model’s\\ncapacity for generating responses that were consistent and\\nsounded more natural.\\nMulti-task learning in the fourth training stage. The\\nfourth training stage of TinyGPT-V focuses on enhancing\\nits conversation ability as a chatbot by tuning the model with\\nmore multi-modal instruction datasets as shown in Table 1,\\nincluding LLaV A, Flickr30k, a mixing multi-task dataset,\\nand Unnatural Instruction using multi-tasks template as de-\\ntailed in appendix A. The LLaV A dataset is utilized for\\nmulti-modal instruction tuning with detailed descriptions\\nand complex reasoning examples. The Flickr30k dataset\\nis used to improve grounded image caption generation and\\nobject parsing and grounding capabilities. Additionally, a\\nmixing multi-task dataset is created to improve the model’s\\nhandling of multiple tasks during multi-round conversations.\\nFinally, to recover the language generation ability, the Un-\\nnatural Instruction dataset is added to the third-stage training\\nof TinyGPT-V .\\n5'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 5}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nData types Dataset Stage 1 Stage 2 Stage 3 Stage 4\\nImage-text pair LAION, CC3M, SBU ✓ ✓ ✗ ✗\\nInstruction tuning MiniGPT-4 Stage2 for CC & SBU ✗ ✗ ✓ ✗\\nCaption Text Captions, COCO Captions ✗ ✗ ✗ ✓\\nREC RefCOCO, RefCOCO+, RefCOCOg, Visual Genome ✗ ✗ ✗ ✓\\nVQA GQA, VQAv2, OK-VQA, AOK-VQA, OCR-VQA ✗ ✗ ✗ ✓\\nMultimodal instruction LLaV A dataset, Flickr30k, Multi-task conversation ✗ ✗ ✗ ✓\\nLangauge dataset Unnatural Instructions ✗ ✗ ✗ ✓\\nTable 1: The full list of datasets used by TinyGPT-MoE during training.\\nFigure 7: Changes in loss during the training stage of TinyGPT-V .\\n4. Experiments\\nIn this section, we describe the training and evaluation meth-\\nods in detail.\\n4.1. Training\\nExperimental setting. The experimental environment for\\nthis study was established with a single NVIDIA RTX 3090\\nGPU, equipped with a substantial 24GB of VRAM. The cen-\\ntral processing was handled by an AMD EPYC 7552 48-core\\nProcessor, offering 15 virtual CPUs. Memory allocation was\\nset at 80GB, ensuring sufficient capacity for handling large\\ndatasets. The software environment was standardized on\\nPyTorch version 2.0.0, with CUDA 11.8 support, facilitating\\noptimized tensor operations on the GPU.\\nTraining process. In our experimental process, we meticu-\\nlously orchestrated the training of our model through four\\ndistinct stages, each characterized by specific learning rate\\nstrategies and loss profiles, as shown in Figure 7.\\nStage 1: Spanning 17 epochs, with each epoch consisting\\nof 1000 iterations, we employed a dynamic learning rate\\napproach. The learning rate commenced at 1e-5 at the begin-\\nning of each epoch and gradually ascended to 1e-4 by the\\nepoch’s end. This pattern was consistently applied across all\\n17 epochs. The training loss exhibited a steady decline, start-\\ning from 7.152 and progressively tapering down to 2.620,\\nreflecting the model’s increasing proficiency in learning\\nfrom the data. The purpose of this stage is to be able to\\nmake the Phi-2 model in TinyGPT-V react in some way to\\nthe input of the imaging modality. The alignment of text\\nand image in the semantic space is done.\\nStage 2: Comprising 4 epochs, each with 5000 iterations,\\nthis stage introduced the “linear_warmup_cosine_lr“ (He\\net al., 2018; Goyal et al., 2018) learning rate schedule. We\\ninitiated a warmup phase of 5000 steps, where the learn-\\ning rate linearly increased from 1e-6 (warmup_lr) to 1e-4\\n(init_lr), followed by a cosine decay down to a minimum\\nlearning rate of 8e-5. This phase saw a consistent reduction\\nin loss, starting at 2.726 and culminating at 2.343. The\\npurpose of this stage is to enable the LoRA module to play\\na role in multimodal data, further reducing the model’s loss\\non image-text pairs and improving the model’s ability to\\nlearn from the data.\\nStage 3: This stage lasted for 5 epochs, each with 200\\niterations. We maintained the “linear_warmup_cosine_lr“\\nschedule, with a warmup phase of 200 steps. The learning\\nrate began at 1e-6, ascending to 3e-5 (init_lr), before decay-\\ning to 1e-5 (min_lr). The loss values reflected significant\\n6'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 6}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nimprovements, starting at 1.992 and reducing to 1.125. The\\npurpose of this stage is to allow TinyGPT-V to accept both\\nverbal and image modal inputs and produce responses to\\nthem. After this stage of training TinyGPT-V has been able\\nto perform most of the image answering tasks.\\nStage 4: The final stage stretched over 50 epochs, each\\ncomprising 1000 iterations. We adhered to the “lin-\\near_warmup_cosine_lr“ schedule with a 1000-step warmup\\nphase. The learning rate was initiated at 1e-6, reaching up\\nto 1e-5 (init_lr), and then experiencing a cosine decay to\\na minimum of 8e-5. The training loss values displayed a\\nconsistent downward trajectory, beginning at 2.720 and ulti-\\nmately reaching as low as 1.399. The purpose of this stage is\\nto allow TinyGPT-V to perform various tasks such as VQA\\nor VSR tasks at the same time, increasing the generalization\\nperformance of TinyGPT-V on multimodal tasks.\\n4.2. Evaluation\\nEvaluation datasets. GQA (Hudson & Manning, 2019)\\nis a dataset for real-world visual reasoning and composi-\\ntional question answering, featuring a powerful question en-\\ngine that generates 22 million diverse reasoning questions.\\nVSR (Liu et al., 2023b) comprises over 10k natural text-\\nimage pairs in English, encompassing 66 types of spatial\\nrelations. IconQA (Lu et al., 2021) with 107,439 questions\\naimed at challenging visual understanding and reasoning in\\nthe context of icon images, encompassing three sub-tasks\\n(multi-image-choice, multi-text-choice, and filling-in-the-\\nblank). VizWiz (Gurari et al., 2018) is a collection of more\\nthan 31,000 visual queries, each derived from a photo taken\\nby a visually impaired individual using a smartphone, ac-\\ncompanied by a vocalized question regarding the image,\\nand supplemented with 10 answers sourced from a crowd\\nfor each query. The Hateful Memes dataset (HM) (Kiela\\net al., 2021), developed by Facebook AI, is a comprehensive\\nmultimodal collection specifically designed for the detec-\\ntion of hateful content in memes, combining both image\\nand text elements, and comprises over 10,000 newly created\\nmultimodal examples.\\nVisual question answering results. As shown in Table 2,\\nit becomes evident that TinyGPT-V , a language model with\\nonly 2.8 billion parameters, exhibits notably competitive per-\\nformance across multiple benchmarks, closely rivaling mod-\\nels with nearly 13 billion parameters. Specifically, in the\\nVSR (Visual-Spatial Reasoning) zero-shot task, TinyGPT-V\\noutshines its counterparts by securing the highest score of\\n54.7%. This is particularly impressive considering its pa-\\nrameter size is approximately 4.6 times smaller than other\\nleading models such as BLIP-2, LLaV A, and InstructBLIP.\\nIn the GQA benchmark, while TinyGPT-V scores are 38.9%,\\nit lags behind the highest score achieved by InstructBLIP,\\nwhich is 49.5%. However, TinyGPT-V shows robust perfor-\\nTinyGPT-V and others answer example compare\\nUres [vqa] where should I hide in this room when playing hide and\\nseek\\nLLaV A-1.5hide behind the bookshelf\\nMiniGPT-v2behind couch\\nGPT-4V Behind the Couch\\nUnder the Table\\nInside the Bookshelf\\nBehind the Curtains\\nBehind the TV\\nTinyGPT-Vunder couch\\nFigure 8: Comparison of reasoning answers from different\\nModels. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\nmance in the IconVQ challenge, attaining a score of 44.7%,\\njust 0.1% short of InstructBLIP’s leading score of 44.8%.\\nSimilarly, in the VizWiz task, TinyGPT-V demonstrates\\ncommendable capabilities with a score of 37.8%, which,\\nis not only the highest but is notable given its reduced pa-\\nrameter count. In the context of the Hateful Memes (HM)\\ndataset, TinyGPT-V matches InstructBLIP’s top score of\\n57.5% with its own score of 54.0%, again underscoring its\\nefficiency and capacity to compete with models of larger\\nscales. Overall, TinyGPT-V’s performance across these di-\\nverse and challenging benchmarks is striking, especially\\nwhen considering its parameter efficiency\\n4.3. Qualitative Evaluation\\nThe comparative analysis revealed TinyGPT-V’s distinct\\nadvantage in delivering concise and accurate visual interpre-\\ntations. In the reasoning task to find a hiding spot during\\na game of hide and seek, TinyGPT-V demonstrated its su-\\nperior capability by providing a singular, viable suggestion:\\n’under couch’. This contrasts with other models that either\\noffered multiple options, some of which were incorrect as\\nindicated by the text in red (e.g., GPT-4V suggesting ’In-\\nside the Bookshelf’), or specified less practical hiding spots.\\nWhen asked about potential activities in an image with an\\nalligator, TinyGPT-V suggested a cautious response with-\\nout speculating beyond what was visible. In contrast, other\\nmodels, like LLaV A-1.5, provided extended narratives that\\nintroduced assumptions not directly inferred from the image.\\nSimilarly, in describing a soccer match scene, TinyGPT-V’s\\nresponse was succinct and focused on the key elements,\\navoiding the inaccuracies noted in MiniGPT-v2’s account,\\nwhich incorrectly identified multiple soccer balls on the\\n7'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 7}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nMethod LLM GQA VSR IconVQ VizWiz HM AverageParameters (zero-shot) (zero-shot) (zero-shot) (zero-shot)\\nFlamingo 9B - 31.8 - 28.8 57.0 39.20\\nIDEFICS (Laurençon et al., 2023) 7B - 38.4 - 35.5 - 37.05\\n65B - 45.2 - 36.0 - 39.60\\nBLIP-2 13B 41.0 50.9 40.6 19.6 53.7 41.16\\nLLaV A 13B 41.3 51.2 43.0 - - 45.17\\nInstructBLIP 13B 49.5 52.1 44.8 33.4 57.5 47.45\\nMiniGPT-4 13B - - 35.9 - - 35.90\\nBLIV A (Hu et al., 2023) 7B - - 44.8 31.4 55.6 41.15\\nLLaV A-Phi (Zhu et al., 2024) 2.8B - - 54.1 37.6 - 43.15\\nMoE-LLaV A (Lin et al., 2024)∗ 1.8B×4 61.5 - - 32.6 - 47.50\\nOurs\\nTinyGPT-V (Phi-2) 2.8B 38.9 54.7 44.7 37.8 54.0 46.02\\nTinyGPT-V (Phi-1.5) 1.3B 34.3 35.8 37.2 28.4 50.3 37.2\\nTable 2: Comparative performance of TinyGPT-V and other MLLMs across multiple visual question answering benchmarks.\\n∗It is worth noting that MoE-LLaV A is required 8xA100-80G for training.\\nMethod TinyGPT-V LLaV A MiniGPT-4\\nseconds per words 0.067 0.426 0.300\\ninference occupancy (8-bit) 5.6GB 22GB 23.5GB\\nTable 3: Comparison of inference time and inference occu-\\npancy about devices.\\npitch. These examples, as tabulated in Table 5 and Table 7,\\nillustrate TinyGPT-V’s superior performance in generating\\nbrief yet precise responses, underscoring its practicality for\\nrapid and reliable visual question answering. For efficient\\nevaluation, as shown in table 3, TinyGPT-V operates at\\nthe fastest pace, taking only 0.067 seconds to generate a\\nword, which suggests upper efficiency in processing speed\\ncompared to LLaV A and MiniGPT-4. On the other hand,\\nLLaV A exhibits a significantly slower word generation time\\nat 0.426 seconds per word, coupled with a higher memory\\noccupancy of 22GB. MiniGPT-4, with a generation time of\\n0.300 seconds per word and a memory usage of 23.5GB.\\n4.4. Ablation Study\\nAs shown in Table 4, the full TinyGPT-V model achieves\\nlow loss across all stages, but the removal of key modules\\nleads to significant training issues. Without the LoRA mod-\\nule, there’s a gradient vanish starting from Stage 3. Omitting\\nInput Layer Norm increases loss notably (to 2.839 in Stage\\n1) and causes gradient vanishing in Stage 4. Without RMS\\nNorm, the model sees an elevated loss in Stage 1 (2.747)\\nand faces early gradient vanishing in Stage 2. The absence\\nof QK Norm results in immediate gradient vanish. This data\\nclearly illustrates each module’s crucial role in preventing\\ngradient vanishing and maintaining low loss throughout the\\ntraining process.\\nMethod Stage 1 Loss Stage 2 Loss Stage 3 Loss Stage 4 LossTinyGPT-V 2.620 2.343 1.125 1.330w/o LoRA 2.620 - Gradient Vanish -w/o Input Layer Norm 2.839 2.555 1.344 Gradient Vanishw/o RMS Norm 2.747 Gradient Vanish - -w/o QK Norm Gradient Vanish - - -\\nTable 4: Importance of each module in TinyGPT-V at each\\nstage of training.\\nFurthermore, our reveal a notable trend: the smaller the\\nlarge language model used for transfer learning (particu-\\nlarly in transitioning from text-to-image modality), the more\\nchallenging the training process becomes. We observed a\\npronounced need for additional normalization layers to sta-\\nbilize the training, especially when scaling down from larger\\nmodels like Vicuna-13B to smaller ones like Phi-2 (2.7B),\\nPhi-1.5 (1.3B), and other small backbones as detailed in the\\nAppendix B.\\n5. Conclusion\\nIn this study, we introduce TinyGPT-V , a parameter-efficient\\nMLLMs tailored for a range of real-world vision-language\\napplications. Our model innovatively builds on the compact\\nyet powerful Phi-2 small language model framework. This\\napproach results in TinyGPT-V delivering exceptional out-\\ncomes in diverse benchmarks like visual question-answering\\nand referring expression comprehension while keeping com-\\nputational demands manageable. Remarkably, TinyGPT-V\\ncan be trained on a 24G GPU and deployed on an 8G device,\\ndemonstrating a significant advancement in creating cost-\\neffective, efficient, and potent MLLMs. This paper marks a\\ncontribution towards crafting smaller, yet robust multimodal\\nlanguage models for practical, real-world use cases. We\\nenvision that our work will catalyze further explorations\\ninto developing compact MLLMs for diverse applications.\\n8'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 8}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nReferences\\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\\nHasson, Y ., Lenc, K., Mensch, A., Millican, K., Reynolds,\\nM., et al. Flamingo: a visual language model for few-shot\\nlearning. Advances in Neural Information Processing\\nSystems, 35:23716–23736, 2022.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems, 33:\\n1877–1901, 2020.\\nCao, Y ., Kang, Y ., Wang, C., and Sun, L. Instruction mining:\\nWhen data mining meets large language model finetuning,\\n2023.\\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con-\\nceptual 12m: Pushing web-scale image-text pre-training\\nto recognize long-tail visual concepts, 2021.\\nChen, J., Guo, H., Yi, K., Li, B., and Elhoseiny, M. Visual-\\ngpt: Data-efficient adaptation of pretrained language mod-\\nels for image captioning. InProceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npp. 18030–18040, 2022.\\nChen, J., Zhu, D., Shen, X., Li, X., Liu, Z., Zhang, P., Krish-\\nnamoorthi, R., Chandra, V ., Xiong, Y ., and Elhoseiny, M.\\nMinigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478, 2023.\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y ., Wu, Z., Zhang,\\nH., Zheng, L., Zhuang, S., Zhuang, Y ., Gonzalez, J. E.,\\net al. Vicuna: An open-source chatbot impressing gpt-4\\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\\n(accessed 14 April 2023), 2023.\\nDai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,\\nW., Li, B., Fung, P., and Hoi, S. Instructblip: Towards\\ngeneral-purpose vision-language models with instruction\\ntuning, 2023.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805,\\n2018.\\nDufter, P., Schmitt, M., and Schütze, H. Position informa-\\ntion in transformers: An overview, 2021.\\nFang, Y ., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X.,\\nHuang, T., Wang, X., and Cao, Y . Eva: Exploring the\\nlimits of masked visual representation learning at scale,\\n2022.\\nGoyal, P., Dollár, P., Girshick, R., Noordhuis, P.,\\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and\\nHe, K. Accurate, large minibatch sgd: Training imagenet\\nin 1 hour, 2018.\\nGurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman,\\nK., Luo, J., and Bigham, J. P. Vizwiz grand challenge:\\nAnswering visual questions from blind people. In Pro-\\nceedings of the IEEE conference on computer vision and\\npattern recognition, pp. 3608–3617, 2018.\\nHe, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\\nBag of tricks for image classification with convolutional\\nneural networks, 2018.\\nHenry, A., Dachapally, P. R., Pawar, S., and Chen, Y . Query-\\nkey normalization for transformers, 2020.\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\\nWelbl, J., Clark, A., et al. Training compute-optimal\\nlarge language models. arXiv preprint arXiv:2203.15556,\\n2022.\\nHu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,\\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation\\nof large language models, 2021.\\nHu, W., Xu, Y ., Li, Y ., Li, W., Chen, Z., and Tu, Z. Bliva:\\nA simple multimodal llm for better handling of text-rich\\nvisual questions, 2023.\\nHudson, D. A. and Manning, C. D. Gqa: A new dataset for\\nreal-world visual reasoning and compositional question\\nanswering. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, pp. 6700–\\n6709, 2019.\\nJavaheripi, M., Bubeck, S., Abdin, M., Aneja, J., Bubeck,\\nS., Mendes, C. C. T., Chen, W., Giorno, A. D., Eldan,\\nR., Gopi, S., Gunasekar, S., Javaheripi, M., Kauff-\\nmann, P., Lee, Y . T., Li, Y ., Nguyen, A., de Rosa, G.,\\nSaarikivi, O., Salim, A., Shah, S., Santacroce, M.,\\nBehl, H. S., Kalai, A. T., Wang, X., Ward, R., Witte,\\nP., Zhang, C., and Zhang, Y . Phi-2: The surprising\\npower of small language models. https://www.\\nmicrosoft.com/en-us/research/blog/\\nphi-2-the-surprising-power-of-small-language-models/ ,\\n2023.\\nKazemzadeh, S., Ordonez, V ., Matten, M., and Berg, T.\\nReferitgame: Referring to objects in photographs of natu-\\nral scenes. In Proceedings of the 2014 conference on em-\\npirical methods in natural language processing (EMNLP),\\npp. 787–798, 2014.\\n9'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 9}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nKiela, D., Firooz, H., Mohan, A., Goswami, V ., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes.\\nAdvances in neural information processing systems, 33:\\n2611–2624, 2020.\\nKiela, D., Firooz, H., Mohan, A., Goswami, V ., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes,\\n2021.\\nLaurençon, H., Saulnier, L., Tronchon, L., Bekman, S.,\\nSingh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush,\\nA. M., Kiela, D., Cord, M., and Sanh, V . Obelics: An\\nopen web-scale filtered dataset of interleaved image-text\\ndocuments, 2023.\\nLi, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping\\nlanguage-image pre-training with frozen image encoders\\nand large language models, 2023a.\\nLi, Y ., Bubeck, S., Eldan, R., Giorno, A. D., Gunasekar,\\nS., and Lee, Y . T. Textbooks are all you need ii: phi-1.5\\ntechnical report, 2023b.\\nLin, B., Tang, Z., Ye, Y ., Cui, J., Zhu, B., Jin, P., Huang, J.,\\nZhang, J., Ning, M., and Yuan, L. Moe-llava: Mixture of\\nexperts for large vision-language models, 2024.\\nLin, T.-Y ., Maire, M., Belongie, S., Bourdev, L., Girshick,\\nR., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and\\nDollár, P. Microsoft coco: Common objects in context,\\n2015.\\nLiu, H., Li, C., Li, Y ., and Lee, Y . J. Improved baselines\\nwith visual instruction tuning, 2023a.\\nLiu, H., Li, C., Wu, Q., and Lee, Y . J. Visual instruction\\ntuning. arXiv preprint arXiv:2304.08485, 2023b.\\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y ., Zhang, W., Yu,\\nZ., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark\\nfor abstract diagram understanding and visual language\\nreasoning. arXiv preprint arXiv:2110.13214, 2021.\\nMao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L.,\\nand Murphy, K. Generation and comprehension of unam-\\nbiguous object descriptions. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 11–20, 2016.\\nOpenAI. Introducing chatgpt. https://openai.com/\\nblog/chatgpt, 2022.\\nOrdonez, V ., Kulkarni, G., and Berg, T. Im2text: Describing\\nimages using 1 million captioned photographs. In\\nShawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and\\nWeinberger, K. (eds.), Advances in Neural Information\\nProcessing Systems, volume 24. Curran Associates, Inc.,\\n2011. URL https://proceedings.neurips.\\ncc/paper_files/paper/2011/file/\\n5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.\\npdf.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\nwith human feedback. Advances in Neural Information\\nProcessing Systems, 35:27730–27744, 2022.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\\nSutskever, I., et al. Language models are unsupervised\\nmultitask learners. OpenAI blog, 1(8):9, 2019.\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\\nJ., Krueger, G., and Sutskever, I. Learning transferable\\nvisual models from natural language supervision, 2021.\\nSchuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk,\\nR., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and\\nKomatsuzaki, A. Laion-400m: Open dataset of clip-\\nfiltered 400 million image-text pairs, 2021.\\nSchwenk, D., Khandelwal, A., Clark, C., Marino, K., and\\nMottaghi, R. A-okvqa: A benchmark for visual ques-\\ntion answering using world knowledge. In European\\nConference on Computer Vision, pp. 146–162. Springer,\\n2022.\\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\\nceptual captions: A cleaned, hypernymed, image alt-text\\ndataset for automatic image captioning. In Gurevych,\\nI. and Miyao, Y . (eds.), Proceedings of the 56th An-\\nnual Meeting of the Association for Computational Lin-\\nguistics (Volume 1: Long Papers), pp. 2556–2565, Mel-\\nbourne, Australia, July 2018. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/P18-1238. URL\\nhttps://aclanthology.org/P18-1238.\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X.,\\nGuestrin, C., Liang, P., and Hashimoto, T. B. Stanford\\nalpaca: An instruction-following llama model, 2023.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\\nFuller, B., Gao, C., Goswami, V ., Goyal, N., Hartshorn,\\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\\nV ., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y .,\\nMao, Y ., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\\nI., Nie, Y ., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\\n10'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 10}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\\nXu, P., Yan, Z., Zarov, I., Zhang, Y ., Fan, A., Kambadur,\\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\\nand Scialom, T. Llama 2: Open foundation and fine-tuned\\nchat models, 2023.\\nTsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S.,\\nVinyals, O., and Hill, F. Multimodal few-shot learn-\\ning with frozen language models. Advances in Neural\\nInformation Processing Systems, 34:200–212, 2021.\\nWei, L., Jiang, Z., Huang, W., and Sun, L. Instructiongpt-\\n4: A 200-instruction paradigm for fine-tuning minigpt-4,\\n2023.\\nWorkshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick, E.,\\nIli´c, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon,\\nF., et al. Bloom: A 176b-parameter open-access multilin-\\ngual language model. arXiv preprint arXiv:2211.05100,\\n2022.\\nYang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and\\nWang, L. The dawn of lmms: Preliminary explorations\\nwith gpt-4v(ision), 2023.\\nYu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L.\\nModeling context in referring expressions. In Computer\\nVision–ECCV 2016: 14th European Conference, Amster-\\ndam, The Netherlands, October 11-14, 2016, Proceed-\\nings, Part II 14, pp. 69–85. Springer, 2016.\\nYuan, Z., Wang, X., Wang, K., Sun, L., and Ye, Y . Artgpt-\\n4: Towards artistic-understanding large vision-language\\nmodels with enhanced adapter, 2023.\\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V .,\\net al. Opt: Open pre-trained transformer language models.\\narXiv preprint arXiv:2205.01068, 2022.\\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z.,\\nZhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang,\\nH., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge\\nwith mt-bench and chatbot arena, 2023.\\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.\\nMinigpt-4: Enhancing vision-language understanding\\nwith advanced large language models. arXiv preprint\\narXiv:2304.10592, 2023.\\nZhu, Y ., Zhu, M., Liu, N., Ou, Z., Mou, X., and Tang,\\nJ. Llava-phi: Efficient multi-modal assistant with small\\nlanguage model, 2024.\\n11'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 11}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nA. Multi-task Instruction Template\\nTo reduce ambiguity in training a unified multimodal model for diverse tasks such as visual question answering, image\\ncaptioning, referring expression comprehension, generation, and object parsing and grounding, we employed task-specific\\ntokens from MiniGPT-v2 within a multitask instruction template. This template, derived from the LLaMA-2 conversation\\ntemplate (Touvron et al., 2023), includes a general input format comprising image features, a task identifier token, and an\\ninstruction input. We incorporated six distinct task identifiers, each associated with a particular task. For tasks necessitating\\nthe identification of spatial locations of referred objects, the model uses textual representations of bounding boxes with\\ncoordinates normalized between 0 and 100. Overall, MiniGPT-v2’s unique task-specific tokens enhance task disambiguation,\\nleading to more precise and effective task execution.\\nTinyGPT-V and others answer example compare\\nUres What might happen in this image in the next second\\nLLaV A-1.5 In the image, a man is sitting on a log next to a large alligator, and he is holding a camera. It is likely\\nthat the man is about to take a photograph or record a video of the alligator. The next second could\\ninvolve the man capturing the alligator’s behavior, appearance, or any other interesting moment.\\nThe alligator might also react to the man’s presence, such as moving or showing curiosity, which\\ncould make the captured image or video more engaging and memorable.\\nMiniGPT-v2 In the next second, the man might be startled or surprised by the close-up shot of the crocodile\\nin the water. This could cause him to jump back or react with caution, ensuring his safety in the\\npresence of the wild animal. The image captures a moment of wonder and curiosity, highlighting\\nthe beauty of the natural environment while also acknowledging the potential danger that comes\\nwith being so close to a wild animal.\\nGPT-4V In the next second, the man might quickly move away to maintain a safe distance from the alligator.\\nAlligators can be unpredictable, and it’s important to keep a safe distance from them at all times to\\navoid potential harm. The man seems to be taking a selfie with the alligator, which suggests that he\\nis close to the animal. This is generally not advisable as it can be dangerous. It’s always important\\nto respect wildlife and observe it from a distance that is safe for both the person and the animal.\\nTinyGPT-V The image shows a man holding a selfie camera next to a crocodile. This could be an unusual and\\nunexpected sight, as crocodiles are not typically associated with human activities like taking selfies.\\nlt’s important for the man to stay safe and avoid any potential risks posed by the crocodile.\\nTable 5: Comparison of prediction answers from different models. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\nB. Small Backbones for Transfer Learning\\nAs shown in Table 6, a striking pattern emerges from the data: smaller LLMs exhibit heightened sensitivity to the removal\\nof these modules, with a pronounced tendency towards training difficulties, such as gradient vanishing. For instance, the\\nabsence of LoRA in both Phi-1.5 and TinyLLaMA resulted in an immediate cessation of training progress post-Stage 1,\\nindicating a critical reliance on this module for sustaining training in smaller models. Similarly, the exclusion of QK Norm\\nled to gradient vanishing at the earliest stage across all smaller LLMs, underscoring its essential role in the initial phases\\n12'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 12}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nof training. Moreover, the sequential progression in training losses across stages for models without these modifications\\ndemonstrates a clear degradation in training efficiency and effectiveness. For example, the removal of Input Layer Norm and\\nRMS Norm not only heightened Stage 1 loss across Phi-1.5 and TinyLLaMA but also precipitated gradient vanishing in\\nlater stages, showcasing the compound impact of these modules on model stability and learning capability. This analysis\\nincontrovertibly highlights a fundamental challenge in training smaller LLMs for migration to MLLMs: the absence of\\nkey architectural and normalization modules severely impedes their training process, making them more prone to early\\ntraining halts and efficiency losses. The results underscore the necessity of these components in supporting the stability and\\ngradual learning progression of smaller LLMs, thus illuminating a pivotal consideration for developers aiming to optimize\\nthe training framework for seamless model.\\nLLM Stage 1 Loss Stage 2 Loss Stage 3 Loss Stage 4 Loss\\nPhi-2 (2.7B) 2.620 2.343 1.125 1.330\\nPhi-1.5 (1.3B) 3.420 3.043 1.525 1.730\\nw/o LoRA 3.420 - Gradient Vanish -\\nw/o Input Layer Norm 3.555 3.221 1.544 Gradient Vanish\\nw/o RMS Norm 3.557 Gradient Vanish - -\\nw/o QK Norm Gradient Vanish - - -\\nTinyLLaMA (1.1B) 3.529 3.053 1.371 1.830\\nw/o LoRA 3.529 - Gradient Vanish -\\nw/o Input Layer Norm 3.611 3.331 1.444 Gradient Vanish\\nw/o RMS Norm 3.607 Gradient Vanish - -\\nw/o QK Norm Gradient Vanish - - -\\nTable 6: Importance of each module and different LLMs in TinyGPT-V training framework at each stage of training.\\n13'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 13}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nTinyGPT-V and others answer example compare\\nUres What’s in the image?\\nLLaV A-1.5 The image shows a group of men playing soccer on a field. They are actively engaged in the game,\\nwith some players running towards the soccer ball.\\nMiniGPT-v2 The image features a lively and competitive soccer game with several players on the field. Some of\\nthe players are wearing jerseys, while others have their own unique outfits. There are at least eight\\nplayers in total, with three of them being the main focus of the scene.\\nOne player is kicking a soccer ball, with another player nearby trying to block the shot. A third\\nplayer is also present, playing offense. In addition, there are two soccer balls visible on the field,\\none closer to the left side and the other near the center of the scene.\\nThe players are engaged in intense action, showcasing their skills and determination to score. The\\natmosphere appears to be a high-energy match with a mix of excitement and competition among\\nthe players.\\nGPT-4V The image shows a moment from a soccer match. In the foreground, there’s a player in a blue\\nuniform who appears to be dribbling the ball with close control, evading two opponents in white\\nuniforms. The player in blue is crouched low to the ground, demonstrating agility and focus, which\\nsuggests he is in the midst of making a strategic play, possibly attempting to bypass the defenders. In\\nthe background, other players can be observed, and the stands are filled with spectators, indicating\\nthat this is a professional match likely taking place in a large stadium. The attire and hairstyles\\nsuggest the photo may be from a past era, possibly the 1980s or 1990s.\\nTinyGPT-V The image shows a socer game with several people playing, including at least one person in a blue\\nshirt. The players are engaged in a friendly match, showcasing their skills and teamwork.\\nTable 7: Comparison of descriptive answers from different Models. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\n14')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 0}, page_content='TinyGPT-V: Efficient Multimodal Large Language Model\\nvia Small Backbones\\nZhengqing Yuan1 Zhaoxu Li 2 Weiran Huang3 Yanfang Ye1 Lichao Sun 4\\nAbstract\\nIn recent years, multimodal large language mod-\\nels (MLLMs) such as GPT-4V have demonstrated\\nremarkable advancements, excelling in a variety\\nof vision-language tasks. Despite their prowess,\\nthe closed-source nature and computational de-\\nmands of such models limit their accessibility and\\napplicability. This study introduces TinyGPT-V ,\\na novel open-source MLLM, designed for effi-\\ncient training and inference across various vision-\\nlanguage tasks, including image captioning (IC)\\nand visual question answering (VQA). Leverag-\\ning a compact yet powerful architecture, TinyGPT-\\nV integrates the Phi-2 language model with pre-\\ntrained vision encoders, utilizing a unique map-\\nping module for visual and linguistic information\\nfusion. With a training regimen optimized for\\nsmall backbones and employing a diverse dataset\\namalgam, TinyGPT-V requires significantly lower\\ncomputational resources—24GB for training and\\nas little as 8GB for inference—without compro-\\nmising on performance. Our experiments demon-\\nstrate that TinyGPT-V , with its language model\\n2.8 billion parameters, achieves comparable re-\\nsults in VQA and image inference tasks to its\\nlarger counterparts while being uniquely suited\\nfor deployment on resource-constrained devices\\nthrough innovative quantization techniques. This\\nwork not only paves the way for more accessible\\nand efficient MLLMs but also underscores the po-\\ntential of smaller, optimized models in bridging\\nthe gap between high performance and computa-\\ntional efficiency in real-world applications. Addi-\\ntionally, this paper introduces a new approach to\\nmultimodal large language models using smaller\\nbackbones. Our code and training weights are\\navailable in the supplementary material.\\n1Universiy of Notre Dame 2Nanyang Technological University\\n3Shanghai Jiao Tong University 4Lehigh University. Correspon-\\ndence to: Lichao Sun <lis221@lehigh.edu>.\\nAccepted to the Workshop on Advancing Neural Network Training\\nat International Conference on Machine Learning (W ANT@ICML\\n2024).\\n5.07.510.012.515.0\\nParameter (Billions)\\n38\\n40\\n42\\n44\\n46Performance Flamingo\\nBLIP-2\\nLLaVA\\nInstructBLIP\\nMiniGPT-4\\nTinyGPT-V\\nFigure 1: Comparison of TinyGPT-V with other current\\nMLLMs models shows TinyGPT-V achieves cost-effective,\\nefficient, and high-performing with fewer parameters.\\n1. Introduction\\nIn recent years, the field of artificial intelligence has seen\\nsignificant advancements through the development of multi-\\nmodal large language models (MLLMs), such as GPT-4V ,\\nwhich have shown exceptional performance across a range\\nof vision-language tasks (Yang et al., 2023). Despite GPT-\\n4V’s impressive capabilities, its closed-source nature limits\\nits widespread application and adaptability. In contrast, the\\nopen-source landscape for MLLMs is rapidly evolving, pre-\\nsenting models like LLaV A and MiniGPT-4 that excel in\\nimage captioning (IC), visual question answering (VQA)\\noften comparable GPT-4V in these areas (Dai et al., 2023;\\nLiu et al., 2023a;b; Zhu et al., 2023). Notably, MiniGPT-\\nv2 (Chen et al., 2023) has demonstrated superior perfor-\\nmance in various visual grounding and question-answering\\ntasks. However, its training code remains proprietary, which\\nposes challenges for community-driven advancements and\\nadaptability.\\nAlthough the impressive vision-language capabilities\\ndemonstrated by some open-source MLLMs, they fre-\\nquently necessitate significant computational resources for\\ntraining and inference. For example, training LLaV A-v1.5-\\n13B (Liu et al., 2023a) required 8 × A100 GPUs, each\\nequipped with 80GB of memory, cumulating in 25.5 hours\\nof continuous training. As shown in Figure 2 (a), the un-\\n1\\narXiv:2312.16862v3  [cs.CV]  21 Jun 2024')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Extract image as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rapidocr-onnxruntime\n",
      "  Downloading rapidocr_onnxruntime-1.4.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pyclipper>=1.2.0 (from rapidocr-onnxruntime)\n",
      "  Downloading pyclipper-1.3.0.post6-cp311-cp311-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: opencv-python>=4.5.1.48 in c:\\users\\ngchn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rapidocr-onnxruntime) (4.10.0.82)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.19.5 in c:\\users\\ngchn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rapidocr-onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: six>=1.15.0 in c:\\users\\ngchn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rapidocr-onnxruntime) (1.16.0)\n",
      "Collecting Shapely!=2.0.4,>=1.7.1 (from rapidocr-onnxruntime)\n",
      "  Downloading shapely-2.0.6-cp311-cp311-win_amd64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\ngchn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rapidocr-onnxruntime) (6.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ngchn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rapidocr-onnxruntime) (9.4.0)\n",
      "Collecting onnxruntime>=1.7.0 (from rapidocr-onnxruntime)\n",
      "  Downloading onnxruntime-1.20.1-cp311-cp311-win_amd64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ngchn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rapidocr-onnxruntime) (4.66.5)\n",
      "Collecting coloredlogs (from onnxruntime>=1.7.0->rapidocr-onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\ngchn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: packaging in c:\\users\\ngchn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (24.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ngchn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (3.20.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\ngchn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ngchn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->rapidocr-onnxruntime) (0.4.6)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ngchn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime)\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading rapidocr_onnxruntime-1.4.3-py3-none-any.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.9 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 4.2/14.9 MB 16.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 8.7/14.9 MB 19.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.1/14.9 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/14.9 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 16.4 MB/s eta 0:00:00\n",
      "Downloading onnxruntime-1.20.1-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 3.7/11.3 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.2/11.3 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.3/11.3 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.9/11.3 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 9.1 MB/s eta 0:00:00\n",
      "Downloading pyclipper-1.3.0.post6-cp311-cp311-win_amd64.whl (110 kB)\n",
      "Downloading shapely-2.0.6-cp311-cp311-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 6.9 MB/s eta 0:00:00\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: pyclipper, Shapely, pyreadline3, humanfriendly, coloredlogs, onnxruntime, rapidocr-onnxruntime\n",
      "Successfully installed Shapely-2.0.6 coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.20.1 pyclipper-1.3.0.post6 pyreadline3-3.5.4 rapidocr-onnxruntime-1.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\ngchn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\ngchn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\ngchn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# !pip install rapidocr-onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader(url, extract_images=True)\n",
    "docs = pdf_loader. load()\n",
    "len (docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Load UnStructured PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "pdf_loader = PDFMinerLoader(url)\n",
    "docs = pdf_loader. load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Load PDF from dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "path_dir = \"./data_source\"\n",
    "pdf_loader = PyPDFDirectoryLoader(path_dir)\n",
    "docs = pdf_loader. load()\n",
    "len(docs) # 36 pages from 2 pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 0}, page_content='MiniGPT-v2: Large Language Model As a Unified\\nInterface for Vision-Language Multi-task Learning\\nJun Chen1,2∗\\nDeyao Zhu1 Xiaoqian Shen1 Xiang Li1 Zechun Liu2 Pengchuan Zhang2\\nRaghuraman Krishnamoorthi2 Vikas Chandra2 Yunyang Xiong2†\\nMohamed Elhoseiny1†\\n1King Abdullah University of Science and Technology (KAUST)\\n2Meta AI Research\\nAbstract\\nLarge language models have shown their remarkable capabilities as a general\\ninterface for various language-related applications. Motivated by this, we target\\nto build a unified interface for completing many vision-language tasks including\\nimage description, visual question answering, and visual grounding, among others.\\nThe challenge is to use a single model for performing diverse vision-language\\ntasks effectively with simple multi-modal instructions. Towards this objective, we\\nintroduce MiniGPT-v2, a model that can be treated as a unified interface for better\\nhandling various vision-language tasks. We propose using unique identifiers for\\ndifferent tasks when training the model. These identifiers enable our model to better\\ndistinguish each task instruction effortlessly and also improve the model learning\\nefficiency for each task. After the three-stage training, the experimental results\\nshow that MiniGPT-v2 achieves strong performance on many visual question-\\nanswering and visual grounding benchmarks compared to other vision-language\\ngeneralist models. Our model and codes are available at https://minigpt-v2.\\ngithub.io/.\\n1 Introduction\\nMulti-modal Large Language Models (LLMs) have emerged as an exciting research topic with a\\nrich set of applications in vision-language community, such as visual AI assistant, image captioning,\\nvisual question answering (VQA), and referring expression comprehension (REC). A key feature\\nof multimodal large language models is that they can inherit advanced capabilities (e.g., logical\\nreasoning, common sense, and strong language expression) from the LLMs [32, 49, 50, 8]. When\\ntuned with proper vision-language instructions, multi-modal LLMs, specifically vision-language\\nmodels, demonstrate strong capabilities such as producing detailed image descriptions, generating\\ncode, localizing the visual objects in the image, and even performing multi-modal reasoning to better\\nanswer complicated visual questions [ 59, 26, 55, 53, 7, 10, 58, 6, 60]. This evolution of LLMs\\nenables interactions of visual and language inputs across communication with individuals and has\\nbeen shown quite effective for building visual chatbots.\\nHowever, learning to perform multiple vision-language tasks effectively and formulating their corre-\\nsponding multi-modal instructions present considerable challenges due to the complexities inherent\\namong different tasks. For instance, given a user input “tell me the location of a person\" , there\\nare many ways to interpret and respond based on the specific task. In the context of the referring\\nexpression comprehension task, it can be answered with one bounding box location of the person.\\nFor the visual question-answering task, the model might describe their spatial location using human\\nnatural language. For the person detection task, the model might identify every spatial location of\\n∗Work partially done during the internship at Meta AI\\n†Equal last author\\narXiv:2310.09478v3  [cs.CV]  7 Nov 2023'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 1}, page_content='OKVQA\\nGQA\\nVSR\\nIconQA\\nVizWiz\\nHM\\nRefCOCO test-B\\nRefCOCO+ test-B\\nRefCOCOg test\\n33\\n45\\n57\\n33\\n45\\n5737\\n49\\n61\\n38\\n44\\n50\\n23\\n37\\n51\\n555759\\n79\\n82\\n85\\n68\\n71\\n74\\n78\\n81\\n84\\nInstructBLIP (13B)\\nLLaVA (13B)\\nMiniGPT-4 (13B)\\nShikra (13B)\\nMiniGPT-v2 (7B)\\nMiniGPT-v2 Chat (7B)\\nFigure 1: Our MiniGPT-v2 achieves state-of-the-art performances on a broad range of vision-language\\ntasks compared with other generalist models.\\neach human in a given image. To alleviate this issue and towards a unified approach, we propose\\na task-oriented instruction training scheme to reduce the multi-modal instructional ambiguity, and\\na vision-language model, MiniGPT-v2. Specifically, we provide a unique task identifier token for\\neach task. For example, we provide a [vqa] identifier token for training all the data samples from the\\nvisual question answering tasks. In total, we provide six different task identifiers during the model\\ntraining stages.\\nOur model, MiniGPT-v2, has a simple architecture design. It directly takes the visual tokens from\\na ViT vision encoder [12] and project them into the feature space of a large language model [ 50].\\nFor better visual perception, we utilize higher-resolution images (448x448) during training. But\\nthis will result in a larger number of visual tokens. To make the model training more efficient, we\\nconcatenate every four neighboring visual tokens into a single token, reducing the total number by\\n75%. Additionally, we utilize a three-stage training strategy to effectively train our model with a\\nmixture of weakly-labeled, fine-grained image-text datasets, and multi-modal instructional datasets,\\nwith different training focus at each stage.\\nTo evaluate the performance of our model, we conducted extensive experiments on diverse vision-\\nlanguage tasks, including (detailed) image/grounded captioning, vision question answering, and\\nvisual grounding. The results demonstrate that our MiniGPT-v2 can achieve SOTA or comparable\\nperformance on diverse benchmarks compared to previous vision-language generalist models, such\\nas MiniGPT-4 [59], InstructBLIP [10], LLaV A [26] and Shikra [7]. For example, our MiniGPT-v2\\noutperforms MiniGPT-4 by 21.3%, InstructBLIP by 11.3%, and LLaV A by 11.7% on the VSR\\nbenchmark [25], and it also performs better than the previously established strong baseline, Shikra,\\nin most validations on RefCOCO, RefCOCO+, and RefCOCOg. Our model establishes new state-\\nof-the-art results on these benchmarks among vision-language generalist models, shown in Fig.\\n1.\\n2 Related Work\\nWe briefly review relevant works on advanced large language models and multi-modal LLMs for\\nvisual aligning.\\nAdvanced Large Language Models (LLMs).Early-stage models such as GPT-2 [38] and BERT [11]\\nare foundation models trained on web-scale text datasets, marking a breakthrough in the NLP field.\\nFollowing the success of foundation models, LLMs with higher capacity and increased training\\ndata are developed, including GPT-3 [ 4], Megatron-turing NLG [ 46], PaLM [ 9], Gopher [ 39],\\n2'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 2}, page_content='Chinchilla [16], OPT [ 57], and BLOOM [ 41]. Most recently, the efforts have been focused on\\nrefining LLMs to work effectively with human instruction and feedback. Representative works in\\nthis direction are InstructGPT [34] and ChatGPT [32], which demonstrate strong capabilities such\\nas answering a diverse range of language questions, engaging in conversations with humans, and\\nlearning to perform complex tasks like writing refinement and coding assistant.\\nConcurrent with these advancements of LLMs is the rise of LLaMA [ 49] language models. To\\nenable human instruction following abilities similar to ChatGPT, some works attempt to finetune\\nthe LLaMA model with additional high-quality instruction datasets [1]. Examples of these models\\ninclude Alpaca [47], Vicuna [ 8], and MPT [48]. Some other open-sourced language models that\\nlearned from the human feedback data, such as Falcon [ 35] and LLaMA-2 [ 50], have also been\\nintroduced to the NLP community with impressive performance.\\nVisual Aligning with LLMs. With the remarkable generalization abilities of LLMs, interesting\\nstudies have extended LLMs to multi-modal domains by aligning visual inputs with LLMs. Early\\nworks such as VisualGPT [5] and Frozen [51] used pre-trained language models to improve vision-\\nlanguage models on image captioning and visual question answering. This initial exploration paved\\nthe way for subsequent vision-language research such as Flamingo [ 2] and BLIP-2 [ 22]. More\\nrecently, GPT-4 has been released and demonstrates many advanced multi-modal abilities, e.g.,\\ngenerating website code based on handwritten text instructions. Those demonstrated capabilities\\ninspired other vision-language LLMs, including MiniGPT-4 [59] and LLaV A [26], which align the\\nimage inputs with a large language model, Vicuna [ 8], using proper instructional tuning. These\\nvision-language models also showcase many advanced multi-modal capabilities after the alignment.\\nRecent works, such as Vision-LLM [ 53], Kosmos-2 [ 36], Shikra [ 7], and our concurrent work,\\nQwen-VL [3], also demonstrate that multi-model LLMs models can also perform visual grounding\\nby generating the text format of bounding boxes through language model.\\n3 Method\\nViT\\n…Linear\\nLlama 2\\n… </Img> [refer] where is the left ear? [/INST][INST] <Img>\\n…\\nconcat\\n{<30><12><49><40>}\\nFigure 2: Architecture of MiniGPT-v2. The\\nmodel takes a ViT visual backbone, which remains\\nfrozen during all training phases. We concatenate\\nfour adjacent visual output tokens from ViT back-\\nbone and project them into LLaMA-2 language\\nmodel space via a linear projection layer.\\nWe start by introducing our vision-language\\nmodel, MiniGPT-v2, then discuss the basic idea\\nof a multi-task instruction template with task\\nidentifiers for training, and finally adapt our task\\nidentifier idea to achieve task-oriented instruc-\\ntion tuning.\\n3.1 Model Architecture\\nOur proposed model architecture, MiniGPT-v2,\\nis shown in Fig. 2. It consists of three com-\\nponents: a visual backbone, a linear projection\\nlayer, and a large language model. We describe\\neach component as follows:\\nVisual backbone. MiniGPT-v2 adapts the\\nEV A [12] as our visual backbone model back-\\nbone. We freeze the visual backbone during\\nthe entire model training. We train our model\\nwith the image resolution 448x448, and we in-\\nterpolate the positional encoding to scale with a\\nhigher image resolution.\\nLinear projection layer. We aim to project all the visual tokens from the frozen vision backbone\\ninto the language model space. However, for higher-resolution images such as 448x448, projecting\\nall the image tokens results in a very long-sequence input (e.g., 1024 tokens) and significantly lowers\\nthe training and inference efficiency. Hence, we simply concatenate 4 adjacent visual tokens in the\\nembedding space and project them together into one single embedding in the same feature space\\nof the large language model, thus reducing the number of visual input tokens by 4 times. With this\\noperation, our MiniGPT-v2 can process high-resolution images much more efficiently during the\\ntraining and inference stage.\\n3'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 3}, page_content='Large language model. MiniGPT-v2 adopts the open-sourced LLaMA2-chat (7B) [ 50] as the\\nlanguage model backbone. In our work, the language model is treated as a unified interface for\\nvarious vision-language inputs. We directly rely on the LLaMA-2 language tokens to perform\\nvarious vision-language tasks. For the visual grounding tasks that necessitate the generation of spatial\\nlocations, we directly ask the language model to produce textual representations of bounding boxes\\nto denote their spatial positions.\\n3.2 Multi-task Instruction Template\\nWhen training a single unified model for multiple different tasks such as visual question answering,\\nimage caption, referring expression, grounded image caption, and region identification, the multi-\\nmodal model might fail to distinguish each task by just aligning visual tokens to language models.\\nFor instance, when you ask “Tell me the spatial location of the person wearing a red jacket?”, the\\nmodel can either respond you the location in a bounding box format (e.g., < Xleft >< Ytop ><\\nXright >< Ybottom >) or describe the object location using natural language (e.g., upper right\\ncorner). To reduce such ambiguity and make each task easily distinguishable, we introduce task-\\nspecific tokens in our designed multi-task instruction template for training. We now describe our\\nmulti-task instruction template in more details.\\nGeneral input format. We follow the LLaMA-2 conversation template design and adapt it for the\\nmulti-modal instructional template. The template is denoted as follows,\\n[INST] <Img> <ImageFeature> </Img> [Task Identifier] Instruction [/INST]\\nIn this template, [INST] is considered as the user role, and [/INST] is considered as the assistant role.\\nWe structure the user input into three parts. The first part is the image features, the second part is the\\ntask identifier token, and the third part is the instruction input.\\nTask identifier tokens. Our model takes a distinct identifier for each task to reduce the ambiguity\\nacross various tasks. As illustrated in Table 1, we have proposed six different task identifiers for visual\\nquestion answering, image caption, grounded image captioning, referring expression comprehension,\\nreferring expression generation, and phrase parsing and grounding respectively. For vision-irrelevant\\ninstructions, our model does not use any task identifier token.\\nTasks VQA Caption Grounded Caption REC REG Object Parsing and Grounding\\nIdentifiers [vqa] [caption] [grounding] [refer] [identify] [detection]\\nTable 1: Task identifier tokens for 6 different tasks, including visual question answering, image cap-\\ntioning, grounded image captioning, referring expression comprehension (REC), referring expression\\ngeneration (REG), and object parsing and grounding (where the model extracts objects from the input\\ntext and determines their bounding box locations).\\nSpatial location representation. For tasks such as referring expression comprehension (REC),\\nreferring expression generation (REG), and grounded image captioning, our model is required\\nto identify the spatial location of the referred objects accurately. We represent the spatial location\\nthrough the textual formatting of bounding boxes in our setting, specifically: “{< Xleft >< Ytop ><\\nXright >< Ybottom >}\". Coordinates for X and Y are represented by integer values normalized in\\nthe range [0,100]. < Xleft > and < Ytop > denote the x and y coordinate top-left corner of the\\ngenerated bounding box, and < Xright > and < Ybottom > denote the x and y coordinates of the\\nbottom-right corner.\\n3.3 Multi-task Instruction Training\\nWe now adapt our designed multi-task instruction template for instruction training. The basic\\nidea is to take instruction with task-specific identifier token as input for task-oriented instruction\\ntraining of MiniGPT-v2. When input instructions have task identifier tokens, our model will become\\nmore prone to multiple-task understanding during training. We train our model with task identifier\\ninstructions for better visual aligment in three stages. The first stage is to help MiniGPT-v2 build\\nbroad vision-language knowledge through many weakly-labeled image-text datasets, and high-quality\\nfine-grained vision-language annotation datasets as well (where we will assign a high data sampling\\nratio for weakly-labeled image-text datasets). The second stage is to improve the model with only\\n4'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 4}, page_content='fine-grained data for multiple tasks. The third stage is to finetune our model with more multi-modal\\ninstruction and language datasets for answering diverse multi-modal instructions better and behaving\\nas a multi-modal chatbot. The datasets used for training at each stage are listed in Table 2.\\nData types Dataset Stage 1 Stage 2 Stage 3\\nWeakly-labeled GRIT-20M (REC and REG), LAION, CC3M, SBU ✓ ✗ ✗\\nGrounded caption GRIT-20M ✓ ✗ ✗\\nCaption COCO caption, Text Captions ✓ ✓ ✓\\nREC RefCOCO, RefCOCO+, RefCOCOg, Visual Genome ✓ ✓ ✓\\nREG RefCOCO, RefCOCO+, RefCOCOg ✓ ✓ ✓\\nVQA GQA, VQAv2, OCR-VQA, OK-VQA, AOK-VQA ✓ ✓ ✓\\nMultimodal instruction LLaV A dataset, Flickr30k, Multi-task conversation ✗ ✗ ✓\\nLangauge dataset Unnatural Instructions ✗ ✗ ✓\\nTable 2: The training datasets used for our model three-stage training.\\nStage 1: Pretraining. To have broad vision-language knowledge, our model is trained on a mix of\\nweakly-labeled and fine-grained datasets. We give a high sampling ratio for weakly-labeled datasets\\nto gain more diverse knowledge in the first-stage.\\nFor the weakly-labeled datasets, we use LAION [42], CC3M [44], SBU [33], and GRIT-20M from\\nKosmos v2 [ 36] that built the dataset for referring expression comprehension (REC), referring\\nexpression generation (REG), and grounded image captioning.\\nFor fine-grained datasets, we use datasets like COCO caption [ 24] and Text Captions [ 45] for\\nimage captioning, RefCOCO [20], RefCOCO+ [56], and RefCOCOg [29] for REC. For REG, we\\nrestructured the data from ReferCOCO and its variants, reversing the order from phrase → bounding\\nboxes to bounding boxes → phrase. For VQA datasets, our training takes a variety of datasets, such\\nas GQA [19], VQA-v2 [14], OCR-VQA [31], OK-VQA [30], and AOK-VQA [43].\\nStage 2: Multi-task training. To improve the performance of MiniGPT-v2 on each task, we only\\nfocus on using fine-grained datasets to train our model at this stage. We exclude the weakly-supervised\\ndatasets such as GRIT-20M and LAION from stage-1 and update the data sampling ratio according\\nto the frequency of each task. This strategy enables our model to prioritize high-quality aligned\\nimage-text data for superior performance across various tasks.\\nStage 3: Multi-modal instruction tuning. Subsequently, we focus on tuning our model with more\\nmulti-modal instruction datasets and enhancing its conversation ability as a chatbot. We continue\\nusing the datasets from the second stage and add instructional datasets, including LLaV A [ 26],\\nFlickr30k dataset [37], our constructed mixing multi-task dataset, and the language dataset, Unnatural\\nInstruction [17]. We give a lower data sampling ratio for the fine-grained datasets from stage-2 and a\\nhigher data sampling ratio for the new instruction datasets.\\n– LLaV A instruction data.We add the multi-modal instruction tuning datasets, including the detailed\\ndescriptions and complex reasoning from LLaV A [26], with 23k and 58k data examples respectively.\\n– Flicker 30k. After the second-stage training, our MiniGPT-v2 can effectively generate the grounded\\nimage caption. Nevertheless, these descriptions tend to be short and often cover very few number of\\nvisual objects. This is because the GRIT-20M dataset from KOSMOS-v2 [36] that our model was\\ntrained with, features a limited number of grounded visual objects in each caption, and our model\\nlacks proper multi-modal instruction tuning to teach it to recognize more visual objects. To improve\\nthis, we fine-tune our model using the Flickr30k dataset [ 37], which provides more contextual\\ngrounding of entities within its captions.\\nWe prepare the Flickr30k dataset in two distinct formats for training our model to perform grounded\\nimage caption and a new task “object parsing and grounding\":\\n1) Grounded image caption. We select captions with a minimum of five grounded phrases, containing\\naround 2.5k samples, and we directly instruct the model to produce the grounded image caption. e.g.,\\na <p>wooden table</p>{<Xleft ><Ytop><Xright><Ybottom>} in the center of the room.\\n2) Object parsing and grounding. This new task is to parse all the objects from an input caption\\nand then ground each object. To enable this, we use the task identifier[detection] to differentiate this\\ncapability from other tasks. Also, we use Flickr30k to construct two types of instruction datasets:\\n5'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 5}, page_content='Method Grounding OKVQA GQA VSR IconVQA VizWiz HM\\n(zero-shot) (zero-shot) (zero-shot) (zero-shot)\\nFlamingo-9B ✗ 44.7 - 31.8 - 28.8 57.0\\nBLIP-2 (13B) ✗ 45.9 41.0 50.9 40.6 19.6 53.7\\nInstructBLIP (13B) ✗ - 49.5 52.1 44.8 33.4 57.5\\nMiniGPT-4 (13B) ✗ 37.5 30.8 41.6 37.6 - -\\nLLaV A (13B) ✗ 54.4 41.3 51.2 43.0 - -\\nShikra (13B) ✓ 47.2 - - - - -\\nOurs (7B) ✓ 56.9 60.3 60.6 47.7 32.9 58.2\\nOurs (7B)-chat ✓ 57.8 60.1 62.9 51.5 53.6 58.8\\nTable 3: Results on multiple VQA tasks. We report top-1 accuracy for each task. Grounding column\\nindicates whether the model incorporates visual localization capability. The best performance for\\neach benchmark is indicated in bold.\\nMethod Model types RefCOCO RefCOCO+ RefCOCOg Avgval test-A test-B val test-A test-B val test\\nUNINEXT Specialist models 92.64 94.33 91.46 85.24 89.63 79.79 88.73 89.37 88.90\\nG-DINO-L 90.56 93.19 88.24 82.75 88.95 75.92 86.13 87.02 86.60\\nVisionLLM-H\\nGeneralist models\\n- 86.70 - - - - - - -\\nOFA-L 79.96 83.67 76.39 68.29 76.00 61.75 67.57 67.58 72.65\\nShikra (7B) 87.01 90.61 80.24 81.60 87.36 72.12 82.27 82.19 82.93\\nShikra (13B) 87.83 91.11 81.81 82.89 87.79 74.41 82.64 83.16 83.96\\nOurs (7B) 88.69 91.65 85.33 79.97 85.12 74.45 84.44 84.66 84.29\\nOurs (7B)-chat 88.06 91.29 84.30 79.58 85.52 73.32 84.19 84.31 83.70\\nTable 4: Results on referring expression comprehension tasks. Our MiniGPT-v2 outperforms\\nmany VL-generalist models including VisionLLM [53], OFA [52] and Shikra [7] and reduces the\\naccuracy gap comparing to specialist models including UNINEXT [54] and G-DINO [27].\\ncaption→ grounded phrases and phrase → grounded phrase, each containing around 2.5k and 3k\\nsamples. Then we prompt our model with the instruction: [detection] description, the model will\\ndirectly parse the objects from the input image description and also ground the objects into bounding\\nboxes.\\n– Mixing multi-task dataset. After extensive training with single-round instruction-answer pairs,\\nthe model might not handle multiple tasks well during multi-round conversations since the context\\nbecomes more complex. To alleviate this situation, we create a new multi-round conversation dataset\\nby mixing the data from different tasks. We include this dataset into our third-stage model training.\\n– Unnatural instruction. The conversation abilities of language model can be reduced after extensive\\nvision-language training. To fix this, we add the language dataset, Unnatural Instruction [17] into our\\nmodel’s third-stage training for helping recover the language generation ability.\\n4 Experiments\\nIn this section, we present experimental settings and results. We primarily conduct experiments\\non (detailed) image/grounded captioning, vision question answering, and visual grounding tasks,\\nincluding referring expression comprehension. We present both quantitative and qualitative results.\\nImplementation details. Throughout the entire training process, the visual backbone of MiniGPT-v2\\nremains frozen. We focus on training the linear projection layer and efficient finetuning the language\\nmodel using LoRA [ 18]. With LoRA, we finetune Wq and Wv via low-rank adaptation. In our\\nimplementation, we set the rank, r = 64. We trained the model with an image resolution of 448x448\\nduring all stages. During each stage, we use our designed multi-modal instructional templates for\\nvarious vision-language tasks during the model training.\\nTraining and hyperparameters. We use AdamW optimizer with a cosine learning rate scheduler\\nto train our model. In the initial stage, we train on 8xA100 GPUs for 400,000 steps with a global\\nbatch size of 96 and an maximum learning rate of 1e-4. This stage takes around 90 hours. During the\\nsecond stage, the model is trained for 50,000 steps on 4xA100 GPUs with a maximum learning rate\\n6'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 6}, page_content='OKVQA GQA WizViz VSR IconVQA HM Average\\nOurs w/o task identifier 50.5 53.4 28.6 57.5 44.8 56.8 48.6\\nOurs 52.1 54.6 29.4 59.9 45.6 57.4 49.8\\nTable 5: Task identifier ablation study on VQA benchmarks. With task identifier during the model\\ntraining can overall improve VQA performances from multiple VQA benchmarks\\nMethod CHAIR I ↓ CHAIRS ↓ Len\\nMiniGPT-4 9.2 31.5 116.2\\nmPLUG-Owl 30.2 76.8 98.5\\nLLaV A 18.8 62.7 90.7\\nMultiModal-GPT 18.2 36.2 45.7\\nMiniGPT-v2 (long) 8.7 25.3 56.5\\nMiniGPT-v2 (grounded) 7.6 12.5 18.9\\nMiniGPT-v2 (short) 4.4 7.1 10.3\\nTable 6: Results on hallucination. We evaluate the hallucination of MiniGPT-v2 with different\\ninstructional templates and output three versions of captions for evaluation. For the “long\" version,\\nwe use the prompt generate a brief description of the given image. For the “grounded\" version, the\\ninstruction is [grounding] describe this image in as detailed as possible. For the “short\" version, the\\nprompt is [caption] briefly describe the image.\\nof 1e-5, adopting a global batch size of 64, and this training stage lasts roughly 20 hours. For the last\\nstage, training is executed for another 35,000 steps on 4xA100 GPUs, using a global batch size of 24\\nand this training stage took around 7 hours, maintaining the same maximum learning rate of 1e-5.\\n4.1 Quantitative Evaluation\\nDataset and evaluation metrics. We evaluate our model across a range of VQA and visual grounding\\nbenchmarks. For VQA benchmarks, we consider OKVQA [43], GQA [19], visual spatial reasoning\\n(VSR) [25], IconVQA [28], VizWiz [15], HatefulMemes and (HM) [21]. For visual grounding, we\\nevaluate our model on RefCOCO [20] and RefCOCO+[56], and RefCOCOg[29] benchmarks.\\nTo evaluate VQA benchmarks, we use an open-ended approach with a greedy decoding strategy. We\\nevaluate each VQA question with the following instruction template: “[vqa] question\". Following\\nthe previous method [10], we evaluate the performance by matching the model’s response to the\\nground-truth and reporting top-1 accuracy. For visual grounding benchmarks, we use the template\\n“[refer] give me the location of Referring expression\" for each referring expression comprehension\\nquestion, and a predicted bounding box is considered as correct for reporting accuracy if its IOU\\nbetween prediction and ground-truth is higher than 0.5.\\nVisual question answering results. Table 3 presents our experimental results on multiple VQA\\nbenchmarks. Our results compare favorably to baselines including MiniGPT-4 [ 59], Shikra [ 7],\\nLLaV A [26], and InstructBLIP [10] across all the VQA tasks. For example, on QKVQA, our MiniGPT-\\nv2 outperforms MiniGPT-4, Shikra, LLaV A, and BLIP-2 by 20.3%, 10.6%, 3.4%, and 11.9%. These\\nresults indicate the strong visual question answering capabilities of our model. Furthermore, we find\\nthat our MiniGPT-v2 (chat) variant shows higher performance than the version trained after the second\\nstage. On OKVQA, VSR, IconVQA, VizWiz, and HM, MiniGPT-v2 (chat) outperforms MiniGPT-v2\\nby 0.9%, 2.3%, 4.2%, 20.7%, and 0.6%. We believe that the better performance can be attributed to\\nthe improved language skills during the third-stage training, which is able to benefit visual question\\ncomprehension and response, especially on VizWiz with 20.7% top-1 accuracy increase.\\nReferring expression comprehension results. Table 4 compares our model to baselines on REC\\nbenchmarks. Our MiniGPT-v2 shows strong REC performance on RefCOCO, RefCOCO+, and Ref-\\nCOCOg, performing better than other vision-language generalist models. MiniGPT-v2 outperforms\\nOFA-L [52] by over 8% accuracy across all tasks of RefCOCO/RefCOCO+/RefCOCOg. Compared\\nwith a strong baseline, Shikra (13B) [ 7], our model still shows better results, e.g., 84.29% vs 83.96%\\naccuracy in average. These results provide direct evidence for the competing visual grounding\\n7'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 7}, page_content=\"[identify] this \\n{<35><45><65><70>} is \\na black chainring Cut slice of fruit cake on a \\nplate with a fork and a \\ncup of coffee with flowers \\nin a vase\\n[grounding] please \\ndescribe this image as \\ndetailed as possible\\n[refer] the right player's hat \\nWho are the people on the \\nright, on the left, and in the \\nmiddle? \\nThe people in the image \\nare:\\n* Barack Obama, the \\nformer President of the \\nUnited States, is on the \\nleft side of the image.\\n* Joe Biden, the current \\nPresident of the United \\nStates, is in the middle of \\nthe image.\\n* Donald Trump, the \\nformer President of the \\nUnited States, is on the \\nright side of the image.\\nThe image depicts a large cave with a \\ngroup of people painted on the walls. \\nThe people are engaged in various \\nactivities, such as hunting, gathering, \\nand riding horses. There are several \\nhorses scattered throughout the scene, \\nwith some in the foreground and others \\nin the background.\\nIn addition to the people and horses, \\nthere are several other animals depicted \\nin the cave, including a lion, a tiger, and \\na dinosaur. The cave is also adorned \\nwith various objects, such as a bow and \\narrows, a spear, and a shield.\\nOverall, the scene appears to be a \\nvibrant and lively depiction of ancient \\nlife, showcasing the skills and activities \\nof the people who lived in this cave.\\nPlease describe this image as \\ndetailed as possible\\n[vqa] Which country does it \\ncome from? \\naustralia\\na living room\\na lamp\\na spacious gray sofa\\ntwo wall paintings\\nshoes\\na blanket \\n[detection] The image \\nshowcases a living room \\nfeaturing a lamp, a \\nspacious gray sofa, two \\nwall paintings, and shoes \\nresting on a blanket\\na) b) c)\\nd)\\ne)\\nf)\\ng)\\nFigure 3: Examples for various multi-modal capabilities of MiniGPT-v2. We showcase that our\\nmodel is capable of completing multiple tasks such as referring expression comprehension, referring\\nexpression generation, detailed grounded image caption, visual question answering, detailed image\\ndescription, and directly parsing phrase and grounding from a given input text.\\ncapabilities of MiniGPT-v2. Although our model underperforms specialist models, the promising\\nperformance indicates its growing competence in visual grounding.\\nAblation on task identifier. We conduct ablation studies on the effect of the task identifier on the\\nperformance of MiniGPT-v2. We compare our model with the variant without using task identifiers\\non VQA benchmarks. Both models were trained on 4xA100 GPUs for 24 hours with an equal number\\nof training steps for multiple vision-language tasks. Results in Table 5 demonstrate the performance\\non multiple VQA benchmarks and consistently show that token identifier training benefits the overall\\nperformance of MiniGPT-v2. Specifically, our MiniGPT-v2 with task-oriented instruction training\\nachieves 1.2% top-1 accuracy improvement on average. These ablation results can validate the clear\\nadvantage of adding task identifier tokens and support the use of multi-task identifiers for multi-task\\nlearning efficiency.\\n8\"),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 8}, page_content='Hallucination. We measure the hallucination of our model on image description generation and\\ncompare the results with other vision-language baselines, including MiniGPT-4 [ 59], mPLUG-\\nOwl [55], LLaV A [26], and MultiModal-GPT [13]. Following the methodology from [23], we use\\nCHAIR [40] to assess hallucination at both object and sentence levels. As shown in Table 6, we find\\nthat our MiniGPT-v2 tends to generate the image description with reduced hallucination compared to\\nother baselines. We have evaluated three types of prompts in MiniGPT-v2. First, we use the prompt\\ngenerate a brief description of the given image without any specific task identifier which tends to\\nproduce more detailed image descriptions. Then we provide the instruction prompt [grounding]\\ndescribe this image in as detailed as possible for evaluating grounded image captions. Lastly, we\\nprompt our model with [caption] briefly describe the image. With these task identifiers, MiniGPT-v2\\nis able to produce a variety of image descriptions with different levels of hallucination. As a result,\\nall these three instruction variants have lower hallucination than our baseline, especially with the task\\nspecifiers of [caption] and [grounding].\\n4.2 Qualitative Results\\nWe now provide the qualitative results for a complementary understanding of our model’s multi-modal\\ncapabilities. Some examples can be seen in Fig. 3. Specifically, we demonstrated various abilities\\nin the examples including a) object identification; b) detailed grounded image captioning; c) visual\\nquestion answering; d) referring expression comprehension; e) visual question answering under\\ntask identifier; f) detailed image description; g) object parsing and grounding from an input text.\\nMore qualitative results can be found in the Appendix. These results demonstrate that our model has\\ncompeting vision-language understanding capabilities. Moreover, notice that we train our model only\\nwith a few thousand of instruction samples on object parsing and grounding tasks at the third-stage,\\nand our model can effectively follow the instructions and generalize on the new task. This indicates\\nthat our model has the flexibility to adapt on many new tasks.\\nNote that our model still occasionally shows hallucinations when generating the image description\\nor visual grounding. e.g., our model may sometimes produce descriptions of non-existent visual\\nobjects or generate inaccurate visual locations of grounded objects. We believe training with more\\nhigh-quality image-text aligned data and integrating with a stronger vision backbone or large language\\nmodel hold the potential for alleviating this issue.\\n5 Conclusion\\nIn this paper, we introduce MiniGPT-v2, a multi-modal LLM that can serve as a unified interface\\nfor various vision-language multi-tasking learning. To develop a single model capable of handling\\nmultiple vision-language tasks, we propose using distinct identifiers for each task during the training\\nand inference. These identifiers help our model easily differentiate various tasks and also improve\\nlearning efficiency. Our MiniGPT-v2 achieves state-of-the-art results across many visual question\\nanswering and referring expression comprehension benchmarks. We also found that our model can\\nefficiently adapt to new vision-language tasks, which suggests that MiniGPT-v2 has many potential\\napplications in the vision-language community.\\n9'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 9}, page_content='References\\n[1] Sharegpt. https://github.com/domeccleston/sharegpt, 2023.\\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\\nfew-shot learning. In Advances in Neural Information Processing Systems, 2022.\\n[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and\\nJingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint\\narXiv:2308.12966, 2023.\\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\\nAdvances in neural information processing systems, 33:1877–1901, 2020.\\n[5] Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. Visualgpt: Data-efficient adaptation\\nof pretrained language models for image captioning. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, pages 18030–18040, 2022.\\n[6] Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, and Mohamed Elhoseiny. Video chatcaptioner:\\nTowards the enriched spatiotemporal descriptions. arXiv preprint arXiv:2304.04227, 2023.\\n[7] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\\nmultimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023.\\n[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source\\nchatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\\n[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\\n[10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\\nLi, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with\\ninstruction tuning, 2023.\\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n[12] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\\nand Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. arXiv preprint\\narXiv:2211.07636, 2022.\\n[13] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei\\nZhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans.\\narXiv preprint arXiv:2305.04790, 2023.\\n[14] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings of the\\nIEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017.\\n[15] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P\\nBigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the\\nIEEE conference on computer vision and pattern recognition, pages 3608–3617, 2018.\\n[16] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\\nlarge language models. arXiv preprint arXiv:2203.15556, 2022.\\n[17] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language\\nmodels with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.\\n[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\\n2021.\\n[19] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and\\ncompositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 6700–6709, 2019.\\n[20] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects\\nin photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural\\nlanguage processing (EMNLP), pages 787–798, 2014.\\n[21] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and\\nDavide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances\\nin neural information processing systems, 33:2611–2624, 2020.\\n[22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\\n[23] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object\\nhallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.\\n[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014:\\n13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages\\n740–755. Springer, 2014.\\n10'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 10}, page_content='[25] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for\\nComputational Linguistics, 11:635–651, 2023.\\n[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\\narXiv:2304.08485, 2023.\\n[27] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\\nHang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object\\ndetection. arXiv preprint arXiv:2303.05499, 2023.\\n[28] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and\\nSong-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language\\nreasoning. arXiv preprint arXiv:2110.13214, 2021.\\n[29] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.\\nGeneration and comprehension of unambiguous object descriptions. InProceedings of the IEEE conference\\non computer vision and pattern recognition, pages 11–20, 2016.\\n[30] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question\\nanswering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on\\ncomputer vision and pattern recognition, pages 3195–3204, 2019.\\n[31] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual\\nquestion answering by reading text in images. In 2019 international conference on document analysis and\\nrecognition (ICDAR), pages 947–952. IEEE, 2019.\\n[32] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022.\\n[33] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned\\nphotographs. Advances in neural information processing systems, 24, 2011.\\n[34] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.\\n[35] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza\\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon\\nllm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116,\\n2023.\\n[36] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:\\nGrounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023.\\n[37] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana\\nLazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence\\nmodels. In Proceedings of the IEEE international conference on computer vision, pages 2641–2649, 2015.\\n[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n[39] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods,\\nanalysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\\n[40] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination\\nin image captioning. arXiv preprint arXiv:1809.02156, 2018.\\n[41] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\\nCastagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter\\nopen-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\\n[42] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush\\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400\\nmillion image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\\n[43] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-\\nokvqa: A benchmark for visual question answering using world knowledge. In European Conference on\\nComputer Vision, pages 146–162. Springer, 2022.\\n[44] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556–2565,\\n2018.\\n[45] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image\\ncaptioningwith reading comprehension. 2020.\\n[46] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper,\\nZhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to\\ntrain megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990,\\n2022.\\n[47] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.\\ncom/tatsu-lab/stanford_alpaca, 2023.\\n[48] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms,\\n2023. Accessed: 2023-05-05.\\n[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\\n11'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 11}, page_content='language models. arXiv preprint arXiv:2302.13971, 2023.\\n[50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n[51] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal\\nfew-shot learning with frozen language models. Advances in Neural Information Processing Systems ,\\n34:200–212, 2021.\\n[52] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\\nZhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-\\nsequence learning framework. In International Conference on Machine Learning, pages 23318–23340.\\nPMLR, 2022.\\n[53] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie\\nZhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric\\ntasks. arXiv preprint arXiv:2305.11175, 2023.\\n[54] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance\\nperception as object discovery and retrieval. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pages 15325–15336, 2023.\\n[55] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\\nPengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with\\nmultimodality. arXiv preprint arXiv:2304.14178, 2023.\\n[56] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in\\nreferring expressions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The\\nNetherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69–85. Springer, 2016.\\n[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\\narXiv preprint arXiv:2205.01068, 2022.\\n[58] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, and Mohamed Elhoseiny.\\nChatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. arXiv preprint\\narXiv:2303.06594, 2023.\\n[59] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\\n2023.\\n[60] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, Róbert Csordás, Anand Gopalakrishnan,\\nAbdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. Mindstorms in\\nnatural language-based societies of mind. arXiv preprint arXiv:2305.17066, 2023.\\n12'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 12}, page_content='A Appendix\\nIn the supplementary, we provide more qualitative results that are generated from our model to\\ndemonstrate the vision-language multi-tasking capabilities.\\nA.1 Instruction template for various vision-language tasks\\nRefCOCO/RefCOCO+/RefCOCOg: [refer] give me the location of question\\nVizWiz: [vqa] Based on the image, respond to this question with a single word or phrase: question,\\nand reply ’unanswerable’ when the provided information is insufficient\\nHateful Meme: [vqa] This is an image with: question written on it. Is it hateful? Answer:\\nVSR: [vqa] Based on the image, is this statement true or false? question\\nIconQA, GQA, OKVQA: [vqa] Based on the image, respond to this question with a single word or\\nphrase: question\\nA.2 Additional Qualitative Results\\nTo study how well our model is able to take visual input and answer questions based on task-oriented\\nidentifier, we use our model to perform multiple vision-language tasks including grounded image\\ncaptioning in Fig. 4, Fig. 5, Fig. 6 and Fig. 7; Object parsing and grounding in Fig. 8, Fig. 9, Fig. 10\\nand Fig. 11; Referring expression comprehension in Fig. 12, Fig. 13, Fig. 14 and Fig. 15; Object\\nidentification in Fig. 16, Fig. 17, Fig. 18 and Fig. 19.\\nFor each task, we share 4 examples for showing the vision-language capabilities of our model. The\\nresults in the demo provide direct evidence for the competing visual understanding capabilities of\\nMiniGPT-v2 on multiple vision-language tasks. For example, in the cases of grounded caption, our\\nmodel is able to give correct grounded image caption with detailed spatial locations of objects. In the\\ncases of identify, the model also generates our expected object names. MiniGPT-v2 can understand\\nthe new scenes and follow the question identifier to respond. But we also need to note that our model\\nstill has some hallucination e.g., In Fig. 6, several persons are not grounded accurately, and in Fig. 7,\\nthere does not exist a vase in the image.\\nFigure 4: Detail grounded image caption example.\\n13'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 13}, page_content='Figure 5: Detail grounded image caption example\\nFigure 6: Detail grounded image caption example\\n14'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 14}, page_content='Figure 7: Detail grounded image caption example\\nFigure 8: Object parsing and grounding example\\n15'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 15}, page_content='Figure 9: Object parsing and grounding example\\nFigure 10: Object parsing and grounding example\\n16'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 16}, page_content='Figure 11: Object parsing and grounding example\\nFigure 12: Referring expression comprehension example\\nFigure 13: Referring expression comprehension example\\n17'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 17}, page_content='Figure 14: Referring expression comprehension example\\nFigure 15: Referring expression comprehension example\\n18'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 18}, page_content='Figure 16: object identification example\\nFigure 17: object identification example\\nFigure 18: object identification example\\n19'),\n",
       " Document(metadata={'source': 'data_source\\\\2310.09478v3.pdf', 'page': 19}, page_content='Figure 19: object identification example\\n20'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 0}, page_content='TinyGPT-V: Efficient Multimodal Large Language Model\\nvia Small Backbones\\nZhengqing Yuan1 Zhaoxu Li 2 Weiran Huang3 Yanfang Ye1 Lichao Sun 4\\nAbstract\\nIn recent years, multimodal large language mod-\\nels (MLLMs) such as GPT-4V have demonstrated\\nremarkable advancements, excelling in a variety\\nof vision-language tasks. Despite their prowess,\\nthe closed-source nature and computational de-\\nmands of such models limit their accessibility and\\napplicability. This study introduces TinyGPT-V ,\\na novel open-source MLLM, designed for effi-\\ncient training and inference across various vision-\\nlanguage tasks, including image captioning (IC)\\nand visual question answering (VQA). Leverag-\\ning a compact yet powerful architecture, TinyGPT-\\nV integrates the Phi-2 language model with pre-\\ntrained vision encoders, utilizing a unique map-\\nping module for visual and linguistic information\\nfusion. With a training regimen optimized for\\nsmall backbones and employing a diverse dataset\\namalgam, TinyGPT-V requires significantly lower\\ncomputational resources—24GB for training and\\nas little as 8GB for inference—without compro-\\nmising on performance. Our experiments demon-\\nstrate that TinyGPT-V , with its language model\\n2.8 billion parameters, achieves comparable re-\\nsults in VQA and image inference tasks to its\\nlarger counterparts while being uniquely suited\\nfor deployment on resource-constrained devices\\nthrough innovative quantization techniques. This\\nwork not only paves the way for more accessible\\nand efficient MLLMs but also underscores the po-\\ntential of smaller, optimized models in bridging\\nthe gap between high performance and computa-\\ntional efficiency in real-world applications. Addi-\\ntionally, this paper introduces a new approach to\\nmultimodal large language models using smaller\\nbackbones. Our code and training weights are\\navailable in the supplementary material.\\n1Universiy of Notre Dame 2Nanyang Technological University\\n3Shanghai Jiao Tong University 4Lehigh University. Correspon-\\ndence to: Lichao Sun <lis221@lehigh.edu>.\\nAccepted to the Workshop on Advancing Neural Network Training\\nat International Conference on Machine Learning (W ANT@ICML\\n2024).\\n5.07.510.012.515.0\\nParameter (Billions)\\n38\\n40\\n42\\n44\\n46Performance Flamingo\\nBLIP-2\\nLLaVA\\nInstructBLIP\\nMiniGPT-4\\nTinyGPT-V\\nFigure 1: Comparison of TinyGPT-V with other current\\nMLLMs models shows TinyGPT-V achieves cost-effective,\\nefficient, and high-performing with fewer parameters.\\n1. Introduction\\nIn recent years, the field of artificial intelligence has seen\\nsignificant advancements through the development of multi-\\nmodal large language models (MLLMs), such as GPT-4V ,\\nwhich have shown exceptional performance across a range\\nof vision-language tasks (Yang et al., 2023). Despite GPT-\\n4V’s impressive capabilities, its closed-source nature limits\\nits widespread application and adaptability. In contrast, the\\nopen-source landscape for MLLMs is rapidly evolving, pre-\\nsenting models like LLaV A and MiniGPT-4 that excel in\\nimage captioning (IC), visual question answering (VQA)\\noften comparable GPT-4V in these areas (Dai et al., 2023;\\nLiu et al., 2023a;b; Zhu et al., 2023). Notably, MiniGPT-\\nv2 (Chen et al., 2023) has demonstrated superior perfor-\\nmance in various visual grounding and question-answering\\ntasks. However, its training code remains proprietary, which\\nposes challenges for community-driven advancements and\\nadaptability.\\nAlthough the impressive vision-language capabilities\\ndemonstrated by some open-source MLLMs, they fre-\\nquently necessitate significant computational resources for\\ntraining and inference. For example, training LLaV A-v1.5-\\n13B (Liu et al., 2023a) required 8 × A100 GPUs, each\\nequipped with 80GB of memory, cumulating in 25.5 hours\\nof continuous training. As shown in Figure 2 (a), the un-\\n1\\narXiv:2312.16862v3  [cs.CV]  21 Jun 2024'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 1}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nProjection (110M)\\n0.8%\\nEVA-VIT (0.98B)\\n7.0%\\nVicuna (13B)\\n92.3%\\nProjection (119M)\\n3.1%\\nEVA-VIT (0.98B)\\n25.1%\\nPhi-2 (2.8B)\\n71.8%\\n(a) MiniGPT-4 (a) TinyGPT-V\\nFigure 2: (a) is the occupancy ratio of each component in MiniGPT-4, and (b) is the occupancy ratio of each component in\\nTinyGPT-V . We have considerably narrowed down the occupancy ratio of the Language model in MLLMs.\\nderlying performance of large language models, which are\\nintegral to MLLMs, is pivotal. Models such as LLaV A-\\nv1.5-13B and MiniGPT-v2 depend on high-capacity back-\\nbones which are Vicuna-13b-v1.5 (Zheng et al., 2023) and\\nLLaMA2-7B-Chat (Touvron et al., 2023), respectively, ne-\\ncessitating a substantial number of parameters to effectively\\ntackle complex tasks including IC and VQA.\\nWe introduce TinyGPT-V , a novel model designed for effi-\\ncient training and inference, requiring only 24GB of GPU\\nmemory for training and as little as 8GB of GPU or CPU\\nmemory for inference. This model makes use of the ad-\\nvanced large language model Phi-2 (Javaheripi et al., 2023)\\nand incorporates pre-trained vision modules from (Li et al.,\\n2023a) and CLIP (Radford et al., 2021) as its vision encoder,\\ncoupled with a mapping module to facilitate the integration\\nof visual information. During training, TinyGPT-V adopts\\na novel training methodology focused on small pre-trained\\nbackbones, unlike any other MLLMs, utilizing the unique\\nmapping module between the visual encoder and the lan-\\nguage model as well as novelty normalization methods,\\nwhile keeping all other components frozen. For its train-\\ning dataset, TinyGPT-V employs the multi-tasks datasets,\\nincluding LAION (Schuhmann et al., 2021), Conceptual\\nCaptions (Changpinyo et al., 2021; Sharma et al., 2018),\\nSBU (Ordonez et al., 2011), and others (Lin et al., 2015;\\nSchwenk et al., 2022; Hudson & Manning, 2019; Kiela et al.,\\n2020; Lu et al., 2021; Gurari et al., 2018; Mao et al., 2016;\\nKazemzadeh et al., 2014; Yu et al., 2016).\\nIn our study, we found that TinyGPT-V exhibits similar traits\\nwith GPT-4, especially when doing some VQA and image\\ninference. With only 2.8 billion parameters of its language\\nmodel, TinyGPT-V employs a unique quantization process,\\nlike using 8-bit quantization, making it well-suited for local\\ndeployment and inference on 8GB mobile devices. This\\nmodel represents a significant advancement in achieving a\\nbalance between exceptional performance and efficiency in\\nMLLMs, as shown in Figure 1. Our work not only aims\\nto enable the community to develop more cost-effective,\\nefficient, and high-performing MLLMs for widespread real-\\nworld applications but also introduces a training framework\\noptimized for small pre-trained backbones.\\n2. Related Work\\nAdvanced language model. The evolution of language\\nmodels has been marked by significant milestones, start-\\ning with early successes like GPT2 (Radford et al., 2019)\\nand BERT (Devlin et al., 2018) in natural language process-\\ning (NLP). These foundational models set the stage for the\\nsubsequent development of vastly larger language models,\\nencompassing hundreds of billions of parameters. This dra-\\nmatic increase in scale has led to the emergence of advanced\\ncapabilities as seen in models like GPT-3 (Brown et al.,\\n2020), Chinchilla (Hoffmann et al., 2022), OPT (Zhang\\net al., 2022), and BLOOM (Workshop et al., 2022). These\\nlarge language models (LLMs) have been instrumental\\nin further advancements in the field. For instance, Chat-\\nGPT (OpenAI, 2022) and InstructGPT (Ouyang et al., 2022)\\nleverage these powerful models to answer diverse ques-\\ntions and perform complex tasks such as coding. The in-\\ntroduction of open-source LLMs like LLaMA (Touvron\\net al., 2023) has further propelled research in this area, in-\\nspiring subsequent developments like Alpaca (Taori et al.,\\n2023), Vicuna (Chiang et al., 2023). These models fine-tune\\nthe LLaMA model with additional high-quality instruction\\ndatasets, showcasing the versatility and adaptability of LLM\\nframeworks.Among the most notable recent advancements\\nare Phi (Li et al., 2023b) and its successor, Phi-2 (Javaheripi\\net al., 2023). These models have demonstrated exceptional\\nperformance, rivaling or even surpassing models up to 25\\ntimes larger in scale. This indicates a significant shift in\\n2'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 2}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nFigure 3: Compared to other general-purpose MLLMs, our TinyGPT-V achieves the same performance as 13B or 7B models\\nin a variety of visual language tasks.\\nthe landscape of language modeling, emphasizing efficiency\\nand effectiveness without necessarily relying on sheer size.\\nMultimodal language model. In recent years, the trend of\\naligning visual input to large language models for vision-\\nlanguage tasks has gained significant attention (Chen et al.,\\n2022; Tsimpoukelli et al., 2021; Alayrac et al., 2022; Li\\net al., 2023a; Liu et al., 2023b;a; Zhu et al., 2023; Chen\\net al., 2023). Seminal works like VisualGPT (Chen et al.,\\n2022) and Frozen (Tsimpoukelli et al., 2021), which uti-\\nlized pre-trained language models for image captioning and\\nvisual question answering. This approach was further ad-\\nvanced by models such as Flamingo (Alayrac et al., 2022),\\nwhich incorporated gated cross-attention mechanisms to\\nalign pre-trained vision encoders and language models, train-\\ning on vast image-text pairs. BLIP-2 (Li et al., 2023a)\\nintroduced an efficient Q-Former for aligning visual and\\nlanguage modalities. These groundbreaking studies have\\npaved the way for further innovations in the field, leading to\\nthe development of models like LLaV A (Liu et al., 2023b)\\nand MiniGPT4 (Zhu et al., 2023), and their subsequent iter-\\nations, LLaV A-v1.5 (Liu et al., 2023a), MiniGPT-v2 (Chen\\net al., 2023), ArtGPT-4 (Yuan et al., 2023), instruction GPT-\\n4 (Wei et al., 2023) and Instruction Mining (Cao et al.,\\n2023). These models have demonstrated advanced multi-\\nmodal capabilities through instruction tuning, showcasing\\nremarkable generalization abilities.\\n3. Method\\nWe briefly introduce our vision-language model, TinyGPT-\\nV , followed by an analysis of its structure, culminating in a\\ndetailed description of the training process for each stage.\\n3.1. Model Architecture\\nIn this subsection, we present the architecture of TinyGPT-V ,\\nwhich consists of a visual encoder, projection layers, and a\\nlarge language model, as shown in Figure 4.\\nVisual encoder backbone. In the TinyGPT-V , it utilizes\\nEV A (Fang et al., 2022) of the ViT serves as the visual\\nfoundation model, which remains inactive during the entire\\ntraining process. Our model operates at an image resolution\\nof 224x224 for Stages 1, 2, and 3, and at 448x448 for Stage\\n4. The positional encoding is enhanced to accommodate the\\nincreased image resolution which is known as the Relative\\nPosition Bias (Dufter et al., 2021). It enhances the model’s\\nunderstanding of the spatial relationships between elements\\nin an image.\\nProjection layers. The Projection layers embed visual\\nfeatures extracted by the visual encoder into the language\\nmodel, enhancing the model’s ability to process image-\\nbased information. We adopt the Q-Former layers from\\nthe BLIP-2 architecture (Li et al., 2023a) as the initial pro-\\n3'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 3}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nPhi-2\\nLoRA+Normalization\\nFreeze Train\\nQ-Former\\nViT\\nLinear 1 (MiniGPT-4)\\nLinear 2\\nThis is an image of an alpaca. \\nAlpacas are domesticated \\nspecies of South American \\ncamelids, known for their \\nsoft fluffy coats which are \\nused for making wool.\\n[vqa]What’s this?\\nFigure 4: Architecture of TinyGPT-V . The model takes a\\nvisual backbone, which remains frozen during all training\\nphases. We concatenate Q-Former layer visual output tokens\\nfrom ViT backbone and project them into Phi-2 language\\nmodel space via two linear projection layers.\\njection layer, aiming to leverage the full potential of the\\npre-trained BLIP system within visual language models.\\nThis strategy significantly reduces the number of parameters\\nneeding training. The second and third layers are linear\\nprojection layers, designed to bridge the dimensionality gap\\nbetween the Q-Former output and the language model’s\\nembedding layer, thereby aligning visual tokens more effec-\\ntively with the language model’s hidden space. As shown in\\nFigure 6, to expedite TinyGPT-V’s training, we initially use\\na pre-trained Linear Projection from MiniGPT-4 (Vicuna\\n7B) as the second layer. We then introduce an additional lin-\\near projection layer, initialized with a Gaussian distribution,\\nas the third layer to seamlessly integrate into the hidden\\nspace of the Phi-2 model.\\nLarge lanuguage model backbone. Our TinyGPT-V large\\nlanguage model is built upon the Phi-2 model (Javaheripi\\net al., 2023) as its backbone. Phi-2, a 2.7 billion-parameter\\nlanguage model, exhibits exceptional reasoning and lan-\\nguage comprehension abilities, achieving state-of-the-art\\nperformance among language models with fewer than 13\\nbillion parameters. In complex benchmarks, Phi-2 either\\nmatches or exceeds the performance of models up to 25\\ntimes its size. We primarily use Phi-2’s linguistic abilities\\nto do various vision-language tasks. Specifically, for vision\\nreasoning tasks that involve spatial location identification,\\nwe instruct the linguistic model to generate textual descrip-\\ntions of what will happen in the next scenario, representing\\ntheir objects’ coordinates, as shown in Table 8.\\nNormalization and LoRA for TinyGPT-V .In Section 4.4,\\nwe conclude that training smaller-scale large language mod-\\nels for transfer learning, especially across different modali-\\nties (e.g., from text to image), poses significant challenges.\\nOur studies indicate that these smaller models are prone to\\nencountering NaN or INF values during multimodal data\\ncomputations. This issue often leads to a computational\\nloss value of NaN, thereby causing failure in the initial\\nbatch forward propagation. Moreover, the limited number\\nof trainable parameters in these models may lead to gradient\\nvanishing during training. To mitigate these problems, as\\ndepicted in Figure 5 (c), we incorporate the post-norm and\\ninput norm mechanisms from LLaMA-2, applying RMS\\nNorm after each Multi-Head Attention Layer (MHA) to\\nnormalize data for downstream processing. In addition, we\\nhave to update all layer norms in the Phi-2 model to improve\\ntraining stability, as detailed in the subsequent equation.\\nLayerNorminput(xhidden) =γ xhidden − µ√\\nσ2 + ϵ\\n+ β (1)\\nWhere, xhidden is the input of this layer, µ and σ2 are the\\nmean and variance of the inputs to the layer, respectively,ϵ\\nis a small number to prevent division by zero, γ and β are\\ntrainable parameters.\\nRMSNorm(xpost) = xpostq\\n1\\nN\\nPN\\ni=1 x2\\ni + ϵ\\n(2)\\nwhere xpost is the input after MHA, N is the dimension of\\nxpost.\\nFurthermore, (Henry et al., 2020) have underscored the vital\\nrole of Query-Key Normalization in low-resource learning\\nscenarios. Hence, as show in Figure 5 (d), we have incor-\\nporated Query-Key Normalization into the Phi-2 model, as\\ndetailed in the following equation.\\nAttention(Q, K, V) =softmax\\n\\x12LayerNorm(Q)LayerNorm(K)T\\n√dk\\n\\x13\\nV\\n(3)\\nwhere dk denotes the dimension of Q or K.\\nThe structure of the LoRA mechanism (Hu et al., 2021)\\nis show in Figure 5 (a), which is an efficient fine-tuning\\nmethod in parallel to the frozen pre-training weights as\\nshown in Figure 5 (c), which does not increase the inference\\ntime consuming for large language models and is easier to\\noptimize.\\n3.2. Training Stages\\nIn this subsection, the four-stage training process of\\nTinyGPT-V will be described.\\nWarm-up training for the first training stage. During\\nthe initial pretraining stage, TinyGPT-V is taught vision-\\nlanguage understanding using large datasets of aligned\\n4'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 4}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nLinear\\nDown\\nr\\nLinear\\nUpPretrained\\nWeights\\n\\ud835\\nInput\\nOutput\\nLoRA\\n(a) LoRA\\nMLP\\nNormalization\\nMHA LoRA\\n(b) LoRA Module \\nfor LLMs Block\\nMHA\\nQuery-Key \\nNormalization\\nLayer Norm\\nRMS Norm\\nMLP\\nLoRA\\nAfter \\nStage1\\n(c) LLMs Block\\n for TinyGPT-V\\nAttention\\nQ K V\\nLayer \\nNorm\\nLayer \\nNorm\\n(d) Query-Key \\nNormalization for MHA\\nFreeze Train Data Pathway Conditional Pathway\\nFigure 5: (a) represents the structure of LoRA, (b) represents how LoRA can efficiently fine-tune large language models\\n(LLMs) in natural language processing, (c) represents the structure of LLMs for TinyGPT-V , and (d) represents the structure\\nof QK Normalization.\\nPhi-2\\nImage-text Pair \\nInstruction Learning\\nFreezeTrain\\nStage 1\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\nPhi-2\\nStage 2\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\n& LoRA\\nPhi-2\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\n& LoRA\\nStage 3\\nPhi-2\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\n& LoRA\\nMulti-Tasks Learning\\nStage 4\\nFigure 6: The training process of TinyGPT-V , the first stage\\nis warm-up training, the second stage is pre-training, the\\nthird stage is instruction fine-tuning, and the fourth stage is\\nmulti-task learning.\\nimage-text pairs. The model identifies the output from the\\nprojection layers as a soft prompt directing it to create rel-\\nevant texts and to allow large language models to accept\\ninputs from the image modality. The pretraining process\\nuses a dataset combination of Conceptual Caption, SBU,\\nand LAION, involving 20000 training steps covering about\\n5 million image-text pairs.\\nPre-training for the second training stage. Following the\\ninitial training stage, the large language model becomes\\nequipped to process image modality inputs. To guarantee\\nmore consistent performance as the model transitions into\\nthe subsequent training stage, we re-employ the dataset from\\nthe first stage, specifically for training the LoRA module.\\nInstruction tuning for the third training stage. We fine-\\ntuned this TinyGPT-V model using a selection of image-text\\npairings from MiniGPT4 or LLaV A, which included in-\\nstructions like “###Human: <Img><ImageHere></Img>\\nTake a look at this image and describe what you no-\\ntice.###Assistant:.”. We used a uniform template inclusive\\nof a randomly chosen prompt that improved the model’s\\ncapacity for generating responses that were consistent and\\nsounded more natural.\\nMulti-task learning in the fourth training stage. The\\nfourth training stage of TinyGPT-V focuses on enhancing\\nits conversation ability as a chatbot by tuning the model with\\nmore multi-modal instruction datasets as shown in Table 1,\\nincluding LLaV A, Flickr30k, a mixing multi-task dataset,\\nand Unnatural Instruction using multi-tasks template as de-\\ntailed in appendix A. The LLaV A dataset is utilized for\\nmulti-modal instruction tuning with detailed descriptions\\nand complex reasoning examples. The Flickr30k dataset\\nis used to improve grounded image caption generation and\\nobject parsing and grounding capabilities. Additionally, a\\nmixing multi-task dataset is created to improve the model’s\\nhandling of multiple tasks during multi-round conversations.\\nFinally, to recover the language generation ability, the Un-\\nnatural Instruction dataset is added to the third-stage training\\nof TinyGPT-V .\\n5'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 5}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nData types Dataset Stage 1 Stage 2 Stage 3 Stage 4\\nImage-text pair LAION, CC3M, SBU ✓ ✓ ✗ ✗\\nInstruction tuning MiniGPT-4 Stage2 for CC & SBU ✗ ✗ ✓ ✗\\nCaption Text Captions, COCO Captions ✗ ✗ ✗ ✓\\nREC RefCOCO, RefCOCO+, RefCOCOg, Visual Genome ✗ ✗ ✗ ✓\\nVQA GQA, VQAv2, OK-VQA, AOK-VQA, OCR-VQA ✗ ✗ ✗ ✓\\nMultimodal instruction LLaV A dataset, Flickr30k, Multi-task conversation ✗ ✗ ✗ ✓\\nLangauge dataset Unnatural Instructions ✗ ✗ ✗ ✓\\nTable 1: The full list of datasets used by TinyGPT-MoE during training.\\nFigure 7: Changes in loss during the training stage of TinyGPT-V .\\n4. Experiments\\nIn this section, we describe the training and evaluation meth-\\nods in detail.\\n4.1. Training\\nExperimental setting. The experimental environment for\\nthis study was established with a single NVIDIA RTX 3090\\nGPU, equipped with a substantial 24GB of VRAM. The cen-\\ntral processing was handled by an AMD EPYC 7552 48-core\\nProcessor, offering 15 virtual CPUs. Memory allocation was\\nset at 80GB, ensuring sufficient capacity for handling large\\ndatasets. The software environment was standardized on\\nPyTorch version 2.0.0, with CUDA 11.8 support, facilitating\\noptimized tensor operations on the GPU.\\nTraining process. In our experimental process, we meticu-\\nlously orchestrated the training of our model through four\\ndistinct stages, each characterized by specific learning rate\\nstrategies and loss profiles, as shown in Figure 7.\\nStage 1: Spanning 17 epochs, with each epoch consisting\\nof 1000 iterations, we employed a dynamic learning rate\\napproach. The learning rate commenced at 1e-5 at the begin-\\nning of each epoch and gradually ascended to 1e-4 by the\\nepoch’s end. This pattern was consistently applied across all\\n17 epochs. The training loss exhibited a steady decline, start-\\ning from 7.152 and progressively tapering down to 2.620,\\nreflecting the model’s increasing proficiency in learning\\nfrom the data. The purpose of this stage is to be able to\\nmake the Phi-2 model in TinyGPT-V react in some way to\\nthe input of the imaging modality. The alignment of text\\nand image in the semantic space is done.\\nStage 2: Comprising 4 epochs, each with 5000 iterations,\\nthis stage introduced the “linear_warmup_cosine_lr“ (He\\net al., 2018; Goyal et al., 2018) learning rate schedule. We\\ninitiated a warmup phase of 5000 steps, where the learn-\\ning rate linearly increased from 1e-6 (warmup_lr) to 1e-4\\n(init_lr), followed by a cosine decay down to a minimum\\nlearning rate of 8e-5. This phase saw a consistent reduction\\nin loss, starting at 2.726 and culminating at 2.343. The\\npurpose of this stage is to enable the LoRA module to play\\na role in multimodal data, further reducing the model’s loss\\non image-text pairs and improving the model’s ability to\\nlearn from the data.\\nStage 3: This stage lasted for 5 epochs, each with 200\\niterations. We maintained the “linear_warmup_cosine_lr“\\nschedule, with a warmup phase of 200 steps. The learning\\nrate began at 1e-6, ascending to 3e-5 (init_lr), before decay-\\ning to 1e-5 (min_lr). The loss values reflected significant\\n6'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 6}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nimprovements, starting at 1.992 and reducing to 1.125. The\\npurpose of this stage is to allow TinyGPT-V to accept both\\nverbal and image modal inputs and produce responses to\\nthem. After this stage of training TinyGPT-V has been able\\nto perform most of the image answering tasks.\\nStage 4: The final stage stretched over 50 epochs, each\\ncomprising 1000 iterations. We adhered to the “lin-\\near_warmup_cosine_lr“ schedule with a 1000-step warmup\\nphase. The learning rate was initiated at 1e-6, reaching up\\nto 1e-5 (init_lr), and then experiencing a cosine decay to\\na minimum of 8e-5. The training loss values displayed a\\nconsistent downward trajectory, beginning at 2.720 and ulti-\\nmately reaching as low as 1.399. The purpose of this stage is\\nto allow TinyGPT-V to perform various tasks such as VQA\\nor VSR tasks at the same time, increasing the generalization\\nperformance of TinyGPT-V on multimodal tasks.\\n4.2. Evaluation\\nEvaluation datasets. GQA (Hudson & Manning, 2019)\\nis a dataset for real-world visual reasoning and composi-\\ntional question answering, featuring a powerful question en-\\ngine that generates 22 million diverse reasoning questions.\\nVSR (Liu et al., 2023b) comprises over 10k natural text-\\nimage pairs in English, encompassing 66 types of spatial\\nrelations. IconQA (Lu et al., 2021) with 107,439 questions\\naimed at challenging visual understanding and reasoning in\\nthe context of icon images, encompassing three sub-tasks\\n(multi-image-choice, multi-text-choice, and filling-in-the-\\nblank). VizWiz (Gurari et al., 2018) is a collection of more\\nthan 31,000 visual queries, each derived from a photo taken\\nby a visually impaired individual using a smartphone, ac-\\ncompanied by a vocalized question regarding the image,\\nand supplemented with 10 answers sourced from a crowd\\nfor each query. The Hateful Memes dataset (HM) (Kiela\\net al., 2021), developed by Facebook AI, is a comprehensive\\nmultimodal collection specifically designed for the detec-\\ntion of hateful content in memes, combining both image\\nand text elements, and comprises over 10,000 newly created\\nmultimodal examples.\\nVisual question answering results. As shown in Table 2,\\nit becomes evident that TinyGPT-V , a language model with\\nonly 2.8 billion parameters, exhibits notably competitive per-\\nformance across multiple benchmarks, closely rivaling mod-\\nels with nearly 13 billion parameters. Specifically, in the\\nVSR (Visual-Spatial Reasoning) zero-shot task, TinyGPT-V\\noutshines its counterparts by securing the highest score of\\n54.7%. This is particularly impressive considering its pa-\\nrameter size is approximately 4.6 times smaller than other\\nleading models such as BLIP-2, LLaV A, and InstructBLIP.\\nIn the GQA benchmark, while TinyGPT-V scores are 38.9%,\\nit lags behind the highest score achieved by InstructBLIP,\\nwhich is 49.5%. However, TinyGPT-V shows robust perfor-\\nTinyGPT-V and others answer example compare\\nUres [vqa] where should I hide in this room when playing hide and\\nseek\\nLLaV A-1.5hide behind the bookshelf\\nMiniGPT-v2behind couch\\nGPT-4V Behind the Couch\\nUnder the Table\\nInside the Bookshelf\\nBehind the Curtains\\nBehind the TV\\nTinyGPT-Vunder couch\\nFigure 8: Comparison of reasoning answers from different\\nModels. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\nmance in the IconVQ challenge, attaining a score of 44.7%,\\njust 0.1% short of InstructBLIP’s leading score of 44.8%.\\nSimilarly, in the VizWiz task, TinyGPT-V demonstrates\\ncommendable capabilities with a score of 37.8%, which,\\nis not only the highest but is notable given its reduced pa-\\nrameter count. In the context of the Hateful Memes (HM)\\ndataset, TinyGPT-V matches InstructBLIP’s top score of\\n57.5% with its own score of 54.0%, again underscoring its\\nefficiency and capacity to compete with models of larger\\nscales. Overall, TinyGPT-V’s performance across these di-\\nverse and challenging benchmarks is striking, especially\\nwhen considering its parameter efficiency\\n4.3. Qualitative Evaluation\\nThe comparative analysis revealed TinyGPT-V’s distinct\\nadvantage in delivering concise and accurate visual interpre-\\ntations. In the reasoning task to find a hiding spot during\\na game of hide and seek, TinyGPT-V demonstrated its su-\\nperior capability by providing a singular, viable suggestion:\\n’under couch’. This contrasts with other models that either\\noffered multiple options, some of which were incorrect as\\nindicated by the text in red (e.g., GPT-4V suggesting ’In-\\nside the Bookshelf’), or specified less practical hiding spots.\\nWhen asked about potential activities in an image with an\\nalligator, TinyGPT-V suggested a cautious response with-\\nout speculating beyond what was visible. In contrast, other\\nmodels, like LLaV A-1.5, provided extended narratives that\\nintroduced assumptions not directly inferred from the image.\\nSimilarly, in describing a soccer match scene, TinyGPT-V’s\\nresponse was succinct and focused on the key elements,\\navoiding the inaccuracies noted in MiniGPT-v2’s account,\\nwhich incorrectly identified multiple soccer balls on the\\n7'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 7}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nMethod LLM GQA VSR IconVQ VizWiz HM AverageParameters (zero-shot) (zero-shot) (zero-shot) (zero-shot)\\nFlamingo 9B - 31.8 - 28.8 57.0 39.20\\nIDEFICS (Laurençon et al., 2023) 7B - 38.4 - 35.5 - 37.05\\n65B - 45.2 - 36.0 - 39.60\\nBLIP-2 13B 41.0 50.9 40.6 19.6 53.7 41.16\\nLLaV A 13B 41.3 51.2 43.0 - - 45.17\\nInstructBLIP 13B 49.5 52.1 44.8 33.4 57.5 47.45\\nMiniGPT-4 13B - - 35.9 - - 35.90\\nBLIV A (Hu et al., 2023) 7B - - 44.8 31.4 55.6 41.15\\nLLaV A-Phi (Zhu et al., 2024) 2.8B - - 54.1 37.6 - 43.15\\nMoE-LLaV A (Lin et al., 2024)∗ 1.8B×4 61.5 - - 32.6 - 47.50\\nOurs\\nTinyGPT-V (Phi-2) 2.8B 38.9 54.7 44.7 37.8 54.0 46.02\\nTinyGPT-V (Phi-1.5) 1.3B 34.3 35.8 37.2 28.4 50.3 37.2\\nTable 2: Comparative performance of TinyGPT-V and other MLLMs across multiple visual question answering benchmarks.\\n∗It is worth noting that MoE-LLaV A is required 8xA100-80G for training.\\nMethod TinyGPT-V LLaV A MiniGPT-4\\nseconds per words 0.067 0.426 0.300\\ninference occupancy (8-bit) 5.6GB 22GB 23.5GB\\nTable 3: Comparison of inference time and inference occu-\\npancy about devices.\\npitch. These examples, as tabulated in Table 5 and Table 7,\\nillustrate TinyGPT-V’s superior performance in generating\\nbrief yet precise responses, underscoring its practicality for\\nrapid and reliable visual question answering. For efficient\\nevaluation, as shown in table 3, TinyGPT-V operates at\\nthe fastest pace, taking only 0.067 seconds to generate a\\nword, which suggests upper efficiency in processing speed\\ncompared to LLaV A and MiniGPT-4. On the other hand,\\nLLaV A exhibits a significantly slower word generation time\\nat 0.426 seconds per word, coupled with a higher memory\\noccupancy of 22GB. MiniGPT-4, with a generation time of\\n0.300 seconds per word and a memory usage of 23.5GB.\\n4.4. Ablation Study\\nAs shown in Table 4, the full TinyGPT-V model achieves\\nlow loss across all stages, but the removal of key modules\\nleads to significant training issues. Without the LoRA mod-\\nule, there’s a gradient vanish starting from Stage 3. Omitting\\nInput Layer Norm increases loss notably (to 2.839 in Stage\\n1) and causes gradient vanishing in Stage 4. Without RMS\\nNorm, the model sees an elevated loss in Stage 1 (2.747)\\nand faces early gradient vanishing in Stage 2. The absence\\nof QK Norm results in immediate gradient vanish. This data\\nclearly illustrates each module’s crucial role in preventing\\ngradient vanishing and maintaining low loss throughout the\\ntraining process.\\nMethod Stage 1 Loss Stage 2 Loss Stage 3 Loss Stage 4 LossTinyGPT-V 2.620 2.343 1.125 1.330w/o LoRA 2.620 - Gradient Vanish -w/o Input Layer Norm 2.839 2.555 1.344 Gradient Vanishw/o RMS Norm 2.747 Gradient Vanish - -w/o QK Norm Gradient Vanish - - -\\nTable 4: Importance of each module in TinyGPT-V at each\\nstage of training.\\nFurthermore, our reveal a notable trend: the smaller the\\nlarge language model used for transfer learning (particu-\\nlarly in transitioning from text-to-image modality), the more\\nchallenging the training process becomes. We observed a\\npronounced need for additional normalization layers to sta-\\nbilize the training, especially when scaling down from larger\\nmodels like Vicuna-13B to smaller ones like Phi-2 (2.7B),\\nPhi-1.5 (1.3B), and other small backbones as detailed in the\\nAppendix B.\\n5. Conclusion\\nIn this study, we introduce TinyGPT-V , a parameter-efficient\\nMLLMs tailored for a range of real-world vision-language\\napplications. Our model innovatively builds on the compact\\nyet powerful Phi-2 small language model framework. This\\napproach results in TinyGPT-V delivering exceptional out-\\ncomes in diverse benchmarks like visual question-answering\\nand referring expression comprehension while keeping com-\\nputational demands manageable. Remarkably, TinyGPT-V\\ncan be trained on a 24G GPU and deployed on an 8G device,\\ndemonstrating a significant advancement in creating cost-\\neffective, efficient, and potent MLLMs. This paper marks a\\ncontribution towards crafting smaller, yet robust multimodal\\nlanguage models for practical, real-world use cases. We\\nenvision that our work will catalyze further explorations\\ninto developing compact MLLMs for diverse applications.\\n8'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 8}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nReferences\\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\\nHasson, Y ., Lenc, K., Mensch, A., Millican, K., Reynolds,\\nM., et al. Flamingo: a visual language model for few-shot\\nlearning. Advances in Neural Information Processing\\nSystems, 35:23716–23736, 2022.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems, 33:\\n1877–1901, 2020.\\nCao, Y ., Kang, Y ., Wang, C., and Sun, L. Instruction mining:\\nWhen data mining meets large language model finetuning,\\n2023.\\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con-\\nceptual 12m: Pushing web-scale image-text pre-training\\nto recognize long-tail visual concepts, 2021.\\nChen, J., Guo, H., Yi, K., Li, B., and Elhoseiny, M. Visual-\\ngpt: Data-efficient adaptation of pretrained language mod-\\nels for image captioning. InProceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npp. 18030–18040, 2022.\\nChen, J., Zhu, D., Shen, X., Li, X., Liu, Z., Zhang, P., Krish-\\nnamoorthi, R., Chandra, V ., Xiong, Y ., and Elhoseiny, M.\\nMinigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478, 2023.\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y ., Wu, Z., Zhang,\\nH., Zheng, L., Zhuang, S., Zhuang, Y ., Gonzalez, J. E.,\\net al. Vicuna: An open-source chatbot impressing gpt-4\\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\\n(accessed 14 April 2023), 2023.\\nDai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,\\nW., Li, B., Fung, P., and Hoi, S. Instructblip: Towards\\ngeneral-purpose vision-language models with instruction\\ntuning, 2023.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805,\\n2018.\\nDufter, P., Schmitt, M., and Schütze, H. Position informa-\\ntion in transformers: An overview, 2021.\\nFang, Y ., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X.,\\nHuang, T., Wang, X., and Cao, Y . Eva: Exploring the\\nlimits of masked visual representation learning at scale,\\n2022.\\nGoyal, P., Dollár, P., Girshick, R., Noordhuis, P.,\\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and\\nHe, K. Accurate, large minibatch sgd: Training imagenet\\nin 1 hour, 2018.\\nGurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman,\\nK., Luo, J., and Bigham, J. P. Vizwiz grand challenge:\\nAnswering visual questions from blind people. In Pro-\\nceedings of the IEEE conference on computer vision and\\npattern recognition, pp. 3608–3617, 2018.\\nHe, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\\nBag of tricks for image classification with convolutional\\nneural networks, 2018.\\nHenry, A., Dachapally, P. R., Pawar, S., and Chen, Y . Query-\\nkey normalization for transformers, 2020.\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\\nWelbl, J., Clark, A., et al. Training compute-optimal\\nlarge language models. arXiv preprint arXiv:2203.15556,\\n2022.\\nHu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,\\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation\\nof large language models, 2021.\\nHu, W., Xu, Y ., Li, Y ., Li, W., Chen, Z., and Tu, Z. Bliva:\\nA simple multimodal llm for better handling of text-rich\\nvisual questions, 2023.\\nHudson, D. A. and Manning, C. D. Gqa: A new dataset for\\nreal-world visual reasoning and compositional question\\nanswering. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, pp. 6700–\\n6709, 2019.\\nJavaheripi, M., Bubeck, S., Abdin, M., Aneja, J., Bubeck,\\nS., Mendes, C. C. T., Chen, W., Giorno, A. D., Eldan,\\nR., Gopi, S., Gunasekar, S., Javaheripi, M., Kauff-\\nmann, P., Lee, Y . T., Li, Y ., Nguyen, A., de Rosa, G.,\\nSaarikivi, O., Salim, A., Shah, S., Santacroce, M.,\\nBehl, H. S., Kalai, A. T., Wang, X., Ward, R., Witte,\\nP., Zhang, C., and Zhang, Y . Phi-2: The surprising\\npower of small language models. https://www.\\nmicrosoft.com/en-us/research/blog/\\nphi-2-the-surprising-power-of-small-language-models/ ,\\n2023.\\nKazemzadeh, S., Ordonez, V ., Matten, M., and Berg, T.\\nReferitgame: Referring to objects in photographs of natu-\\nral scenes. In Proceedings of the 2014 conference on em-\\npirical methods in natural language processing (EMNLP),\\npp. 787–798, 2014.\\n9'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 9}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nKiela, D., Firooz, H., Mohan, A., Goswami, V ., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes.\\nAdvances in neural information processing systems, 33:\\n2611–2624, 2020.\\nKiela, D., Firooz, H., Mohan, A., Goswami, V ., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes,\\n2021.\\nLaurençon, H., Saulnier, L., Tronchon, L., Bekman, S.,\\nSingh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush,\\nA. M., Kiela, D., Cord, M., and Sanh, V . Obelics: An\\nopen web-scale filtered dataset of interleaved image-text\\ndocuments, 2023.\\nLi, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping\\nlanguage-image pre-training with frozen image encoders\\nand large language models, 2023a.\\nLi, Y ., Bubeck, S., Eldan, R., Giorno, A. D., Gunasekar,\\nS., and Lee, Y . T. Textbooks are all you need ii: phi-1.5\\ntechnical report, 2023b.\\nLin, B., Tang, Z., Ye, Y ., Cui, J., Zhu, B., Jin, P., Huang, J.,\\nZhang, J., Ning, M., and Yuan, L. Moe-llava: Mixture of\\nexperts for large vision-language models, 2024.\\nLin, T.-Y ., Maire, M., Belongie, S., Bourdev, L., Girshick,\\nR., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and\\nDollár, P. Microsoft coco: Common objects in context,\\n2015.\\nLiu, H., Li, C., Li, Y ., and Lee, Y . J. Improved baselines\\nwith visual instruction tuning, 2023a.\\nLiu, H., Li, C., Wu, Q., and Lee, Y . J. Visual instruction\\ntuning. arXiv preprint arXiv:2304.08485, 2023b.\\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y ., Zhang, W., Yu,\\nZ., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark\\nfor abstract diagram understanding and visual language\\nreasoning. arXiv preprint arXiv:2110.13214, 2021.\\nMao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L.,\\nand Murphy, K. Generation and comprehension of unam-\\nbiguous object descriptions. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 11–20, 2016.\\nOpenAI. Introducing chatgpt. https://openai.com/\\nblog/chatgpt, 2022.\\nOrdonez, V ., Kulkarni, G., and Berg, T. Im2text: Describing\\nimages using 1 million captioned photographs. In\\nShawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and\\nWeinberger, K. (eds.), Advances in Neural Information\\nProcessing Systems, volume 24. Curran Associates, Inc.,\\n2011. URL https://proceedings.neurips.\\ncc/paper_files/paper/2011/file/\\n5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.\\npdf.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\nwith human feedback. Advances in Neural Information\\nProcessing Systems, 35:27730–27744, 2022.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\\nSutskever, I., et al. Language models are unsupervised\\nmultitask learners. OpenAI blog, 1(8):9, 2019.\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\\nJ., Krueger, G., and Sutskever, I. Learning transferable\\nvisual models from natural language supervision, 2021.\\nSchuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk,\\nR., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and\\nKomatsuzaki, A. Laion-400m: Open dataset of clip-\\nfiltered 400 million image-text pairs, 2021.\\nSchwenk, D., Khandelwal, A., Clark, C., Marino, K., and\\nMottaghi, R. A-okvqa: A benchmark for visual ques-\\ntion answering using world knowledge. In European\\nConference on Computer Vision, pp. 146–162. Springer,\\n2022.\\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\\nceptual captions: A cleaned, hypernymed, image alt-text\\ndataset for automatic image captioning. In Gurevych,\\nI. and Miyao, Y . (eds.), Proceedings of the 56th An-\\nnual Meeting of the Association for Computational Lin-\\nguistics (Volume 1: Long Papers), pp. 2556–2565, Mel-\\nbourne, Australia, July 2018. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/P18-1238. URL\\nhttps://aclanthology.org/P18-1238.\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X.,\\nGuestrin, C., Liang, P., and Hashimoto, T. B. Stanford\\nalpaca: An instruction-following llama model, 2023.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\\nFuller, B., Gao, C., Goswami, V ., Goyal, N., Hartshorn,\\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\\nV ., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y .,\\nMao, Y ., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\\nI., Nie, Y ., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\\n10'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 10}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\\nXu, P., Yan, Z., Zarov, I., Zhang, Y ., Fan, A., Kambadur,\\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\\nand Scialom, T. Llama 2: Open foundation and fine-tuned\\nchat models, 2023.\\nTsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S.,\\nVinyals, O., and Hill, F. Multimodal few-shot learn-\\ning with frozen language models. Advances in Neural\\nInformation Processing Systems, 34:200–212, 2021.\\nWei, L., Jiang, Z., Huang, W., and Sun, L. Instructiongpt-\\n4: A 200-instruction paradigm for fine-tuning minigpt-4,\\n2023.\\nWorkshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick, E.,\\nIli´c, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon,\\nF., et al. Bloom: A 176b-parameter open-access multilin-\\ngual language model. arXiv preprint arXiv:2211.05100,\\n2022.\\nYang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and\\nWang, L. The dawn of lmms: Preliminary explorations\\nwith gpt-4v(ision), 2023.\\nYu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L.\\nModeling context in referring expressions. In Computer\\nVision–ECCV 2016: 14th European Conference, Amster-\\ndam, The Netherlands, October 11-14, 2016, Proceed-\\nings, Part II 14, pp. 69–85. Springer, 2016.\\nYuan, Z., Wang, X., Wang, K., Sun, L., and Ye, Y . Artgpt-\\n4: Towards artistic-understanding large vision-language\\nmodels with enhanced adapter, 2023.\\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V .,\\net al. Opt: Open pre-trained transformer language models.\\narXiv preprint arXiv:2205.01068, 2022.\\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z.,\\nZhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang,\\nH., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge\\nwith mt-bench and chatbot arena, 2023.\\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.\\nMinigpt-4: Enhancing vision-language understanding\\nwith advanced large language models. arXiv preprint\\narXiv:2304.10592, 2023.\\nZhu, Y ., Zhu, M., Liu, N., Ou, Z., Mou, X., and Tang,\\nJ. Llava-phi: Efficient multi-modal assistant with small\\nlanguage model, 2024.\\n11'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 11}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nA. Multi-task Instruction Template\\nTo reduce ambiguity in training a unified multimodal model for diverse tasks such as visual question answering, image\\ncaptioning, referring expression comprehension, generation, and object parsing and grounding, we employed task-specific\\ntokens from MiniGPT-v2 within a multitask instruction template. This template, derived from the LLaMA-2 conversation\\ntemplate (Touvron et al., 2023), includes a general input format comprising image features, a task identifier token, and an\\ninstruction input. We incorporated six distinct task identifiers, each associated with a particular task. For tasks necessitating\\nthe identification of spatial locations of referred objects, the model uses textual representations of bounding boxes with\\ncoordinates normalized between 0 and 100. Overall, MiniGPT-v2’s unique task-specific tokens enhance task disambiguation,\\nleading to more precise and effective task execution.\\nTinyGPT-V and others answer example compare\\nUres What might happen in this image in the next second\\nLLaV A-1.5 In the image, a man is sitting on a log next to a large alligator, and he is holding a camera. It is likely\\nthat the man is about to take a photograph or record a video of the alligator. The next second could\\ninvolve the man capturing the alligator’s behavior, appearance, or any other interesting moment.\\nThe alligator might also react to the man’s presence, such as moving or showing curiosity, which\\ncould make the captured image or video more engaging and memorable.\\nMiniGPT-v2 In the next second, the man might be startled or surprised by the close-up shot of the crocodile\\nin the water. This could cause him to jump back or react with caution, ensuring his safety in the\\npresence of the wild animal. The image captures a moment of wonder and curiosity, highlighting\\nthe beauty of the natural environment while also acknowledging the potential danger that comes\\nwith being so close to a wild animal.\\nGPT-4V In the next second, the man might quickly move away to maintain a safe distance from the alligator.\\nAlligators can be unpredictable, and it’s important to keep a safe distance from them at all times to\\navoid potential harm. The man seems to be taking a selfie with the alligator, which suggests that he\\nis close to the animal. This is generally not advisable as it can be dangerous. It’s always important\\nto respect wildlife and observe it from a distance that is safe for both the person and the animal.\\nTinyGPT-V The image shows a man holding a selfie camera next to a crocodile. This could be an unusual and\\nunexpected sight, as crocodiles are not typically associated with human activities like taking selfies.\\nlt’s important for the man to stay safe and avoid any potential risks posed by the crocodile.\\nTable 5: Comparison of prediction answers from different models. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\nB. Small Backbones for Transfer Learning\\nAs shown in Table 6, a striking pattern emerges from the data: smaller LLMs exhibit heightened sensitivity to the removal\\nof these modules, with a pronounced tendency towards training difficulties, such as gradient vanishing. For instance, the\\nabsence of LoRA in both Phi-1.5 and TinyLLaMA resulted in an immediate cessation of training progress post-Stage 1,\\nindicating a critical reliance on this module for sustaining training in smaller models. Similarly, the exclusion of QK Norm\\nled to gradient vanishing at the earliest stage across all smaller LLMs, underscoring its essential role in the initial phases\\n12'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 12}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nof training. Moreover, the sequential progression in training losses across stages for models without these modifications\\ndemonstrates a clear degradation in training efficiency and effectiveness. For example, the removal of Input Layer Norm and\\nRMS Norm not only heightened Stage 1 loss across Phi-1.5 and TinyLLaMA but also precipitated gradient vanishing in\\nlater stages, showcasing the compound impact of these modules on model stability and learning capability. This analysis\\nincontrovertibly highlights a fundamental challenge in training smaller LLMs for migration to MLLMs: the absence of\\nkey architectural and normalization modules severely impedes their training process, making them more prone to early\\ntraining halts and efficiency losses. The results underscore the necessity of these components in supporting the stability and\\ngradual learning progression of smaller LLMs, thus illuminating a pivotal consideration for developers aiming to optimize\\nthe training framework for seamless model.\\nLLM Stage 1 Loss Stage 2 Loss Stage 3 Loss Stage 4 Loss\\nPhi-2 (2.7B) 2.620 2.343 1.125 1.330\\nPhi-1.5 (1.3B) 3.420 3.043 1.525 1.730\\nw/o LoRA 3.420 - Gradient Vanish -\\nw/o Input Layer Norm 3.555 3.221 1.544 Gradient Vanish\\nw/o RMS Norm 3.557 Gradient Vanish - -\\nw/o QK Norm Gradient Vanish - - -\\nTinyLLaMA (1.1B) 3.529 3.053 1.371 1.830\\nw/o LoRA 3.529 - Gradient Vanish -\\nw/o Input Layer Norm 3.611 3.331 1.444 Gradient Vanish\\nw/o RMS Norm 3.607 Gradient Vanish - -\\nw/o QK Norm Gradient Vanish - - -\\nTable 6: Importance of each module and different LLMs in TinyGPT-V training framework at each stage of training.\\n13'),\n",
       " Document(metadata={'source': 'data_source\\\\2312.16862v3.pdf', 'page': 13}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nTinyGPT-V and others answer example compare\\nUres What’s in the image?\\nLLaV A-1.5 The image shows a group of men playing soccer on a field. They are actively engaged in the game,\\nwith some players running towards the soccer ball.\\nMiniGPT-v2 The image features a lively and competitive soccer game with several players on the field. Some of\\nthe players are wearing jerseys, while others have their own unique outfits. There are at least eight\\nplayers in total, with three of them being the main focus of the scene.\\nOne player is kicking a soccer ball, with another player nearby trying to block the shot. A third\\nplayer is also present, playing offense. In addition, there are two soccer balls visible on the field,\\none closer to the left side and the other near the center of the scene.\\nThe players are engaged in intense action, showcasing their skills and determination to score. The\\natmosphere appears to be a high-energy match with a mix of excitement and competition among\\nthe players.\\nGPT-4V The image shows a moment from a soccer match. In the foreground, there’s a player in a blue\\nuniform who appears to be dribbling the ball with close control, evading two opponents in white\\nuniforms. The player in blue is crouched low to the ground, demonstrating agility and focus, which\\nsuggests he is in the midst of making a strategic play, possibly attempting to bypass the defenders. In\\nthe background, other players can be observed, and the stands are filled with spectators, indicating\\nthat this is a professional match likely taking place in a large stadium. The attire and hairstyles\\nsuggest the photo may be from a past era, possibly the 1980s or 1990s.\\nTinyGPT-V The image shows a socer game with several people playing, including at least one person in a blue\\nshirt. The players are engaged in a friendly match, showcasing their skills and teamwork.\\nTable 7: Comparison of descriptive answers from different Models. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\n14')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Load from HTML file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.Load Unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "html_path = \"./data_source/a.html\"\n",
    "\n",
    "html_loader = UnstructuredHTMLLoader(html_path)\n",
    "docs = html_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './data_source/a.html'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Load with BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data_source/a.html', 'title': 'Multimodality and Large Multimodal Models (LMMs)'}, page_content=\"\\n\\n\\nMultimodality and Large Multimodal Models (LMMs)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMultimodality and Large Multimodal Models (LMMs) | Chip Huyen\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nChip Huyen\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBlog\\nBooks\\nEvents\\n\\nAI Guide\\n\\nAI Roadmap\\nLlama Police\\nML Interviews\\n\\n\\nList 100\\nVN\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable of Contents\\n\\nPart 1. Understanding Multimodal\\n\\nWhy multimodal\\nData modalities\\nMultimodal tasks\\n\\nGeneration\\nVision-language understanding\\n\\n\\n\\nPart 2. Fundamentals of Multimodal Training\\n\\nCLIP: Contrastive Language-Image Pre-training\\n\\nCLIP's high-level architecture\\nNatural language supervision\\nContrastive learning\\n\\nClassifier objective\\nLanguage model objective\\nContrastive objective\\n\\n\\nCLIP applications\\n\\n\\nFlamingo: the dawns of LMMs\\n\\nFlamingo's high-level architecture\\nData\\nFlamingo's vision encoder\\nFlamingo's language model\\n\\n\\nTL;DR: CLIP vs. Flamingo\\n\\nPart 3. Research Directions for LMMs\\n\\nIncorporating more data modalities\\nMultimodal systems for instruction-following\\nAdapters for more efficient multimodal training\\nGenerating multimodal outputs\\n\\nConclusion\\nResources\\n\\n\\n\\n    Table of Contents \\n\\n\\n\\n\\nMultimodality and Large Multimodal Models (LMMs)\\n\\n\\n        \\n        Oct 10, 2023\\n      \\n      \\n        • Chip Huyen\\n\\n\\n\\nFor a long time, each ML model operated in one data mode – text (translation, language modeling), image (object detection, image classification), or audio (speech recognition).\\nHowever, natural intelligence is not limited to just a single modality. Humans can read, talk, and see. We listen to music to relax and watch out for strange noises to detect danger. Being able to work with multimodal data is essential for us or any AI to operate in the real world.\\nOpenAI noted in their GPT-4V system card that “incorporating additional modalities (such as image inputs) into LLMs is viewed by some as a key frontier in AI research and development.”\\nIncorporating additional modalities to LLMs (Large Language Models) creates LMMs (Large Multimodal Models). Not all multimodal systems are LMMs. For example, text-to-image models like Midjourney, Stable Diffusion, and Dall-E are multimodal but don’t have a language model component. Multimodal can mean one or more of the following:\\n\\nInput and output are of different modalities (e.g. text-to-image, image-to-text)\\nInputs are multimodal (e.g. a system that can process both text and images)\\nOutputs are multimodal (e.g. a system that can generate both text and images)\\n\\nThis post covers multimodal systems in general, including LMMs. It consists of 3 parts.\\n\\nPart 1 covers the context for multimodality, including why multimodal, different data modalities, and types of multimodal tasks.\\nPart 2 discusses the fundamentals of a multimodal system, using the examples of CLIP, which lays the foundation for many future multimodal systems, and Flamingo, whose impressive performance gave rise to LMMs.\\nPart 3 discusses some active research areas for LMMs, including generating multimodal outputs and adapters for more efficient multimodal training, covering newer multimodal systems such as BLIP-2, LLaVA, LLaMA-Adapter V2, LAVIN, etc.\\n\\nThe post is long. Feel free to skip to the sections most interesting to you.\\n⚠ Ambiguous terminology ⚠\\nMultimodal data can also refer to multimodal distributions, e.g. bimodal distribution, which is different from multimodal data in this post.\\n\\n\\nTable of contents\\nPart 1. Understanding Multimodal\\n…. Why multimodal\\n…. Data modalities\\n…. Multimodal tasks\\n…….. Generation\\n…….. Vision-language understanding\\nPart 2. Fundamentals of Multimodal Training\\n…. CLIP: Contrastive Language-Image Pre-training\\n…….. CLIP’s high-level architecture\\n…….. Natural language supervision\\n…….. Contrastive learning\\n…….. CLIP applications\\n…. Flamingo: the dawns of LMMs\\n…….. Flamingo’s high-level architecture\\n…….. Data\\n…….. Flamingo’s vision encoder\\n…….. Flamingo’s language model\\n…. TL;DR: CLIP vs. Flamingo\\nPart 3. Research Directions for LMMs\\n…. Incorporating more data modalities\\n…. Multimodal systems for instruction-following\\n…. Adapters for more efficient multimodal training\\n…. Generating multimodal outputs\\nConclusion\\nResources\\n\\n\\nPart 1. Understanding Multimodal\\nWhy multimodal\\nMany use cases are impossible without multimodality, especially those in industries that deal with a mixture of data modalities such as healthcare, robotics, e-commerce, retail, gaming, etc.\\n\\n\\n\\n\\n    An example of how multimodality can be used in healthcare. Image from Multimodal biomedical AI (Acosta et al., Nature Medicine 2022)\\n\\n\\nNot only that, incorporating data from other modalities can help boost model performance. Shouldn’t a model that can learn from both text and images perform better than a model that can learn from only text or only image?\\nMultimodal systems can provide a more flexible interface, allowing you to interact with them in whichever way works best for you at the moment. Imagine you can ask a question by typing, talking, or just pointing your camera at something.\\nOne use case that I’m especially excited about, is that multimodality can also enable visually impaired people to browse the Internet and also navigate the real world.\\n\\n\\n\\n\\n    Some cool multimodal use cases from GPT-4V\\n\\n\\nData modalities\\nDifferent data modes are text, image, audio, tabular data, etc. One data mode can be represented or approximated in another data mode. For example:\\n\\nAudio can be represented as images (mel spectrograms).\\nSpeech can be transcribed into text, though its text-only representation loses information such as volume, intonation, pauses, etc.\\nAn image can be represented as a vector, which, in turn, can be flattened and represented as a sequence of text tokens.\\nA video is a sequence of images plus audio. ML models today mostly treat videos as sequences of images. This is a severe limitation, as sounds have proved to be just as important as visuals for videos. 88% of TikTok users shared that sound is essential for their TikTok experience.\\nA text can be represented as an image if you simply take a picture of it.\\nA data table can be converted into a chart, which is an image.\\n\\n\\nHow about other data modalities?\\nAll digital data formats can be represented using bitstrings (strings of 0 and 1) or bytestrings. A model that can effectively learn from bitstrings or bytestrings will be very powerful, and it can learn from any data mode.\\nThere are other data modalities we haven’t touched on, such as graphs and 3D assets. We also haven’t touched on the formats used to represent smell and touch (haptics).\\n\\n\\nIn ML today, audio is still largely treated as a voice-based alternative to text. The most common use cases for audio are still speech recognition (speech-to-text) and speech synthesis (text-to-speech). Non-speech audio use cases, e.g. music generation, are still pretty niche. See the fake Drake & Weeknd song and MusicGen model on HuggingFace.\\nImage is perhaps the most versatile format for model inputs, as it can be used to represent text, tabular data, audio, and to some extent, videos. There’s also so much more visual data than text data. We have phones/webcams that constantly take pictures and videos today.\\nText is a much more powerful mode for model outputs. A model that can generate images can only be used for image generation, whereas a model that can generate text can be used for many tasks: summarization, translation, reasoning, question answering, etc.\\nFor simplicity, we’ll focus on 2 modalities: images and text. The learnings can be somewhat generalized to other modalities.\\nMultimodal tasks\\nTo understand multimodal systems, it’s helpful to look at the tasks they are built to solve. In literature, I commonly see vision-language tasks divided into two groups: generation and vision-language understanding (VLU), which is the umbrella term for all tasks that don’t require generation. The line between these two groups is blurred, as being able to generate answers requires understanding too.\\nGeneration\\nFor generative tasks, the output can be unimodal (e.g. text, image, 3D rendering) or multimodal. While unimodal outputs are common today, multimodal outputs are still shaping up. We’ll discuss multimodal outputs at the end of this post.\\nImage generation (text-to-image synthesis)\\nThis category is straightforward. Examples: Dall-E, Stable Diffusion, and Midjourney.\\nText generation\\nA common text generation task is visual question answering. Instead of relying only on text for the context, you can give the model both text and images. Imagine you can point your camera to anything and ask questions like: “My car won’t start. What’s wrong with it?”, “How to make this dish?”, or “What is this meme about?”.\\nAnother common use case is image captioning, which can be used as part of a text-based image retrieval system. An organization might have millions, if not billions, of images: product images, graphs, designs, team pictures, promotional materials, etc. AI can automatically generate captions and metadata for them, making it easier to find the exact images you want.\\nVision-language understanding\\nWe’ll zoom into two task types: classification and text-based image retrieval (TBIR).\\nClassification\\nClassification models can only generate outputs that belong to a pre-determined list of classes. This works when you only care about a fixed number of outcomes. For example, an OCR system only needs to predict if a visual is one of the known characters (e.g. a digit or a letter).\\nSide note: An OCR system processes data at the character level. When used together with a system that can understand the broader context, it can improve use cases such as allowing you to “talk” to any textbook, contract, assembly instructions, etc.\\n\\n\\n\\n\\n    Document processing with GPT-4V. The model's mistake is highlighted in red.\\n\\n\\nOne related task to classification is image-to-text retrieval: given an image and a pool of pre-defined texts, find the text that’s most likely to accompany the image. This can be helpful for product image search, i.e. retrieving product reviews from a picture.\\nText-based image retrieval (image search)\\nImage search matters not only for search engines but also for enterprises to be able to search through all their internal images and documents. Some people call text-based image retrieval “text-to-image retrieval”.\\nThere are several approaches to text-based image retrieval. Two of them are:\\n\\nGenerate captions and metadata for each image, either manually or automatically (see image captioning in Text generation). Given a text query, find images whose captions/metadata are closest to this text query.\\nTrain a joint embedding space for both images and text. Given a text query, generate an embedding for this query, and find all images whose embeddings are closest to this embedding.\\n\\nThe second approach is more flexible, and I believe will be more widely used. This approach requires having a strong joint embedding space for both vision and language, like the one that OpenAI’s CLIP developed.\\nPart 2. Fundamentals of Multimodal Training\\nGiven the existence of so many amazing multimodal systems, a challenge of writing this post is choosing which systems to focus on. In the end, I decided to focus on two models: CLIP (2021) and Flamingo (2022) both for their significance as well as availability and clarity of public details.\\n\\nCLIP was the first model that could generalize to multiple image classification tasks with zero- and few-shot learning.\\nFlamingo wasn’t the first large multimodal model that could generate open-ended responses (Salesforce’s BLIP came out 3 months prior). However, Flamingo’s strong performance prompted some to consider it the GPT-3 moment in the multimodal domain.\\n\\nEven though these two models are older, many techniques they use are still relevant today. I hope they serve as the foundation to understanding newer models. The multimodal space is evolving repaidly, with many new ideas being developed. We’ll go over these newer models in Part 3.\\nAt a high level, a multimodal system consists of the following components:\\n\\nAn encoder for each data modality to generate the embeddings for data of that modality.\\nA way to align embeddings of different modalities into the same multimodal embedding space.\\n[Generative models only] A language model to generate text responses. Since inputs can contain both text and visuals, new techniques need to be developed to allow the language model to condition its responses on not just text, but also visuals.\\n\\nIdeally, as many of these components should be pretrained and reusable as possible.\\nCLIP: Contrastive Language-Image Pre-training\\nCLIP’s key contribution is its ability to map data of different modalities, text and images, into a shared embedding space. This shared multimodal embedding space makes text-to-image and image-to-text tasks so much easier.\\nTraining this multimodal embedding space also produced a strong image encoder, which allows CLIP to achieve competitive zero-shot performance on many image classification tasks. This strong image encoder can be used for many other tasks: image generation, visual question answering, and text-based image retrieval. Flamingo and LLaVa use CLIP as their image encoder. DALL-E uses CLIP to rerank generated images. It’s unclear if GPT-4V uses CLIP.\\n\\n\\n\\n\\n    Zero-shot image classification with CLIP\\n\\n\\nCLIP leveraged natural language supervision and contrastive learning, which allowed CLIP to both scale up their data and make training more efficient. We’ll go over why/how these two techniques work.\\nCLIP's high-level architecture\\n\\n\\n\\n\\n    CLIP's architecture. Both encoders and projection matrices are jointly trained together from scratch. The training goal is to maximize the similarity scores of the right (image, text) pairings while minimizing the similarity scores of the wrong pairings (contrastive learning). \\n\\n\\nFor the image encoder, the authors experimented with both ResNet and ViT. Their best-performing model is ViT-L/14@336px:\\n\\nLarge vision transformer (ViT-L)\\n14 patches (each image is divided into 14x14 pixel patches/sub-images)\\non 336x336 pixel input\\n\\nFor the text encoder, CLIP uses a Transformer model similar to GPT-2 but smaller. Their base model has only 63M parameters with 8 attention heads. The authors found CLIP’s performance to be less sensitive to the capacity of the text encoder.\\nEmbeddings generated by the image encoder and text encoder are projected into the same embedding space using two projection matrices WvWv and WlWl.\\n\\nGiven an image embedding ViVi, the corresponding multimodal embedding is computed as: WvViWvVi.\\nGiven a text embedding LiLi, the corresponding multimodal embedding is computed as: WlLiWlLi.\\n\\nWhen people say CLIP embeddings, they either refer to these multimodal embeddings or the embeddings generated by CLIP’s image encoder.\\nNatural language supervision\\nFor many years, image models were trained with manually annotated (image, text) datasets (e.g. ImageNet, MS COCO). This isn’t scalable. Manual annotation is time-consuming and expensive.\\nThe CLIP paper noted that none of the then-available (image, text) datasets was big and high quality enough. They created their own dataset – 400M (image, text) pairs – as follows.\\n\\nConstruct a list of 500,000 queries. Queries are common words, bigrams, and titles of popular Wikipedia articles.\\nFind images matching these queries (string and substring match). The paper mentioned this search did NOT happen on search engines but didn’t specify where. My theory is that since OpenAI already scraped the entire Internet for their GPT models, they probably just queried their internal database.\\nEach image is paired with a text that co-occurs with it (e.g. captions, comments) instead of the query since queries are too short to be descriptive.\\n\\nBecause some queries are more popular than others, to avoid data imbalance, they used at most 20K images for a query.\\nContrastive learning\\nPre-CLIP, most vision-language models were trained using a classifier or language model objectives. Contrastive objective is a clever technique that allows CLIP to scale and generalize to multiple tasks.\\nWe’ll show why the constrastive objective works better for CLIP using an example task of image captioning: given an image, generate a text that describes it.\\nClassifier objective\\nA classifier predicts the correct class among a predetermined list of classes. This works when the output space is finite. Previous models that work with (image, text) pair datasets all had this limitation. For example, models working with ILSVRC-2012 limited themselves to 1,000 classes, and JFT-300M to 18,291 classes.\\nThis objective limits not only the model’s capacity to output meaningful responses but also its capacity for zero-shot learning. Say, if the model was trained to predict among 10 classes, it won’t work for a task that has 100 classes.\\nLanguage model objective\\nIf a classifier outputs only one class for each input, a language model outputs a sequence of classes. Each generated class is called a token. Each token is from a predetermined list, the vocabulary, of the language model.\\n\\n\\n\\n\\n    Classifier vs. language model objectives\\n\\n\\nContrastive objective\\nWhile the language model objective allows for vastly more flexible outputs, CLIP authors noted this objective made the training difficult. They hypothesized that this is because the model tries to generate exactly the text accompanying each image, while many possible texts can accompany an image: alt-text, caption, comments, etc.\\nFor example, in the Flickr30K dataset, each image has 5 captions provided by human annotators, and the captions for the same image can be very different.\\n\\n\\n\\n\\n\\n\\nContrastive learning is to overcome this challenge. Instead of predicting the exact text of each image, CLIP was trained to predict whether a text is more likely to accompany an image than other texts.\\nFor each batch of NN (image, text) pairs, the model generates N text embeddings and N image embeddings.\\n\\nLet V1,V2,...,VnV1,V2,...,Vn be the embeddings for the NN images.\\nLet L1,L2,...,LnL1,L2,...,Ln be the embeddings for the NN texts.\\n\\nCLIP computes the cosine similarity scores of the N2N2 possible (Vi,LjVi,Lj) pairings. The model is trained to maximize the similarity scores of the NN correct pairings while minimizing the scores of the N2−NN2−N incorrect pairings. For CLIP, N=32,768N=32,768.\\n\\n\\n\\n\\n\\n\\nAnother way to look at this is that each training batch of CLIP is two classification tasks.\\n\\n\\nEach image can be paired with N possible texts, and the model tries to predict the correct one. This is the same setup as image-to-text retrieval.\\nLcontrastive:txt2im=−1N∑iNlog(exp(LTiViβ)∑Njexp(LTiVjβ))Lcontrastive:txt2im=−1N∑iNlog\\u2061(exp\\u2061(LiTViβ)∑jNexp\\u2061(LiTVjβ))\\n\\n\\nEach text can be paired with N possible images, and the model tries to predict the correct image. This is the same setup as text-to-image retrieval.\\nLcontrastive:im2txt=−1N∑iNlog(exp(VTiLiβ)∑Njexp(VTiLjβ))Lcontrastive:im2txt=−1N∑iNlog\\u2061(exp\\u2061(ViTLiβ)∑jNexp\\u2061(ViTLjβ))\\n\\n\\nThe sum of these two losses is minimized. 𝛽 is a trainable inverse temperature parameter.\\nThis is what it all looks like in pseudocode.\\n\\n\\n\\n\\n\\nCLIP authors found that the contrastive objective provided a 12x improvement in efficiency compared to the language model objective baseline while producing higher-quality image embeddings.\\n\\n\\n\\n\\n\\nCLIP applications\\nClassification\\nToday, for many image classification tasks, CLIP is still a strong out-of-the-box baseline to be used as-is or fine-tuned.\\n\\n\\n\\n\\n\\nText-based image retrieval\\nSince CLIP’s training process was conceptually similar to image-to-text retrieval and text-to-image retrieval, CLIP “displays significant promise for widely-applicable tasks like image retrieval or search.” However, “on image retrieval, CLIP’s performance relative to the overall state of the art is noticeably lower.”\\nThere are attempts to use CLIP for image retrieval. For example, clip-retrieval package works as follows:\\n\\nGenerate CLIP embeddings for all your images and store them in a vector database.\\nFor each text query, generate a CLIP embedding for this text.\\nQuery in the vector database for all images whose embeddings are close to this text query embedding.\\n\\nImage generation\\nCLIP’s joint image-text embeddings are useful for image generation. Given a text prompt, DALL-E (2021) generates many different visuals and uses CLIP to rerank these visuals before showing the top visuals to users.\\nIn 2022, OpenAI introduced unCLIP, a text-to-image synthesis model conditioned on CLIP latents. It consists of two main components:\\n\\nCLIP is trained and frozen. The pretrained CLIP model can generate embeddings for both text and images in the same embedding space.\\nTwo things happen at image generation:\\n    \\nUse CLIP to generate embedding for this text.\\nUse a diffusion decoder to generate images conditioned on this embedding.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nText generation: visual question answering, captioning\\nCLIP authors did attempt to create a model for text generation. One version they experimented with is called LM RN50. Though this model could generate text responses, its performance was consistently around 10% below CLIP’s best-performing model on all the vision-language understanding tasks that CLIP was evaluated on.\\nWhile today CLIP isn’t used directly for text generation, its image encoder is often the backbone for LMMs that can generate texts.\\nFlamingo: the dawns of LMMs\\nUnlike CLIP, Flamingo can generate text responses. In a reductive view, Flamingo is CLIP + a language model, with added techniques to make it possible for the language model to generate text tokens conditioned on both visual and text inputs.\\n\\n\\n\\n\\n    Flamingo can generate text responses conditioned on both text and images\\n\\n\\nFlamingo's high-level architecture\\nAt a high level, Flamingo consists of 2 parts:\\n\\nVision encoder: a CLIP-like model is trained using contrastive learning. The text encoder of this model is then discarded. The vision encoder is frozen to be used in the main model.\\nLanguage model: Flamingo finetunes Chinchilla to generate text tokens, conditioned on visuals and text, using language model loss, with two additional components Perceiver Resampler and GATED XATTN-DENSE layers. We’ll discuss them later in this blog.\\n\\n\\n\\n\\n\\n\\n\\nData\\nFlamingo used 4 datasets: 2 (image, text) pair datasets, 1 (video, text) pair dataset, and 1 interleaved image and text dataset.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDataset\\n\\nType\\n\\nSize\\n\\nHow\\n\\nTraining weight\\n\\n\\n\\nM3W\\n   \\nInterleaved image and text dataset\\n   \\n43M webpages\\n   \\nFor each webpage, they sample a random subsequence of 256 tokens and take up to the first 5 images included in the sampled sequence.\\n   \\n1.0\\n   \\n\\n\\nALIGN\\n   \\n(Image, text) pairs\\n   \\n1.8B pairs\\n   \\nTexts are alt-texts, averaging 12 tokens/text.\\n   \\n0.2\\n   \\n\\n\\nLTIP\\n   \\n(Image, text) pairs\\n   \\n312M pairs\\n   \\nTexts are long descriptions, averaging 20.5 tokens/text.\\n   \\n0.2\\n   \\n\\n\\nVTP\\n   \\n(Video, text) pairs\\n   \\n27M short videos\\n   \\n~22 seconds/video on average\\n   \\n0.03\\n   \\n\\n\\n\\nFlamingo's vision encoder\\nFlamingo first trains a CLIP-like model from scratch using contrastive learning. This component only uses the 2 (image, text) pair datasets, ALIGN and LTIP, totaling 2.1B (image, text) pairs. This is 5x larger than the dataset CLIP was trained on.\\n\\nFor the text encoder, Flamingo uses BERT instead of GPT-2.\\nFor the vision encoder, Flamingo uses a NormalizerFree ResNet (NFNet) F6 model.\\nText and vision embeddings are meanpooled before being projected to the joint embedding space.\\n\\nFlamingo's language model\\nFlamingo uses Chinchilla as their language model. More specifically, they freeze the 9 pretrained Chinchilla LM layers. A traditional language model predicts the next text token based on the preceding text tokens. Flamingo predicts the next text token based on both the preceding text and visual tokens.\\n\\n\\n\\n\\n    Next token generation is conditioned on both text and visual tokens. Illustration taken from Chunyuan Li's CVPR 2023 tutorial: Large Multimodal Models.\\n\\n\\nTo be able to generate text conditioned on both text and visual inputs, Flamingo relied on Perceiver Resampler and GATED XATTN-DENSE layers.\\nPerceiver Resampler\\nAs the visual inputs can be both images and videos, the vision encoder can produce a variable number of image or video features. Perceiver Resampler converts these variable features into a consistent 64 visual outputs.\\nInterestingly enough, while training the vision encoder, the resolution used was 288 x 288. However, at this phase, visual inputs are resized to 320 × 320. It’s been shown that a higher test-time resolution can lead to improved performance when using CNNs.\\n\\n\\n\\n\\n\\n\\nGATED XATTN-DENSE layers\\nGATED XATTN-DENSE layers are inserted between existing and frozen LM layers to allow the language model to attend more efficiently to the visual tokens when generating text tokens. Without these layers, Flamingo authors noted a drop of 4.2% in the overall score.\\n\\n\\n\\n\\n\\n\\nLoss function\\nFlamingo computes the likelihood of text yy conditioned on the interleaved images and videos xx.\\np(y|x)=∏l=1Np(yl|y<l,x≤l)p(y|x)=∏l=1Np(yl|y<l,x≤l)\\nThe training loss function was a weighted sum of expected negative log-likelihoods of generated text across all 4 datasets, with λmλm being the training weight of dataset mm.\\n∑m=1MλmE(x,y)∼Dm[−∑l=1Llogp(y|x)]∑m=1MλmE(x,y)∼Dm[−∑l=1Llog\\u2061p(y|x)]\\nTraining\\nWhile the Chinchilla LM layers are finetuned and frozen, the additional components are trained from scratch, using all 4 Flamingo datasets, with different weights. Finding the right per-dataset weights was key to performance. The weight for each dataset is in the Training weight column in the dataset table above.\\nVTP’s weight is much smaller than other datasets (0.03 compared to 0.2 and 1), so its contribution to the training should be minimal. However, the authors noted that removing this dataset negatively affects performance on all video tasks.\\nWhile Flamingo isn’t open-sourced, there are many open-source replications of Flamingo.\\n\\nIDEFICS (HuggingFace)\\nmlfoundations/open_flamingo\\n\\nTL;DR: CLIP vs. Flamingo\\n\\n\\n\\n\\n\\n\\nPart 3. Research Directions for LMMs\\nCLIP is 3 years old and Flamingo is almost 2. While their architectures serve as a good foundation for us to understand how LMMs are built, there have been many new progresses in the space.\\nHere are a few directions that I’m excited about. This is far from an exhaustive list, both because this post has been long and because I’m still learning about the space too. If you have any pointers or suggestions, please let me know!\\nIncorporating more data modalities\\nToday, most multimodal systems work with text and images. It’s only a matter of time before we need systems that can incorporate other modalities such as videos, music, and 3D. Wouldn’t it be amazing to have one shared embedding space for ALL data modalities?\\nExamples of works in this space:\\n\\nULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding (Xue et al., Dec 2022)\\nImageBind: One Embedding Space To Bind Them All (Girdhar et al., May 2023)\\nNExT-GPT: Any-to-Any Multimodal Large Language Model (Wu et al., Sep 2023)\\nJeff Dean’s ambitious Pathways project (2021): its vision is to “enable multimodal models that encompass vision, auditory, and language understanding simultaneously.”\\n\\n\\n\\n\\n\\n\\nMultimodal systems for instruction-following\\nFlamingo was trained for completion, but not for dialogue or for following instructions. (If you’re not familiar with completion vs. dialogue, check out my post on RLHF). Many people are working on building LMMs that can follow instructions and have conversations, such as:\\n\\nMultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (Xu et al., Dec 2022)\\nLLaVA: Visual Instruction Tuning (Liu et al., Apr 28, 2023)\\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Salesforce, May 11, 2023)\\nLaVIN: Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models (Luo et al., May 24, 2023)\\n\\n\\n\\n\\n\\n    Examples of LaVIN's outputs compared to other LMMs, shown in LaVIN's paper\\n\\n\\nAdapters for more efficient multimodal training\\nWhile Flamingo used 9 pretrained and frozen layers from Chinchilla, it had to pretrain its vision encoder, Perceiver Resampler, and GATED XATTN-DENSE layers from scratch. These train-from-scratch modules could be compute-intensive. Many works focus on more efficient ways to bootstrap multimodal systems using less training from scratch.\\nSome works are quite promising. BLIP-2, for example, outperformed Flamingo-80B by 8.7% on zero-shot VQA-v2 with 54x fewer trainable parameters.\\nWorks in this space include:\\n\\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\\n[LAVIN] Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models\\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model\\n\\nThe two images below are from Chunyuan Li’s Large Multimodal Models tutorial at CVPR 2023, which is, btw, an excellent tutorial.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGenerating multimodal outputs\\nWhile models that can process multimodal inputs are becoming the norm, multimodal output is still lagging. Many use cases require multimodal outputs. For example, if we ask ChatGPT to explain RLHF, an effective explanation might require graphs, equations, and even simple animations.\\nTo generate multimodal outputs, a model would first need to generate a shared intermediate output. One key question is what the intermediate output would look like.\\nOne option for intermediate output is text, which will then be used to generate/synthesize other actions.\\nFor example, CM3 (Aghajanyan et al., 2022) outputs HTML markup which can be compiled into webpages that contain not only text but also formattings, links, and images. GPT-4V generates Latex code, which can then be reconstructed as data tables.\\n\\n\\n\\n\\n    Sampled outputs from CM3\\n\\n\\n\\n\\n\\n\\n    GPT-4V generates Latex code, which can then be reconstructed as a data table\\n\\n\\nAnother option for intermediate output would be multimodal tokens. This is the option that Caiming Xiong, whose team at Salesforce has done a lot of awesome work on multimodality, told me. Each token will have a tag to denote whether it’s a text token or an image token. Image tokens will then be input into an image model like Diffusion to generate images. Text tokens will then be input into a language model.\\nGenerating Images with Multimodal Language Models (Koh et al., Jun 2023) is an awesome paper that shows how LMMs can generate and retrieve images together with generating texts. See below.\\n\\n\\n\\n\\n\\n\\n\\nConclusion\\nIt’s been a lot of fun going over so many multimodal papers as well as talking to people doing awesome work and trying to summarize the key patterns in one blog post. There’s so much about multimodality that I’m sure there are many things that I’ve missed, but I hope that this post provides the core patterns that will help you develop multimodal systems and apply them to your work.\\nAs you see in part 3 of this post, we’re still in the early days of multimodal systems (so early that a friend told me he’s not sure if the LMM abbreviation would catch on). Yes, in most of my conversations, there’s little doubt that multimodal systems in general, and LMMs in particular, will be even more impactful than large language models. However, keep in mind that LMMs do not make LLMs obsolete. As LMMs extend upon LLMs, the performance of an LMM relies on the performance of its base LLM. Many labs that work on multimodal systems work on LLMs in parallel.\\nEarly reviewers\\nI’d like to thank the amazing early reviewers who gave me plenty of pointers and suggestions to make this post better: Han-chung Lee, Sam Reiswig, and Luke Metz.\\nResources\\nModels\\nAn incomplete list of multimodal systems by time to give you a sense of how fast the space is moving!\\n\\nMicrosoft COCO Captions: Data Collection and Evaluation Server (Apr 2015)\\nVQA: Visual Question Answering (May 2015)\\nVideoBERT: A Joint Model for Video and Language Representation Learning (Google, Apr 3, 2019)\\nLXMERT: Learning Cross-Modality Encoder Representations from Transformers (UNC Chapel Hill, Aug 20, 2019)\\n[CLIP] Learning Transferable Visual Models From Natural Language Supervision (OpenAI, 2021)\\nUnifying Vision-and-Language Tasks via Text Generation (UNC Chapel Hill, May 2021)\\nBLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (Salesforce, Jan 28, 2022)\\nFlamingo: a Visual Language Model for Few-Shot Learning (DeepMind, April 29, 2022)\\nGIT: A Generative Image-to-text Transformer for Vision and Language (Microsoft, May 2, 2022)\\nMultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (Xu et al., Dec 2022)\\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (Salesforce, Jan 30, 2023)\\nCross-Modal Fine-Tuning: Align then Refine (Shen et al., Feb 11, 2023)\\nKOSMOS-1: Language Is Not All You Need: Aligning Perception with Language Models (Microsoft, Feb 27, 2023)\\nPaLM-E: An Embodied Multimodal Language Model (Google, Mar 10, 2023)\\nLLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention (Zhang et al., Mar 28, 2023)\\nmPLUG-Owl: Modularization Empowers Large Language Models with Multimodality (Ye et al., Apr 2, 2023)\\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model (Gao et al., Apr 28, 2023)\\nLLaVA: Visual Instruction Tuning (Liu et al., Apr 28, 2023)\\nX-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages (Chen et al., May 7, 2023)\\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Salesforce, May 11, 2023)\\nTowards Expert-Level Medical Question Answering with Large Language Models (Singhal et al., May 16, 2023)\\nCheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models (Luo et al., May 24, 2023)\\nShikra: Unleashing Multimodal LLM’s Referential Dialogue Magic (SenseTime, Jun 3, 2023)\\nMacaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration (Tencent, Jun 15, 2023)\\n\\nOther resources\\n\\n[CVPR2023 Tutorial Talk] Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4\\n\\nSlides: Large Multimodal Models\\n\\n\\n[CMU course] 11-777 MMML\\n[Open source] Salesforce’s LAVIS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSubscribe\\n\\n\\n\\nPlease enable JavaScript to view the comments powered by Disqus.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nchip@[thiswebsite]\\n\\n\\n\\nchiphuyen\\n\\n\\n\\nchipro\\n\\n\\n\\nchipiscrazy\\n\\n\\n\\n\\n\\n\\n\\n\\nhuyenchip19\\n\\n\\nchiphuyen\\n\\n\\n\\n\\nI help companies deploy machine learning into production. I write about AI applications, tooling, and best practices.\\n\\n\\nSubscribe\\n\\n\\n\\n\\nHighlight an imageHighlightAsk AITurn offDeleteImportantImportantImportantImportantImportantImportantChange a colorWrite a memoGo to LinerAsk AI\\n\\n\\n\\nSelect text only\\n\\n\\n\\nImportant\\nMore important\\nMost Important\\nKey phrase\\nAgree\\nDisagree\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community. document_loaders import BSHTMLLoader\n",
    "\n",
    "html_loader = BSHTMLLoader(html_path, open_encoding=\"utf-8\")\n",
    "docs = html_loader.load()\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './data_source/a.html',\n",
       " 'title': 'Multimodality and Large Multimodal Models (LMMs)'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load from Markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "markdown_file = \"./data_source/OpenAI_cookbook_README.md\"\n",
    "modes = [\"single\", \"elements\", \"paged\"]\n",
    "markdown_loader = UnstructuredMarkdownLoader(markdown_file, mode=modes [0])\n",
    "docs = markdown_loader. load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Load from Web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://huyenchip.com/2023/10/10/multimodal.html'}, page_content=\"\\n\\n\\n\\n\\nTable of Contents\\n\\nPart 1. Understanding Multimodal\\n\\nWhy multimodal\\nData modalities\\nMultimodal tasks\\n\\nGeneration\\nVision-language understanding\\n\\n\\n\\nPart 2. Fundamentals of Multimodal Training\\n\\nCLIP: Contrastive Language-Image Pre-training\\n\\nCLIP's high-level architecture\\nNatural language supervision\\nContrastive learning\\n\\nClassifier objective\\nLanguage model objective\\nContrastive objective\\n\\n\\nCLIP applications\\n\\n\\nFlamingo: the dawns of LMMs\\n\\nFlamingo's high-level architecture\\nData\\nFlamingo's vision encoder\\nFlamingo's language model\\n\\n\\nTL;DR: CLIP vs. Flamingo\\n\\nPart 3. Research Directions for LMMs\\n\\nIncorporating more data modalities\\nMultimodal systems for instruction-following\\nAdapters for more efficient multimodal training\\nGenerating multimodal outputs\\n\\nConclusion\\nResources\\n\\n\\n\\n    Table of Contents \\n\\n\\n\\n\\nMultimodality and Large Multimodal Models (LMMs)\\n\\n\\n        \\n        Oct 10, 2023\\n      \\n      \\n        â€¢ Chip Huyen\\n\\n\\n\\nFor a long time, each ML model operated in one data mode â€“ text (translation, language modeling), image (object detection, image classification), or audio (speech recognition).\\nHowever, natural intelligence is not limited to just a single modality. Humans can read, talk, and see. We listen to music to relax and watch out for strange noises to detect danger. Being able to work with multimodal data is essential for us or any AI to operate in the real world.\\nOpenAI noted in their GPT-4V system card that â€œincorporating additional modalities (such as image inputs) into LLMs is viewed by some as a key frontier in AI research and development.â€�\\nIncorporating additional modalities to LLMs (Large Language Models) creates LMMs (Large Multimodal Models). Not all multimodal systems are LMMs. For example, text-to-image models like Midjourney, Stable Diffusion, and Dall-E are multimodal but donâ€™t have a language model component. Multimodal can mean one or more of the following:\\n\\nInput and output are of different modalities (e.g. text-to-image, image-to-text)\\nInputs are multimodal (e.g. a system that can process both text and images)\\nOutputs are multimodal (e.g. a system that can generate both text and images)\\n\\nThis post covers multimodal systems in general, including LMMs. It consists of 3 parts.\\n\\nPart 1 covers the context for multimodality, including why multimodal, different data modalities, and types of multimodal tasks.\\nPart 2 discusses the fundamentals of a multimodal system, using the examples of CLIP, which lays the foundation for many future multimodal systems, and Flamingo, whose impressive performance gave rise to LMMs.\\nPart 3 discusses some active research areas for LMMs, including generating multimodal outputs and adapters for more efficient multimodal training, covering newer multimodal systems such as BLIP-2, LLaVA, LLaMA-Adapter V2, LAVIN, etc.\\n\\nThe post is long. Feel free to skip to the sections most interesting to you.\\nâš\\xa0 Ambiguous terminology âš\\xa0\\nMultimodal data can also refer to multimodal distributions, e.g. bimodal distribution, which is different from multimodal data in this post.\\n\\n\\nTable of contents\\nPart 1. Understanding Multimodal\\nâ€¦. Why multimodal\\nâ€¦. Data modalities\\nâ€¦. Multimodal tasks\\nâ€¦â€¦.. Generation\\nâ€¦â€¦.. Vision-language understanding\\nPart 2. Fundamentals of Multimodal Training\\nâ€¦. CLIP: Contrastive Language-Image Pre-training\\nâ€¦â€¦.. CLIPâ€™s high-level architecture\\nâ€¦â€¦.. Natural language supervision\\nâ€¦â€¦.. Contrastive learning\\nâ€¦â€¦.. CLIP applications\\nâ€¦. Flamingo: the dawns of LMMs\\nâ€¦â€¦.. Flamingoâ€™s high-level architecture\\nâ€¦â€¦.. Data\\nâ€¦â€¦.. Flamingoâ€™s vision encoder\\nâ€¦â€¦.. Flamingoâ€™s language model\\nâ€¦. TL;DR: CLIP vs. Flamingo\\nPart 3. Research Directions for LMMs\\nâ€¦. Incorporating more data modalities\\nâ€¦. Multimodal systems for instruction-following\\nâ€¦. Adapters for more efficient multimodal training\\nâ€¦. Generating multimodal outputs\\nConclusion\\nResources\\n\\n\\nPart 1. Understanding Multimodal\\nWhy multimodal\\nMany use cases are impossible without multimodality, especially those in industries that deal with a mixture of data modalities such as healthcare, robotics, e-commerce, retail, gaming, etc.\\n\\n\\n\\n\\n    An example of how multimodality can be used in healthcare. Image from Multimodal biomedical AI (Acosta et al., Nature Medicine 2022)\\n\\n\\nNot only that, incorporating data from other modalities can help boost model performance. Shouldnâ€™t a model that can learn from both text and images perform better than a model that can learn from only text or only image?\\nMultimodal systems can provide a more flexible interface, allowing you to interact with them in whichever way works best for you at the moment. Imagine you can ask a question by typing, talking, or just pointing your camera at something.\\nOne use case that Iâ€™m especially excited about, is that multimodality can also enable visually impaired people to browse the Internet and also navigate the real world.\\n\\n\\n\\n\\n    Some cool multimodal use cases from GPT-4V\\n\\n\\nData modalities\\nDifferent data modes are text, image, audio, tabular data, etc. One data mode can be represented or approximated in another data mode. For example:\\n\\nAudio can be represented as images (mel spectrograms).\\nSpeech can be transcribed into text, though its text-only representation loses information such as volume, intonation, pauses, etc.\\nAn image can be represented as a vector, which, in turn, can be flattened and represented as a sequence of text tokens.\\nA video is a sequence of images plus audio. ML models today mostly treat videos as sequences of images. This is a severe limitation, as sounds have proved to be just as important as visuals for videos. 88% of TikTok users shared that sound is essential for their TikTok experience.\\nA text can be represented as an image if you simply take a picture of it.\\nA data table can be converted into a chart, which is an image.\\n\\n\\nHow about other data modalities?\\nAll digital data formats can be represented using bitstrings (strings of 0 and 1) or bytestrings. A model that can effectively learn from bitstrings or bytestrings will be very powerful, and it can learn from any data mode.\\nThere are other data modalities we havenâ€™t touched on, such as graphs and 3D assets. We also havenâ€™t touched on the formats used to represent smell and touch (haptics).\\n\\n\\nIn ML today, audio is still largely treated as a voice-based alternative to text. The most common use cases for audio are still speech recognition (speech-to-text) and speech synthesis (text-to-speech). Non-speech audio use cases, e.g. music generation, are still pretty niche. See the fake Drake & Weeknd song and MusicGen model on HuggingFace.\\nImage is perhaps the most versatile format for model inputs, as it can be used to represent text, tabular data, audio, and to some extent, videos. Thereâ€™s also so much more visual data than text data. We have phones/webcams that constantly take pictures and videos today.\\nText is a much more powerful mode for model outputs. A model that can generate images can only be used for image generation, whereas a model that can generate text can be used for many tasks: summarization, translation, reasoning, question answering, etc.\\nFor simplicity, weâ€™ll focus on 2 modalities: images and text. The learnings can be somewhat generalized to other modalities.\\nMultimodal tasks\\nTo understand multimodal systems, itâ€™s helpful to look at the tasks they are built to solve. In literature, I commonly see vision-language tasks divided into two groups: generation and vision-language understanding (VLU), which is the umbrella term for all tasks that donâ€™t require generation. The line between these two groups is blurred, as being able to generate answers requires understanding too.\\nGeneration\\nFor generative tasks, the output can be unimodal (e.g. text, image, 3D rendering) or multimodal. While unimodal outputs are common today, multimodal outputs are still shaping up. Weâ€™ll discuss multimodal outputs at the end of this post.\\nImage generation (text-to-image synthesis)\\nThis category is straightforward. Examples: Dall-E, Stable Diffusion, and Midjourney.\\nText generation\\nA common text generation task is visual question answering. Instead of relying only on text for the context, you can give the model both text and images. Imagine you can point your camera to anything and ask questions like: â€œMy car wonâ€™t start. Whatâ€™s wrong with it?â€�, â€œHow to make this dish?â€�, or â€œWhat is this meme about?â€�.\\nAnother common use case is image captioning, which can be used as part of a text-based image retrieval system. An organization might have millions, if not billions, of images: product images, graphs, designs, team pictures, promotional materials, etc. AI can automatically generate captions and metadata for them, making it easier to find the exact images you want.\\nVision-language understanding\\nWeâ€™ll zoom into two task types: classification and text-based image retrieval (TBIR).\\nClassification\\nClassification models can only generate outputs that belong to a pre-determined list of classes. This works when you only care about a fixed number of outcomes. For example, an OCR system only needs to predict if a visual is one of the known characters (e.g. a digit or a letter).\\nSide note: An OCR system processes data at the character level. When used together with a system that can understand the broader context, it can improve use cases such as allowing you to â€œtalkâ€� to any textbook, contract, assembly instructions, etc.\\n\\n\\n\\n\\n    Document processing with GPT-4V. The model's mistake is highlighted in red.\\n\\n\\nOne related task to classification is image-to-text retrieval: given an image and a pool of pre-defined texts, find the text thatâ€™s most likely to accompany the image. This can be helpful for product image search, i.e. retrieving product reviews from a picture.\\nText-based image retrieval (image search)\\nImage search matters not only for search engines but also for enterprises to be able to search through all their internal images and documents. Some people call text-based image retrieval â€œtext-to-image retrievalâ€�.\\nThere are several approaches to text-based image retrieval. Two of them are:\\n\\nGenerate captions and metadata for each image, either manually or automatically (see image captioning in Text generation). Given a text query, find images whose captions/metadata are closest to this text query.\\nTrain a joint embedding space for both images and text. Given a text query, generate an embedding for this query, and find all images whose embeddings are closest to this embedding.\\n\\nThe second approach is more flexible, and I believe will be more widely used. This approach requires having a strong joint embedding space for both vision and language, like the one that OpenAIâ€™s CLIP developed.\\nPart 2. Fundamentals of Multimodal Training\\nGiven the existence of so many amazing multimodal systems, a challenge of writing this post is choosing which systems to focus on. In the end, I decided to focus on two models: CLIP (2021) and Flamingo (2022) both for their significance as well as availability and clarity of public details.\\n\\nCLIP was the first model that could generalize to multiple image classification tasks with zero- and few-shot learning.\\nFlamingo wasnâ€™t the first large multimodal model that could generate open-ended responses (Salesforceâ€™s BLIP came out 3 months prior). However, Flamingoâ€™s strong performance prompted some to consider it the GPT-3 moment in the multimodal domain.\\n\\nEven though these two models are older, many techniques they use are still relevant today. I hope they serve as the foundation to understanding newer models. The multimodal space is evolving repaidly, with many new ideas being developed. Weâ€™ll go over these newer models in Part 3.\\nAt a high level, a multimodal system consists of the following components:\\n\\nAn encoder for each data modality to generate the embeddings for data of that modality.\\nA way to align embeddings of different modalities into the same multimodal embedding space.\\n[Generative models only] A language model to generate text responses. Since inputs can contain both text and visuals, new techniques need to be developed to allow the language model to condition its responses on not just text, but also visuals.\\n\\nIdeally, as many of these components should be pretrained and reusable as possible.\\nCLIP: Contrastive Language-Image Pre-training\\nCLIPâ€™s key contribution is its ability to map data of different modalities, text and images, into a shared embedding space. This shared multimodal embedding space makes text-to-image and image-to-text tasks so much easier.\\nTraining this multimodal embedding space also produced a strong image encoder, which allows CLIP to achieve competitive zero-shot performance on many image classification tasks. This strong image encoder can be used for many other tasks: image generation, visual question answering, and text-based image retrieval. Flamingo and LLaVa use CLIP as their image encoder. DALL-E uses CLIP to rerank generated images. Itâ€™s unclear if GPT-4V uses CLIP.\\n\\n\\n\\n\\n    Zero-shot image classification with CLIP\\n\\n\\nCLIP leveraged natural language supervision and contrastive learning, which allowed CLIP to both scale up their data and make training more efficient. Weâ€™ll go over why/how these two techniques work.\\nCLIP's high-level architecture\\n\\n\\n\\n\\n    CLIP's architecture. Both encoders and projection matrices are jointly trained together from scratch. The training goal is to maximize the similarity scores of the right (image, text) pairings while minimizing the similarity scores of the wrong pairings (contrastive learning). \\n\\n\\nFor the image encoder, the authors experimented with both ResNet and ViT. Their best-performing model is ViT-L/14@336px:\\n\\nLarge vision transformer (ViT-L)\\n14 patches (each image is divided into 14x14 pixel patches/sub-images)\\non 336x336 pixel input\\n\\nFor the text encoder, CLIP uses a Transformer model similar to GPT-2 but smaller. Their base model has only 63M parameters with 8 attention heads. The authors found CLIPâ€™s performance to be less sensitive to the capacity of the text encoder.\\nEmbeddings generated by the image encoder and text encoder are projected into the same embedding space using two projection matrices \\\\(W_v\\\\) and \\\\(W_l\\\\).\\n\\nGiven an image embedding \\\\(V_i\\\\), the corresponding multimodal embedding is computed as: \\\\(W_vV_i\\\\).\\nGiven a text embedding \\\\(L_i\\\\), the corresponding multimodal embedding is computed as: \\\\(W_lL_i\\\\).\\n\\nWhen people say CLIP embeddings, they either refer to these multimodal embeddings or the embeddings generated by CLIPâ€™s image encoder.\\nNatural language supervision\\nFor many years, image models were trained with manually annotated (image, text) datasets (e.g. ImageNet, MS COCO). This isnâ€™t scalable. Manual annotation is time-consuming and expensive.\\nThe CLIP paper noted that none of the then-available (image, text) datasets was big and high quality enough. They created their own dataset â€“ 400M (image, text) pairs â€“ as follows.\\n\\nConstruct a list of 500,000 queries. Queries are common words, bigrams, and titles of popular Wikipedia articles.\\nFind images matching these queries (string and substring match). The paper mentioned this search did NOT happen on search engines but didnâ€™t specify where. My theory is that since OpenAI already scraped the entire Internet for their GPT models, they probably just queried their internal database.\\nEach image is paired with a text that co-occurs with it (e.g. captions, comments) instead of the query since queries are too short to be descriptive.\\n\\nBecause some queries are more popular than others, to avoid data imbalance, they used at most 20K images for a query.\\nContrastive learning\\nPre-CLIP, most vision-language models were trained using a classifier or language model objectives. Contrastive objective is a clever technique that allows CLIP to scale and generalize to multiple tasks.\\nWeâ€™ll show why the constrastive objective works better for CLIP using an example task of image captioning: given an image, generate a text that describes it.\\nClassifier objective\\nA classifier predicts the correct class among a predetermined list of classes. This works when the output space is finite. Previous models that work with (image, text) pair datasets all had this limitation. For example, models working with ILSVRC-2012 limited themselves to 1,000 classes, and JFT-300M to 18,291 classes.\\nThis objective limits not only the modelâ€™s capacity to output meaningful responses but also its capacity for zero-shot learning. Say, if the model was trained to predict among 10 classes, it wonâ€™t work for a task that has 100 classes.\\nLanguage model objective\\nIf a classifier outputs only one class for each input, a language model outputs a sequence of classes. Each generated class is called a token. Each token is from a predetermined list, the vocabulary, of the language model.\\n\\n\\n\\n\\n    Classifier vs. language model objectives\\n\\n\\nContrastive objective\\nWhile the language model objective allows for vastly more flexible outputs, CLIP authors noted this objective made the training difficult. They hypothesized that this is because the model tries to generate exactly the text accompanying each image, while many possible texts can accompany an image: alt-text, caption, comments, etc.\\nFor example, in the Flickr30K dataset, each image has 5 captions provided by human annotators, and the captions for the same image can be very different.\\n\\n\\n\\n\\n\\n\\nContrastive learning is to overcome this challenge. Instead of predicting the exact text of each image, CLIP was trained to predict whether a text is more likely to accompany an image than other texts.\\nFor each batch of \\\\(N\\\\) (image, text) pairs, the model generates N text embeddings and N image embeddings.\\n\\nLet \\\\(V_1, V_2, ..., V_n\\\\) be the embeddings for the \\\\(N\\\\) images.\\nLet \\\\(L_1, L_2, ..., L_n\\\\) be the embeddings for the \\\\(N\\\\) texts.\\n\\nCLIP computes the cosine similarity scores of the \\\\(N^2\\\\) possible (\\\\(V_i, L_j\\\\)) pairings. The model is trained to maximize the similarity scores of the \\\\(N\\\\) correct pairings while minimizing the scores of the \\\\(N^2 - N\\\\) incorrect pairings. For CLIP, \\\\(N = 32,768\\\\).\\n\\n\\n\\n\\n\\n\\nAnother way to look at this is that each training batch of CLIP is two classification tasks.\\n\\n\\nEach image can be paired with N possible texts, and the model tries to predict the correct one. This is the same setup as image-to-text retrieval.\\n\\n\\\\[L_{\\\\text{contrastive:txt2im}} = -\\\\frac{1}{N}\\\\sum_i^N\\\\log(\\\\frac{\\\\exp(L_i^TV_i\\\\beta)}{\\\\sum_j^N\\\\exp(L_i^TV_j\\\\beta)})\\\\]\\n  \\n\\nEach text can be paired with N possible images, and the model tries to predict the correct image. This is the same setup as text-to-image retrieval.\\n\\n\\\\[L_{\\\\text{contrastive:im2txt}} = -\\\\frac{1}{N}\\\\sum_i^N\\\\log(\\\\frac{\\\\exp(V_i^TL_i\\\\beta)}{\\\\sum_j^N\\\\exp(V_i^TL_j\\\\beta)})\\\\]\\n  \\n\\nThe sum of these two losses is minimized. ğ�›½ is a trainable inverse temperature parameter.\\nThis is what it all looks like in pseudocode.\\n\\n\\n\\n\\n\\nCLIP authors found that the contrastive objective provided a 12x improvement in efficiency compared to the language model objective baseline while producing higher-quality image embeddings.\\n\\n\\n\\n\\n\\nCLIP applications\\nClassification\\nToday, for many image classification tasks, CLIP is still a strong out-of-the-box baseline to be used as-is or fine-tuned.\\n\\n\\n\\n\\n\\nText-based image retrieval\\nSince CLIPâ€™s training process was conceptually similar to image-to-text retrieval and text-to-image retrieval, CLIP â€œdisplays significant promise for widely-applicable tasks like image retrieval or search.â€� However, â€œon image retrieval, CLIPâ€™s performance relative to the overall state of the art is noticeably lower.â€�\\nThere are attempts to use CLIP for image retrieval. For example, clip-retrieval package works as follows:\\n\\nGenerate CLIP embeddings for all your images and store them in a vector database.\\nFor each text query, generate a CLIP embedding for this text.\\nQuery in the vector database for all images whose embeddings are close to this text query embedding.\\n\\nImage generation\\nCLIPâ€™s joint image-text embeddings are useful for image generation. Given a text prompt, DALL-E (2021) generates many different visuals and uses CLIP to rerank these visuals before showing the top visuals to users.\\nIn 2022, OpenAI introduced unCLIP, a text-to-image synthesis model conditioned on CLIP latents. It consists of two main components:\\n\\nCLIP is trained and frozen. The pretrained CLIP model can generate embeddings for both text and images in the same embedding space.\\nTwo things happen at image generation:\\n    \\nUse CLIP to generate embedding for this text.\\nUse a diffusion decoder to generate images conditioned on this embedding.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nText generation: visual question answering, captioning\\nCLIP authors did attempt to create a model for text generation. One version they experimented with is called LM RN50. Though this model could generate text responses, its performance was consistently around 10% below CLIPâ€™s best-performing model on all the vision-language understanding tasks that CLIP was evaluated on.\\nWhile today CLIP isnâ€™t used directly for text generation, its image encoder is often the backbone for LMMs that can generate texts.\\nFlamingo: the dawns of LMMs\\nUnlike CLIP, Flamingo can generate text responses. In a reductive view, Flamingo is CLIP + a language model, with added techniques to make it possible for the language model to generate text tokens conditioned on both visual and text inputs.\\n\\n\\n\\n\\n    Flamingo can generate text responses conditioned on both text and images\\n\\n\\nFlamingo's high-level architecture\\nAt a high level, Flamingo consists of 2 parts:\\n\\nVision encoder: a CLIP-like model is trained using contrastive learning. The text encoder of this model is then discarded. The vision encoder is frozen to be used in the main model.\\nLanguage model: Flamingo finetunes Chinchilla to generate text tokens, conditioned on visuals and text, using language model loss, with two additional components Perceiver Resampler and GATED XATTN-DENSE layers. Weâ€™ll discuss them later in this blog.\\n\\n\\n\\n\\n\\n\\n\\nData\\nFlamingo used 4 datasets: 2 (image, text) pair datasets, 1 (video, text) pair dataset, and 1 interleaved image and text dataset.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDataset\\n\\nType\\n\\nSize\\n\\nHow\\n\\nTraining weight\\n\\n\\n\\nM3W\\n   \\nInterleaved image and text dataset\\n   \\n43M webpages\\n   \\nFor each webpage, they sample a random subsequence of 256 tokens and take up to the first 5 images included in the sampled sequence.\\n   \\n1.0\\n   \\n\\n\\nALIGN\\n   \\n(Image, text) pairs\\n   \\n1.8B pairs\\n   \\nTexts are alt-texts, averaging 12 tokens/text.\\n   \\n0.2\\n   \\n\\n\\nLTIP\\n   \\n(Image, text) pairs\\n   \\n312M pairs\\n   \\nTexts are long descriptions, averaging 20.5 tokens/text.\\n   \\n0.2\\n   \\n\\n\\nVTP\\n   \\n(Video, text) pairs\\n   \\n27M short videos\\n   \\n~22 seconds/video on average\\n   \\n0.03\\n   \\n\\n\\n\\nFlamingo's vision encoder\\nFlamingo first trains a CLIP-like model from scratch using contrastive learning. This component only uses the 2 (image, text) pair datasets, ALIGN and LTIP, totaling 2.1B (image, text) pairs. This is 5x larger than the dataset CLIP was trained on.\\n\\nFor the text encoder, Flamingo uses BERT instead of GPT-2.\\nFor the vision encoder, Flamingo uses a NormalizerFree ResNet (NFNet) F6 model.\\nText and vision embeddings are meanpooled before being projected to the joint embedding space.\\n\\nFlamingo's language model\\nFlamingo uses Chinchilla as their language model. More specifically, they freeze the 9 pretrained Chinchilla LM layers. A traditional language model predicts the next text token based on the preceding text tokens. Flamingo predicts the next text token based on both the preceding text and visual tokens.\\n\\n\\n\\n\\n    Next token generation is conditioned on both text and visual tokens. Illustration taken from Chunyuan Li's CVPR 2023 tutorial: Large Multimodal Models.\\n\\n\\nTo be able to generate text conditioned on both text and visual inputs, Flamingo relied on Perceiver Resampler and GATED XATTN-DENSE layers.\\nPerceiver Resampler\\nAs the visual inputs can be both images and videos, the vision encoder can produce a variable number of image or video features. Perceiver Resampler converts these variable features into a consistent 64 visual outputs.\\nInterestingly enough, while training the vision encoder, the resolution used was 288 x 288. However, at this phase, visual inputs are resized to 320 Ã— 320. Itâ€™s been shown that a higher test-time resolution can lead to improved performance when using CNNs.\\n\\n\\n\\n\\n\\n\\nGATED XATTN-DENSE layers\\nGATED XATTN-DENSE layers are inserted between existing and frozen LM layers to allow the language model to attend more efficiently to the visual tokens when generating text tokens. Without these layers, Flamingo authors noted a drop of 4.2% in the overall score.\\n\\n\\n\\n\\n\\n\\nLoss function\\nFlamingo computes the likelihood of text \\\\(y\\\\) conditioned on the interleaved images and videos \\\\(x\\\\).\\n\\n\\\\[p(y|x) = \\\\prod_{l=1}^N p(y_l|y_{<l}, x_{\\\\leq l})\\\\]\\n\\nThe training loss function was a weighted sum of expected negative log-likelihoods of generated text across all 4 datasets, with \\\\(\\\\lambda_m\\\\) being the training weight of dataset \\\\(m\\\\).\\n\\n\\\\[\\\\sum_{m=1}^M \\\\lambda_m E_{(x, y)\\\\sim D_m} [ -\\\\sum_{l=1}^L \\\\log p(y|x)]\\\\]\\n\\nTraining\\nWhile the Chinchilla LM layers are finetuned and frozen, the additional components are trained from scratch, using all 4 Flamingo datasets, with different weights. Finding the right per-dataset weights was key to performance. The weight for each dataset is in the Training weight column in the dataset table above.\\nVTPâ€™s weight is much smaller than other datasets (0.03 compared to 0.2 and 1), so its contribution to the training should be minimal. However, the authors noted that removing this dataset negatively affects performance on all video tasks.\\nWhile Flamingo isnâ€™t open-sourced, there are many open-source replications of Flamingo.\\n\\nIDEFICS (HuggingFace)\\nmlfoundations/open_flamingo\\n\\nTL;DR: CLIP vs. Flamingo\\n\\n\\n\\n\\n\\n\\nPart 3. Research Directions for LMMs\\nCLIP is 3 years old and Flamingo is almost 2. While their architectures serve as a good foundation for us to understand how LMMs are built, there have been many new progresses in the space.\\nHere are a few directions that Iâ€™m excited about. This is far from an exhaustive list, both because this post has been long and because Iâ€™m still learning about the space too. If you have any pointers or suggestions, please let me know!\\nIncorporating more data modalities\\nToday, most multimodal systems work with text and images. Itâ€™s only a matter of time before we need systems that can incorporate other modalities such as videos, music, and 3D. Wouldnâ€™t it be amazing to have one shared embedding space for ALL data modalities?\\nExamples of works in this space:\\n\\nULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding (Xue et al., Dec 2022)\\nImageBind: One Embedding Space To Bind Them All (Girdhar et al., May 2023)\\nNExT-GPT: Any-to-Any Multimodal Large Language Model (Wu et al., Sep 2023)\\nJeff Deanâ€™s ambitious Pathways project (2021): its vision is to â€œenable multimodal models that encompass vision, auditory, and language understanding simultaneously.â€�\\n\\n\\n\\n\\n\\n\\nMultimodal systems for instruction-following\\nFlamingo was trained for completion, but not for dialogue or for following instructions. (If youâ€™re not familiar with completion vs. dialogue, check out my post on RLHF). Many people are working on building LMMs that can follow instructions and have conversations, such as:\\n\\nMultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (Xu et al., Dec 2022)\\nLLaVA: Visual Instruction Tuning (Liu et al., Apr 28, 2023)\\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Salesforce, May 11, 2023)\\nLaVIN: Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models (Luo et al., May 24, 2023)\\n\\n\\n\\n\\n\\n    Examples of LaVIN's outputs compared to other LMMs, shown in LaVIN's paper\\n\\n\\nAdapters for more efficient multimodal training\\nWhile Flamingo used 9 pretrained and frozen layers from Chinchilla, it had to pretrain its vision encoder, Perceiver Resampler, and GATED XATTN-DENSE layers from scratch. These train-from-scratch modules could be compute-intensive. Many works focus on more efficient ways to bootstrap multimodal systems using less training from scratch.\\nSome works are quite promising. BLIP-2, for example, outperformed Flamingo-80B by 8.7% on zero-shot VQA-v2 with 54x fewer trainable parameters.\\nWorks in this space include:\\n\\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\\n[LAVIN] Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models\\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model\\n\\nThe two images below are from Chunyuan Liâ€™s Large Multimodal Models tutorial at CVPR 2023, which is, btw, an excellent tutorial.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGenerating multimodal outputs\\nWhile models that can process multimodal inputs are becoming the norm, multimodal output is still lagging. Many use cases require multimodal outputs. For example, if we ask ChatGPT to explain RLHF, an effective explanation might require graphs, equations, and even simple animations.\\nTo generate multimodal outputs, a model would first need to generate a shared intermediate output. One key question is what the intermediate output would look like.\\nOne option for intermediate output is text, which will then be used to generate/synthesize other actions.\\nFor example, CM3 (Aghajanyan et al., 2022) outputs HTML markup which can be compiled into webpages that contain not only text but also formattings, links, and images. GPT-4V generates Latex code, which can then be reconstructed as data tables.\\n\\n\\n\\n\\n    Sampled outputs from CM3\\n\\n\\n\\n\\n\\n\\n    GPT-4V generates Latex code, which can then be reconstructed as a data table\\n\\n\\nAnother option for intermediate output would be multimodal tokens. This is the option that Caiming Xiong, whose team at Salesforce has done a lot of awesome work on multimodality, told me. Each token will have a tag to denote whether itâ€™s a text token or an image token. Image tokens will then be input into an image model like Diffusion to generate images. Text tokens will then be input into a language model.\\nGenerating Images with Multimodal Language Models (Koh et al., Jun 2023) is an awesome paper that shows how LMMs can generate and retrieve images together with generating texts. See below.\\n\\n\\n\\n\\n\\n\\n\\nConclusion\\nItâ€™s been a lot of fun going over so many multimodal papers as well as talking to people doing awesome work and trying to summarize the key patterns in one blog post. Thereâ€™s so much about multimodality that Iâ€™m sure there are many things that Iâ€™ve missed, but I hope that this post provides the core patterns that will help you develop multimodal systems and apply them to your work.\\nAs you see in part 3 of this post, weâ€™re still in the early days of multimodal systems (so early that a friend told me heâ€™s not sure if the LMM abbreviation would catch on). Yes, in most of my conversations, thereâ€™s little doubt that multimodal systems in general, and LMMs in particular, will be even more impactful than large language models. However, keep in mind that LMMs do not make LLMs obsolete. As LMMs extend upon LLMs, the performance of an LMM relies on the performance of its base LLM. Many labs that work on multimodal systems work on LLMs in parallel.\\nEarly reviewers\\nIâ€™d like to thank the amazing early reviewers who gave me plenty of pointers and suggestions to make this post better: Han-chung Lee, Sam Reiswig, and Luke Metz.\\nResources\\nModels\\nAn incomplete list of multimodal systems by time to give you a sense of how fast the space is moving!\\n\\nMicrosoft COCO Captions: Data Collection and Evaluation Server (Apr 2015)\\nVQA: Visual Question Answering (May 2015)\\nVideoBERT: A Joint Model for Video and Language Representation Learning (Google, Apr 3, 2019)\\nLXMERT: Learning Cross-Modality Encoder Representations from Transformers (UNC Chapel Hill, Aug 20, 2019)\\n[CLIP] Learning Transferable Visual Models From Natural Language Supervision (OpenAI, 2021)\\nUnifying Vision-and-Language Tasks via Text Generation (UNC Chapel Hill, May 2021)\\nBLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (Salesforce, Jan 28, 2022)\\nFlamingo: a Visual Language Model for Few-Shot Learning (DeepMind, April 29, 2022)\\nGIT: A Generative Image-to-text Transformer for Vision and Language (Microsoft, May 2, 2022)\\nMultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (Xu et al., Dec 2022)\\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (Salesforce, Jan 30, 2023)\\nCross-Modal Fine-Tuning: Align then Refine (Shen et al., Feb 11, 2023)\\nKOSMOS-1: Language Is Not All You Need: Aligning Perception with Language Models (Microsoft, Feb 27, 2023)\\nPaLM-E: An Embodied Multimodal Language Model (Google, Mar 10, 2023)\\nLLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention (Zhang et al., Mar 28, 2023)\\nmPLUG-Owl: Modularization Empowers Large Language Models with Multimodality (Ye et al., Apr 2, 2023)\\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model (Gao et al., Apr 28, 2023)\\nLLaVA: Visual Instruction Tuning (Liu et al., Apr 28, 2023)\\nX-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages (Chen et al., May 7, 2023)\\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Salesforce, May 11, 2023)\\nTowards Expert-Level Medical Question Answering with Large Language Models (Singhal et al., May 16, 2023)\\nCheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models (Luo et al., May 24, 2023)\\nShikra: Unleashing Multimodal LLMâ€™s Referential Dialogue Magic (SenseTime, Jun 3, 2023)\\nMacaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration (Tencent, Jun 15, 2023)\\n\\nOther resources\\n\\n[CVPR2023 Tutorial Talk] Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4\\n\\nSlides: Large Multimodal Models\\n\\n\\n[CMU course] 11-777 MMML\\n[Open source] Salesforceâ€™s LAVIS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlease enable JavaScript to view the comments powered by Disqus.\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "web_paths = [\"https://huyenchip.com/2023/10/10/multimodal.html\"]\n",
    "\n",
    "classes = ['post-content', 'post-title', 'post-header', 'page-content']\n",
    "bs4_strainer = bs4.SoupStrainer(class_= classes)\n",
    "\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=web_paths,\n",
    "    bs_kwargs=dict(\n",
    "    parse_only=bs4_strainer\n",
    "),\n",
    ")\n",
    "docs = web_loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://huyenchip.com/2023/10/10/multimodal.html'}, page_content=\"\\n\\n\\n\\n\\nTable of Contents\\n\\nPart 1. Understanding Multimodal\\n\\nWhy multimodal\\nData modalities\\nMultimodal tasks\\n\\nGeneration\\nVision-language understanding\\n\\n\\n\\nPart 2. Fundamentals of Multimodal Training\\n\\nCLIP: Contrastive Language-Image Pre-training\\n\\nCLIP's high-level architecture\\nNatural language supervision\\nContrastive learning\\n\\nClassifier objective\\nLanguage model objective\\nContrastive objective\\n\\n\\nCLIP applications\\n\\n\\nFlamingo: the dawns of LMMs\\n\\nFlamingo's high-level architecture\\nData\\nFlamingo's vision encoder\\nFlamingo's language model\\n\\n\\nTL;DR: CLIP vs. Flamingo\\n\\nPart 3. Research Directions for LMMs\\n\\nIncorporating more data modalities\\nMultimodal systems for instruction-following\\nAdapters for more efficient multimodal training\\nGenerating multimodal outputs\\n\\nConclusion\\nResources\\n\\n\\n\\n    Table of Contents \\n\\n\\n\\n\\nMultimodality and Large Multimodal Models (LMMs)\\n\\n\\n        \\n        Oct 10, 2023\\n      \\n      \\n        â€¢ Chip Huyen\\n\\n\\n\\nFor a long time, each ML model operated in one data mode â€“ text (translation, language modeling), image (object detection, image classification), or audio (speech recognition).\\nHowever, natural intelligence is not limited to just a single modality. Humans can read, talk, and see. We listen to music to relax and watch out for strange noises to detect danger. Being able to work with multimodal data is essential for us or any AI to operate in the real world.\\nOpenAI noted in their GPT-4V system card that â€œincorporating additional modalities (such as image inputs) into LLMs is viewed by some as a key frontier in AI research and development.â€�\\nIncorporating additional modalities to LLMs (Large Language Models) creates LMMs (Large Multimodal Models). Not all multimodal systems are LMMs. For example, text-to-image models like Midjourney, Stable Diffusion, and Dall-E are multimodal but donâ€™t have a language model component. Multimodal can mean one or more of the following:\\n\\nInput and output are of different modalities (e.g. text-to-image, image-to-text)\\nInputs are multimodal (e.g. a system that can process both text and images)\\nOutputs are multimodal (e.g. a system that can generate both text and images)\\n\\nThis post covers multimodal systems in general, including LMMs. It consists of 3 parts.\\n\\nPart 1 covers the context for multimodality, including why multimodal, different data modalities, and types of multimodal tasks.\\nPart 2 discusses the fundamentals of a multimodal system, using the examples of CLIP, which lays the foundation for many future multimodal systems, and Flamingo, whose impressive performance gave rise to LMMs.\\nPart 3 discusses some active research areas for LMMs, including generating multimodal outputs and adapters for more efficient multimodal training, covering newer multimodal systems such as BLIP-2, LLaVA, LLaMA-Adapter V2, LAVIN, etc.\\n\\nThe post is long. Feel free to skip to the sections most interesting to you.\\nâš\\xa0 Ambiguous terminology âš\\xa0\\nMultimodal data can also refer to multimodal distributions, e.g. bimodal distribution, which is different from multimodal data in this post.\\n\\n\\nTable of contents\\nPart 1. Understanding Multimodal\\nâ€¦. Why multimodal\\nâ€¦. Data modalities\\nâ€¦. Multimodal tasks\\nâ€¦â€¦.. Generation\\nâ€¦â€¦.. Vision-language understanding\\nPart 2. Fundamentals of Multimodal Training\\nâ€¦. CLIP: Contrastive Language-Image Pre-training\\nâ€¦â€¦.. CLIPâ€™s high-level architecture\\nâ€¦â€¦.. Natural language supervision\\nâ€¦â€¦.. Contrastive learning\\nâ€¦â€¦.. CLIP applications\\nâ€¦. Flamingo: the dawns of LMMs\\nâ€¦â€¦.. Flamingoâ€™s high-level architecture\\nâ€¦â€¦.. Data\\nâ€¦â€¦.. Flamingoâ€™s vision encoder\\nâ€¦â€¦.. Flamingoâ€™s language model\\nâ€¦. TL;DR: CLIP vs. Flamingo\\nPart 3. Research Directions for LMMs\\nâ€¦. Incorporating more data modalities\\nâ€¦. Multimodal systems for instruction-following\\nâ€¦. Adapters for more efficient multimodal training\\nâ€¦. Generating multimodal outputs\\nConclusion\\nResources\\n\\n\\nPart 1. Understanding Multimodal\\nWhy multimodal\\nMany use cases are impossible without multimodality, especially those in industries that deal with a mixture of data modalities such as healthcare, robotics, e-commerce, retail, gaming, etc.\\n\\n\\n\\n\\n    An example of how multimodality can be used in healthcare. Image from Multimodal biomedical AI (Acosta et al., Nature Medicine 2022)\\n\\n\\nNot only that, incorporating data from other modalities can help boost model performance. Shouldnâ€™t a model that can learn from both text and images perform better than a model that can learn from only text or only image?\\nMultimodal systems can provide a more flexible interface, allowing you to interact with them in whichever way works best for you at the moment. Imagine you can ask a question by typing, talking, or just pointing your camera at something.\\nOne use case that Iâ€™m especially excited about, is that multimodality can also enable visually impaired people to browse the Internet and also navigate the real world.\\n\\n\\n\\n\\n    Some cool multimodal use cases from GPT-4V\\n\\n\\nData modalities\\nDifferent data modes are text, image, audio, tabular data, etc. One data mode can be represented or approximated in another data mode. For example:\\n\\nAudio can be represented as images (mel spectrograms).\\nSpeech can be transcribed into text, though its text-only representation loses information such as volume, intonation, pauses, etc.\\nAn image can be represented as a vector, which, in turn, can be flattened and represented as a sequence of text tokens.\\nA video is a sequence of images plus audio. ML models today mostly treat videos as sequences of images. This is a severe limitation, as sounds have proved to be just as important as visuals for videos. 88% of TikTok users shared that sound is essential for their TikTok experience.\\nA text can be represented as an image if you simply take a picture of it.\\nA data table can be converted into a chart, which is an image.\\n\\n\\nHow about other data modalities?\\nAll digital data formats can be represented using bitstrings (strings of 0 and 1) or bytestrings. A model that can effectively learn from bitstrings or bytestrings will be very powerful, and it can learn from any data mode.\\nThere are other data modalities we havenâ€™t touched on, such as graphs and 3D assets. We also havenâ€™t touched on the formats used to represent smell and touch (haptics).\\n\\n\\nIn ML today, audio is still largely treated as a voice-based alternative to text. The most common use cases for audio are still speech recognition (speech-to-text) and speech synthesis (text-to-speech). Non-speech audio use cases, e.g. music generation, are still pretty niche. See the fake Drake & Weeknd song and MusicGen model on HuggingFace.\\nImage is perhaps the most versatile format for model inputs, as it can be used to represent text, tabular data, audio, and to some extent, videos. Thereâ€™s also so much more visual data than text data. We have phones/webcams that constantly take pictures and videos today.\\nText is a much more powerful mode for model outputs. A model that can generate images can only be used for image generation, whereas a model that can generate text can be used for many tasks: summarization, translation, reasoning, question answering, etc.\\nFor simplicity, weâ€™ll focus on 2 modalities: images and text. The learnings can be somewhat generalized to other modalities.\\nMultimodal tasks\\nTo understand multimodal systems, itâ€™s helpful to look at the tasks they are built to solve. In literature, I commonly see vision-language tasks divided into two groups: generation and vision-language understanding (VLU), which is the umbrella term for all tasks that donâ€™t require generation. The line between these two groups is blurred, as being able to generate answers requires understanding too.\\nGeneration\\nFor generative tasks, the output can be unimodal (e.g. text, image, 3D rendering) or multimodal. While unimodal outputs are common today, multimodal outputs are still shaping up. Weâ€™ll discuss multimodal outputs at the end of this post.\\nImage generation (text-to-image synthesis)\\nThis category is straightforward. Examples: Dall-E, Stable Diffusion, and Midjourney.\\nText generation\\nA common text generation task is visual question answering. Instead of relying only on text for the context, you can give the model both text and images. Imagine you can point your camera to anything and ask questions like: â€œMy car wonâ€™t start. Whatâ€™s wrong with it?â€�, â€œHow to make this dish?â€�, or â€œWhat is this meme about?â€�.\\nAnother common use case is image captioning, which can be used as part of a text-based image retrieval system. An organization might have millions, if not billions, of images: product images, graphs, designs, team pictures, promotional materials, etc. AI can automatically generate captions and metadata for them, making it easier to find the exact images you want.\\nVision-language understanding\\nWeâ€™ll zoom into two task types: classification and text-based image retrieval (TBIR).\\nClassification\\nClassification models can only generate outputs that belong to a pre-determined list of classes. This works when you only care about a fixed number of outcomes. For example, an OCR system only needs to predict if a visual is one of the known characters (e.g. a digit or a letter).\\nSide note: An OCR system processes data at the character level. When used together with a system that can understand the broader context, it can improve use cases such as allowing you to â€œtalkâ€� to any textbook, contract, assembly instructions, etc.\\n\\n\\n\\n\\n    Document processing with GPT-4V. The model's mistake is highlighted in red.\\n\\n\\nOne related task to classification is image-to-text retrieval: given an image and a pool of pre-defined texts, find the text thatâ€™s most likely to accompany the image. This can be helpful for product image search, i.e. retrieving product reviews from a picture.\\nText-based image retrieval (image search)\\nImage search matters not only for search engines but also for enterprises to be able to search through all their internal images and documents. Some people call text-based image retrieval â€œtext-to-image retrievalâ€�.\\nThere are several approaches to text-based image retrieval. Two of them are:\\n\\nGenerate captions and metadata for each image, either manually or automatically (see image captioning in Text generation). Given a text query, find images whose captions/metadata are closest to this text query.\\nTrain a joint embedding space for both images and text. Given a text query, generate an embedding for this query, and find all images whose embeddings are closest to this embedding.\\n\\nThe second approach is more flexible, and I believe will be more widely used. This approach requires having a strong joint embedding space for both vision and language, like the one that OpenAIâ€™s CLIP developed.\\nPart 2. Fundamentals of Multimodal Training\\nGiven the existence of so many amazing multimodal systems, a challenge of writing this post is choosing which systems to focus on. In the end, I decided to focus on two models: CLIP (2021) and Flamingo (2022) both for their significance as well as availability and clarity of public details.\\n\\nCLIP was the first model that could generalize to multiple image classification tasks with zero- and few-shot learning.\\nFlamingo wasnâ€™t the first large multimodal model that could generate open-ended responses (Salesforceâ€™s BLIP came out 3 months prior). However, Flamingoâ€™s strong performance prompted some to consider it the GPT-3 moment in the multimodal domain.\\n\\nEven though these two models are older, many techniques they use are still relevant today. I hope they serve as the foundation to understanding newer models. The multimodal space is evolving repaidly, with many new ideas being developed. Weâ€™ll go over these newer models in Part 3.\\nAt a high level, a multimodal system consists of the following components:\\n\\nAn encoder for each data modality to generate the embeddings for data of that modality.\\nA way to align embeddings of different modalities into the same multimodal embedding space.\\n[Generative models only] A language model to generate text responses. Since inputs can contain both text and visuals, new techniques need to be developed to allow the language model to condition its responses on not just text, but also visuals.\\n\\nIdeally, as many of these components should be pretrained and reusable as possible.\\nCLIP: Contrastive Language-Image Pre-training\\nCLIPâ€™s key contribution is its ability to map data of different modalities, text and images, into a shared embedding space. This shared multimodal embedding space makes text-to-image and image-to-text tasks so much easier.\\nTraining this multimodal embedding space also produced a strong image encoder, which allows CLIP to achieve competitive zero-shot performance on many image classification tasks. This strong image encoder can be used for many other tasks: image generation, visual question answering, and text-based image retrieval. Flamingo and LLaVa use CLIP as their image encoder. DALL-E uses CLIP to rerank generated images. Itâ€™s unclear if GPT-4V uses CLIP.\\n\\n\\n\\n\\n    Zero-shot image classification with CLIP\\n\\n\\nCLIP leveraged natural language supervision and contrastive learning, which allowed CLIP to both scale up their data and make training more efficient. Weâ€™ll go over why/how these two techniques work.\\nCLIP's high-level architecture\\n\\n\\n\\n\\n    CLIP's architecture. Both encoders and projection matrices are jointly trained together from scratch. The training goal is to maximize the similarity scores of the right (image, text) pairings while minimizing the similarity scores of the wrong pairings (contrastive learning). \\n\\n\\nFor the image encoder, the authors experimented with both ResNet and ViT. Their best-performing model is ViT-L/14@336px:\\n\\nLarge vision transformer (ViT-L)\\n14 patches (each image is divided into 14x14 pixel patches/sub-images)\\non 336x336 pixel input\\n\\nFor the text encoder, CLIP uses a Transformer model similar to GPT-2 but smaller. Their base model has only 63M parameters with 8 attention heads. The authors found CLIPâ€™s performance to be less sensitive to the capacity of the text encoder.\\nEmbeddings generated by the image encoder and text encoder are projected into the same embedding space using two projection matrices \\\\(W_v\\\\) and \\\\(W_l\\\\).\\n\\nGiven an image embedding \\\\(V_i\\\\), the corresponding multimodal embedding is computed as: \\\\(W_vV_i\\\\).\\nGiven a text embedding \\\\(L_i\\\\), the corresponding multimodal embedding is computed as: \\\\(W_lL_i\\\\).\\n\\nWhen people say CLIP embeddings, they either refer to these multimodal embeddings or the embeddings generated by CLIPâ€™s image encoder.\\nNatural language supervision\\nFor many years, image models were trained with manually annotated (image, text) datasets (e.g. ImageNet, MS COCO). This isnâ€™t scalable. Manual annotation is time-consuming and expensive.\\nThe CLIP paper noted that none of the then-available (image, text) datasets was big and high quality enough. They created their own dataset â€“ 400M (image, text) pairs â€“ as follows.\\n\\nConstruct a list of 500,000 queries. Queries are common words, bigrams, and titles of popular Wikipedia articles.\\nFind images matching these queries (string and substring match). The paper mentioned this search did NOT happen on search engines but didnâ€™t specify where. My theory is that since OpenAI already scraped the entire Internet for their GPT models, they probably just queried their internal database.\\nEach image is paired with a text that co-occurs with it (e.g. captions, comments) instead of the query since queries are too short to be descriptive.\\n\\nBecause some queries are more popular than others, to avoid data imbalance, they used at most 20K images for a query.\\nContrastive learning\\nPre-CLIP, most vision-language models were trained using a classifier or language model objectives. Contrastive objective is a clever technique that allows CLIP to scale and generalize to multiple tasks.\\nWeâ€™ll show why the constrastive objective works better for CLIP using an example task of image captioning: given an image, generate a text that describes it.\\nClassifier objective\\nA classifier predicts the correct class among a predetermined list of classes. This works when the output space is finite. Previous models that work with (image, text) pair datasets all had this limitation. For example, models working with ILSVRC-2012 limited themselves to 1,000 classes, and JFT-300M to 18,291 classes.\\nThis objective limits not only the modelâ€™s capacity to output meaningful responses but also its capacity for zero-shot learning. Say, if the model was trained to predict among 10 classes, it wonâ€™t work for a task that has 100 classes.\\nLanguage model objective\\nIf a classifier outputs only one class for each input, a language model outputs a sequence of classes. Each generated class is called a token. Each token is from a predetermined list, the vocabulary, of the language model.\\n\\n\\n\\n\\n    Classifier vs. language model objectives\\n\\n\\nContrastive objective\\nWhile the language model objective allows for vastly more flexible outputs, CLIP authors noted this objective made the training difficult. They hypothesized that this is because the model tries to generate exactly the text accompanying each image, while many possible texts can accompany an image: alt-text, caption, comments, etc.\\nFor example, in the Flickr30K dataset, each image has 5 captions provided by human annotators, and the captions for the same image can be very different.\\n\\n\\n\\n\\n\\n\\nContrastive learning is to overcome this challenge. Instead of predicting the exact text of each image, CLIP was trained to predict whether a text is more likely to accompany an image than other texts.\\nFor each batch of \\\\(N\\\\) (image, text) pairs, the model generates N text embeddings and N image embeddings.\\n\\nLet \\\\(V_1, V_2, ..., V_n\\\\) be the embeddings for the \\\\(N\\\\) images.\\nLet \\\\(L_1, L_2, ..., L_n\\\\) be the embeddings for the \\\\(N\\\\) texts.\\n\\nCLIP computes the cosine similarity scores of the \\\\(N^2\\\\) possible (\\\\(V_i, L_j\\\\)) pairings. The model is trained to maximize the similarity scores of the \\\\(N\\\\) correct pairings while minimizing the scores of the \\\\(N^2 - N\\\\) incorrect pairings. For CLIP, \\\\(N = 32,768\\\\).\\n\\n\\n\\n\\n\\n\\nAnother way to look at this is that each training batch of CLIP is two classification tasks.\\n\\n\\nEach image can be paired with N possible texts, and the model tries to predict the correct one. This is the same setup as image-to-text retrieval.\\n\\n\\\\[L_{\\\\text{contrastive:txt2im}} = -\\\\frac{1}{N}\\\\sum_i^N\\\\log(\\\\frac{\\\\exp(L_i^TV_i\\\\beta)}{\\\\sum_j^N\\\\exp(L_i^TV_j\\\\beta)})\\\\]\\n  \\n\\nEach text can be paired with N possible images, and the model tries to predict the correct image. This is the same setup as text-to-image retrieval.\\n\\n\\\\[L_{\\\\text{contrastive:im2txt}} = -\\\\frac{1}{N}\\\\sum_i^N\\\\log(\\\\frac{\\\\exp(V_i^TL_i\\\\beta)}{\\\\sum_j^N\\\\exp(V_i^TL_j\\\\beta)})\\\\]\\n  \\n\\nThe sum of these two losses is minimized. ğ�›½ is a trainable inverse temperature parameter.\\nThis is what it all looks like in pseudocode.\\n\\n\\n\\n\\n\\nCLIP authors found that the contrastive objective provided a 12x improvement in efficiency compared to the language model objective baseline while producing higher-quality image embeddings.\\n\\n\\n\\n\\n\\nCLIP applications\\nClassification\\nToday, for many image classification tasks, CLIP is still a strong out-of-the-box baseline to be used as-is or fine-tuned.\\n\\n\\n\\n\\n\\nText-based image retrieval\\nSince CLIPâ€™s training process was conceptually similar to image-to-text retrieval and text-to-image retrieval, CLIP â€œdisplays significant promise for widely-applicable tasks like image retrieval or search.â€� However, â€œon image retrieval, CLIPâ€™s performance relative to the overall state of the art is noticeably lower.â€�\\nThere are attempts to use CLIP for image retrieval. For example, clip-retrieval package works as follows:\\n\\nGenerate CLIP embeddings for all your images and store them in a vector database.\\nFor each text query, generate a CLIP embedding for this text.\\nQuery in the vector database for all images whose embeddings are close to this text query embedding.\\n\\nImage generation\\nCLIPâ€™s joint image-text embeddings are useful for image generation. Given a text prompt, DALL-E (2021) generates many different visuals and uses CLIP to rerank these visuals before showing the top visuals to users.\\nIn 2022, OpenAI introduced unCLIP, a text-to-image synthesis model conditioned on CLIP latents. It consists of two main components:\\n\\nCLIP is trained and frozen. The pretrained CLIP model can generate embeddings for both text and images in the same embedding space.\\nTwo things happen at image generation:\\n    \\nUse CLIP to generate embedding for this text.\\nUse a diffusion decoder to generate images conditioned on this embedding.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nText generation: visual question answering, captioning\\nCLIP authors did attempt to create a model for text generation. One version they experimented with is called LM RN50. Though this model could generate text responses, its performance was consistently around 10% below CLIPâ€™s best-performing model on all the vision-language understanding tasks that CLIP was evaluated on.\\nWhile today CLIP isnâ€™t used directly for text generation, its image encoder is often the backbone for LMMs that can generate texts.\\nFlamingo: the dawns of LMMs\\nUnlike CLIP, Flamingo can generate text responses. In a reductive view, Flamingo is CLIP + a language model, with added techniques to make it possible for the language model to generate text tokens conditioned on both visual and text inputs.\\n\\n\\n\\n\\n    Flamingo can generate text responses conditioned on both text and images\\n\\n\\nFlamingo's high-level architecture\\nAt a high level, Flamingo consists of 2 parts:\\n\\nVision encoder: a CLIP-like model is trained using contrastive learning. The text encoder of this model is then discarded. The vision encoder is frozen to be used in the main model.\\nLanguage model: Flamingo finetunes Chinchilla to generate text tokens, conditioned on visuals and text, using language model loss, with two additional components Perceiver Resampler and GATED XATTN-DENSE layers. Weâ€™ll discuss them later in this blog.\\n\\n\\n\\n\\n\\n\\n\\nData\\nFlamingo used 4 datasets: 2 (image, text) pair datasets, 1 (video, text) pair dataset, and 1 interleaved image and text dataset.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDataset\\n\\nType\\n\\nSize\\n\\nHow\\n\\nTraining weight\\n\\n\\n\\nM3W\\n   \\nInterleaved image and text dataset\\n   \\n43M webpages\\n   \\nFor each webpage, they sample a random subsequence of 256 tokens and take up to the first 5 images included in the sampled sequence.\\n   \\n1.0\\n   \\n\\n\\nALIGN\\n   \\n(Image, text) pairs\\n   \\n1.8B pairs\\n   \\nTexts are alt-texts, averaging 12 tokens/text.\\n   \\n0.2\\n   \\n\\n\\nLTIP\\n   \\n(Image, text) pairs\\n   \\n312M pairs\\n   \\nTexts are long descriptions, averaging 20.5 tokens/text.\\n   \\n0.2\\n   \\n\\n\\nVTP\\n   \\n(Video, text) pairs\\n   \\n27M short videos\\n   \\n~22 seconds/video on average\\n   \\n0.03\\n   \\n\\n\\n\\nFlamingo's vision encoder\\nFlamingo first trains a CLIP-like model from scratch using contrastive learning. This component only uses the 2 (image, text) pair datasets, ALIGN and LTIP, totaling 2.1B (image, text) pairs. This is 5x larger than the dataset CLIP was trained on.\\n\\nFor the text encoder, Flamingo uses BERT instead of GPT-2.\\nFor the vision encoder, Flamingo uses a NormalizerFree ResNet (NFNet) F6 model.\\nText and vision embeddings are meanpooled before being projected to the joint embedding space.\\n\\nFlamingo's language model\\nFlamingo uses Chinchilla as their language model. More specifically, they freeze the 9 pretrained Chinchilla LM layers. A traditional language model predicts the next text token based on the preceding text tokens. Flamingo predicts the next text token based on both the preceding text and visual tokens.\\n\\n\\n\\n\\n    Next token generation is conditioned on both text and visual tokens. Illustration taken from Chunyuan Li's CVPR 2023 tutorial: Large Multimodal Models.\\n\\n\\nTo be able to generate text conditioned on both text and visual inputs, Flamingo relied on Perceiver Resampler and GATED XATTN-DENSE layers.\\nPerceiver Resampler\\nAs the visual inputs can be both images and videos, the vision encoder can produce a variable number of image or video features. Perceiver Resampler converts these variable features into a consistent 64 visual outputs.\\nInterestingly enough, while training the vision encoder, the resolution used was 288 x 288. However, at this phase, visual inputs are resized to 320 Ã— 320. Itâ€™s been shown that a higher test-time resolution can lead to improved performance when using CNNs.\\n\\n\\n\\n\\n\\n\\nGATED XATTN-DENSE layers\\nGATED XATTN-DENSE layers are inserted between existing and frozen LM layers to allow the language model to attend more efficiently to the visual tokens when generating text tokens. Without these layers, Flamingo authors noted a drop of 4.2% in the overall score.\\n\\n\\n\\n\\n\\n\\nLoss function\\nFlamingo computes the likelihood of text \\\\(y\\\\) conditioned on the interleaved images and videos \\\\(x\\\\).\\n\\n\\\\[p(y|x) = \\\\prod_{l=1}^N p(y_l|y_{<l}, x_{\\\\leq l})\\\\]\\n\\nThe training loss function was a weighted sum of expected negative log-likelihoods of generated text across all 4 datasets, with \\\\(\\\\lambda_m\\\\) being the training weight of dataset \\\\(m\\\\).\\n\\n\\\\[\\\\sum_{m=1}^M \\\\lambda_m E_{(x, y)\\\\sim D_m} [ -\\\\sum_{l=1}^L \\\\log p(y|x)]\\\\]\\n\\nTraining\\nWhile the Chinchilla LM layers are finetuned and frozen, the additional components are trained from scratch, using all 4 Flamingo datasets, with different weights. Finding the right per-dataset weights was key to performance. The weight for each dataset is in the Training weight column in the dataset table above.\\nVTPâ€™s weight is much smaller than other datasets (0.03 compared to 0.2 and 1), so its contribution to the training should be minimal. However, the authors noted that removing this dataset negatively affects performance on all video tasks.\\nWhile Flamingo isnâ€™t open-sourced, there are many open-source replications of Flamingo.\\n\\nIDEFICS (HuggingFace)\\nmlfoundations/open_flamingo\\n\\nTL;DR: CLIP vs. Flamingo\\n\\n\\n\\n\\n\\n\\nPart 3. Research Directions for LMMs\\nCLIP is 3 years old and Flamingo is almost 2. While their architectures serve as a good foundation for us to understand how LMMs are built, there have been many new progresses in the space.\\nHere are a few directions that Iâ€™m excited about. This is far from an exhaustive list, both because this post has been long and because Iâ€™m still learning about the space too. If you have any pointers or suggestions, please let me know!\\nIncorporating more data modalities\\nToday, most multimodal systems work with text and images. Itâ€™s only a matter of time before we need systems that can incorporate other modalities such as videos, music, and 3D. Wouldnâ€™t it be amazing to have one shared embedding space for ALL data modalities?\\nExamples of works in this space:\\n\\nULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding (Xue et al., Dec 2022)\\nImageBind: One Embedding Space To Bind Them All (Girdhar et al., May 2023)\\nNExT-GPT: Any-to-Any Multimodal Large Language Model (Wu et al., Sep 2023)\\nJeff Deanâ€™s ambitious Pathways project (2021): its vision is to â€œenable multimodal models that encompass vision, auditory, and language understanding simultaneously.â€�\\n\\n\\n\\n\\n\\n\\nMultimodal systems for instruction-following\\nFlamingo was trained for completion, but not for dialogue or for following instructions. (If youâ€™re not familiar with completion vs. dialogue, check out my post on RLHF). Many people are working on building LMMs that can follow instructions and have conversations, such as:\\n\\nMultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (Xu et al., Dec 2022)\\nLLaVA: Visual Instruction Tuning (Liu et al., Apr 28, 2023)\\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Salesforce, May 11, 2023)\\nLaVIN: Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models (Luo et al., May 24, 2023)\\n\\n\\n\\n\\n\\n    Examples of LaVIN's outputs compared to other LMMs, shown in LaVIN's paper\\n\\n\\nAdapters for more efficient multimodal training\\nWhile Flamingo used 9 pretrained and frozen layers from Chinchilla, it had to pretrain its vision encoder, Perceiver Resampler, and GATED XATTN-DENSE layers from scratch. These train-from-scratch modules could be compute-intensive. Many works focus on more efficient ways to bootstrap multimodal systems using less training from scratch.\\nSome works are quite promising. BLIP-2, for example, outperformed Flamingo-80B by 8.7% on zero-shot VQA-v2 with 54x fewer trainable parameters.\\nWorks in this space include:\\n\\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\\n[LAVIN] Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models\\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model\\n\\nThe two images below are from Chunyuan Liâ€™s Large Multimodal Models tutorial at CVPR 2023, which is, btw, an excellent tutorial.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGenerating multimodal outputs\\nWhile models that can process multimodal inputs are becoming the norm, multimodal output is still lagging. Many use cases require multimodal outputs. For example, if we ask ChatGPT to explain RLHF, an effective explanation might require graphs, equations, and even simple animations.\\nTo generate multimodal outputs, a model would first need to generate a shared intermediate output. One key question is what the intermediate output would look like.\\nOne option for intermediate output is text, which will then be used to generate/synthesize other actions.\\nFor example, CM3 (Aghajanyan et al., 2022) outputs HTML markup which can be compiled into webpages that contain not only text but also formattings, links, and images. GPT-4V generates Latex code, which can then be reconstructed as data tables.\\n\\n\\n\\n\\n    Sampled outputs from CM3\\n\\n\\n\\n\\n\\n\\n    GPT-4V generates Latex code, which can then be reconstructed as a data table\\n\\n\\nAnother option for intermediate output would be multimodal tokens. This is the option that Caiming Xiong, whose team at Salesforce has done a lot of awesome work on multimodality, told me. Each token will have a tag to denote whether itâ€™s a text token or an image token. Image tokens will then be input into an image model like Diffusion to generate images. Text tokens will then be input into a language model.\\nGenerating Images with Multimodal Language Models (Koh et al., Jun 2023) is an awesome paper that shows how LMMs can generate and retrieve images together with generating texts. See below.\\n\\n\\n\\n\\n\\n\\n\\nConclusion\\nItâ€™s been a lot of fun going over so many multimodal papers as well as talking to people doing awesome work and trying to summarize the key patterns in one blog post. Thereâ€™s so much about multimodality that Iâ€™m sure there are many things that Iâ€™ve missed, but I hope that this post provides the core patterns that will help you develop multimodal systems and apply them to your work.\\nAs you see in part 3 of this post, weâ€™re still in the early days of multimodal systems (so early that a friend told me heâ€™s not sure if the LMM abbreviation would catch on). Yes, in most of my conversations, thereâ€™s little doubt that multimodal systems in general, and LMMs in particular, will be even more impactful than large language models. However, keep in mind that LMMs do not make LLMs obsolete. As LMMs extend upon LLMs, the performance of an LMM relies on the performance of its base LLM. Many labs that work on multimodal systems work on LLMs in parallel.\\nEarly reviewers\\nIâ€™d like to thank the amazing early reviewers who gave me plenty of pointers and suggestions to make this post better: Han-chung Lee, Sam Reiswig, and Luke Metz.\\nResources\\nModels\\nAn incomplete list of multimodal systems by time to give you a sense of how fast the space is moving!\\n\\nMicrosoft COCO Captions: Data Collection and Evaluation Server (Apr 2015)\\nVQA: Visual Question Answering (May 2015)\\nVideoBERT: A Joint Model for Video and Language Representation Learning (Google, Apr 3, 2019)\\nLXMERT: Learning Cross-Modality Encoder Representations from Transformers (UNC Chapel Hill, Aug 20, 2019)\\n[CLIP] Learning Transferable Visual Models From Natural Language Supervision (OpenAI, 2021)\\nUnifying Vision-and-Language Tasks via Text Generation (UNC Chapel Hill, May 2021)\\nBLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (Salesforce, Jan 28, 2022)\\nFlamingo: a Visual Language Model for Few-Shot Learning (DeepMind, April 29, 2022)\\nGIT: A Generative Image-to-text Transformer for Vision and Language (Microsoft, May 2, 2022)\\nMultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (Xu et al., Dec 2022)\\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (Salesforce, Jan 30, 2023)\\nCross-Modal Fine-Tuning: Align then Refine (Shen et al., Feb 11, 2023)\\nKOSMOS-1: Language Is Not All You Need: Aligning Perception with Language Models (Microsoft, Feb 27, 2023)\\nPaLM-E: An Embodied Multimodal Language Model (Google, Mar 10, 2023)\\nLLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention (Zhang et al., Mar 28, 2023)\\nmPLUG-Owl: Modularization Empowers Large Language Models with Multimodality (Ye et al., Apr 2, 2023)\\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model (Gao et al., Apr 28, 2023)\\nLLaVA: Visual Instruction Tuning (Liu et al., Apr 28, 2023)\\nX-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages (Chen et al., May 7, 2023)\\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Salesforce, May 11, 2023)\\nTowards Expert-Level Medical Question Answering with Large Language Models (Singhal et al., May 16, 2023)\\nCheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models (Luo et al., May 24, 2023)\\nShikra: Unleashing Multimodal LLMâ€™s Referential Dialogue Magic (SenseTime, Jun 3, 2023)\\nMacaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration (Tencent, Jun 15, 2023)\\n\\nOther resources\\n\\n[CVPR2023 Tutorial Talk] Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4\\n\\nSlides: Large Multimodal Models\\n\\n\\n[CMU course] 11-777 MMML\\n[Open source] Salesforceâ€™s LAVIS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlease enable JavaScript to view the comments powered by Disqus.\\n\\n\\n\\n\\n\")]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
